{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045374,"sourceType":"datasetVersion","datasetId":4054084}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":23.548694,"end_time":"2025-02-26T10:31:30.799146","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-26T10:31:07.250452","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Medical QA T5 w/GRPO Fine-Tuning\n## RL fine-tuning for Seq2Seq","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"In the hybrid approach described in the code, the same model (the T5 model) is trained with both supervised learning and reinforcement learning (GPRO). That is:\n\n### First, the model acquires basic predictive ability through supervised learning (using labeled data)\n\n### Then, reward-based optimization (GPRO) is applied to the answers generated by the model to further refine the model\n\nWhen predicting the actual answer, a single model trained with both approaches is used. Supervised learning and reinforcement learning are not separate models, but techniques for training the same model in different ways.\n\nThe final model has both the basic ability of supervised learning and the ability to generate higher quality answers acquired through reinforcement learning. With this hybrid approach, it is expected to achieve higher performance with a single model.","metadata":{}},{"cell_type":"code","source":"!pip install trl\n!pip install nltk rouge-score ","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom trl import GRPOConfig\nimport os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom rouge_score import rouge_scorer\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Download NLTK resources\ntry:\n    nltk.download('punkt')\nexcept:\n    pass\n\n# Set seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\n# Device configuration - with fallback to CPU if CUDA is not available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize T5 model and tokenizer\nmodel_name = \"t5-small\"  # You can use larger models like t5-base if you have enough GPU memory\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nbase_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# Function to load and preprocess data\ndef load_and_preprocess_data(file_path, sample_size=None):\n    # Load data from CSV file\n    df = pd.read_csv(file_path)\n    if sample_size:\n        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n    \n    # Extract questions and answers\n    questions = df['Question'].tolist()\n    answers = df['Answer'].tolist()\n    \n    # Split into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        questions, answers, test_size=0.2, random_state=42\n    )\n    \n    return X_train, X_test, y_train, y_test\n\n# Create a GRPO environment wrapper for Seq2Seq tasks\nclass QAEnvironment:\n    def __init__(self, model, tokenizer, questions, answers):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.questions = questions\n        self.answers = answers\n        self.current_idx = 0\n        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n        \n    def step(self, generated_tokens):\n        # Get reference answer\n        reference = self.answers[self.current_idx]\n        \n        # Decode the generated tokens\n        generated_text = self.tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n        \n        # Calculate reward based on ROUGE scores\n        scores = self.calculate_reward(generated_text, reference)\n        reward = scores['combined']\n        \n        # Move to next example\n        self.current_idx = (self.current_idx + 1) % len(self.questions)\n        \n        # Return reward as tensor on the same device as the model\n        return torch.tensor([reward], device=device)\n    \n    def calculate_reward(self, prediction, reference):\n        # Calculate ROUGE scores\n        rouge_scores = self.scorer.score(prediction, reference)\n        \n        # Calculate BLEU score\n        try:\n            smooth = SmoothingFunction().method1\n            reference_tokens = nltk.word_tokenize(reference.lower())\n            prediction_tokens = nltk.word_tokenize(prediction.lower())\n            bleu_score = sentence_bleu([reference_tokens], prediction_tokens, smoothing_function=smooth)\n        except:\n            bleu_score = 0.0\n        \n        # Create a combined score\n        scores = {\n            'rouge1': rouge_scores['rouge1'].fmeasure,\n            'rouge2': rouge_scores['rouge2'].fmeasure,\n            'rougeL': rouge_scores['rougeL'].fmeasure,\n            'bleu': bleu_score,\n            'combined': (\n                rouge_scores['rouge1'].fmeasure + \n                rouge_scores['rouge2'].fmeasure + \n                rouge_scores['rougeL'].fmeasure + \n                bleu_score\n            ) / 4.0\n        }\n        \n        return scores\n    \n    def reset(self):\n        # Reset index to beginning or randomly\n        self.current_idx = random.randint(0, len(self.questions) - 1)\n        return self.get_current_input()\n    \n    def get_current_input(self):\n        question = self.questions[self.current_idx]\n        # For T5, prefix the input with a task-specific prefix\n        encoding = self.tokenizer(f\"answer: {question}\", return_tensors='pt', truncation=True, max_length=128)\n        return {k: v.to(device) for k, v in encoding.items()}\n\n    def get_current_target(self):\n        answer = self.answers[self.current_idx]\n        target_encoding = self.tokenizer(text_target=answer, return_tensors='pt', truncation=True, max_length=128)\n        return target_encoding['input_ids'].to(device)\n\n# Custom GRPO Trainer for Seq2Seq tasks\nclass QAGRPOTrainer:\n    def __init__(self, model, tokenizer, train_questions, train_answers, test_questions, test_answers, \n                 max_length=128, max_new_tokens=64):\n        self.model = model.to(device)\n        self.tokenizer = tokenizer\n        self.train_env = QAEnvironment(model, tokenizer, train_questions, train_answers)\n        self.test_env = QAEnvironment(model, tokenizer, test_questions, test_answers)\n        self.max_length = max_length\n        self.max_new_tokens = max_new_tokens\n        \n        # Configure GRPO\n        self.grpo_config = GRPOConfig(\n            learning_rate=5e-5,\n            gradient_accumulation_steps=1,\n            seed=42,\n            output_dir='./'\n        )\n        \n        # Initialize optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.grpo_config.learning_rate)\n        \n        # Initialize losses and metrics tracking\n        self.train_losses = []\n        self.metrics = []\n        \n    def train(self, epochs=10, eval_freq=1):\n        best_score = 0.0\n        \n        for epoch in range(epochs):\n            print(f\"Epoch {epoch+1}/{epochs}\")\n            epoch_losses = []\n            \n            # Training loop\n            progress_bar = tqdm(range(len(self.train_env.questions)))\n            for i in progress_bar:\n                # Get current example\n                inputs = self.train_env.get_current_input()\n                target_ids = self.train_env.get_current_target()\n                \n                # Forward pass with teacher forcing for supervised learning\n                outputs = self.model(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    labels=target_ids\n                )\n                \n                # Calculate standard seq2seq loss (teacher forcing loss)\n                supervised_loss = outputs.loss\n                \n                # Sample from the model for GRPO using generate()\n                with torch.no_grad():\n                    generated_tokens = self.model.generate(\n                        input_ids=inputs['input_ids'],\n                        attention_mask=inputs['attention_mask'],\n                        max_new_tokens=self.max_new_tokens,\n                        do_sample=True,\n                        temperature=0.7,\n                        top_p=0.9,\n                    )\n                \n                # Get reward from environment\n                reward = self.train_env.step(generated_tokens)\n                \n                # Use log probabilities and reward for GRPO loss\n                # For simplicity, we're just scaling the supervised loss by the reward\n                # This is a simple approximation - in a real implementation, you'd compute \n                # the policy gradient loss more precisely\n                grpo_loss = -reward * supervised_loss\n                \n                # Combine losses - balance between supervised and RL loss\n                loss = supervised_loss + 0.5 * grpo_loss\n                \n                # Backpropagate\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                self.optimizer.step()\n                \n                # Record loss\n                epoch_losses.append(loss.item())\n                progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n                \n                # Move to next example\n                self.train_env.current_idx = (self.train_env.current_idx + 1) % len(self.train_env.questions)\n            \n            # Record average loss for epoch\n            avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n            self.train_losses.append(avg_loss)\n            print(f\"Average training loss: {avg_loss:.4f}\")\n            \n            # Evaluate on test set\n            if (epoch + 1) % eval_freq == 0:\n                metrics = self.evaluate()\n                self.metrics.append(metrics)\n                \n                # Save best model based on combined score\n                if metrics['combined'] > best_score:\n                    best_score = metrics['combined']\n                    torch.save(self.model.state_dict(), \"best_qa_model.pt\")\n                    print(f\"Saved new best model with combined score: {best_score:.4f}\")\n        \n        # Load best model for final evaluation\n        self.model.load_state_dict(torch.load(\"best_qa_model.pt\"))\n        return self.model\n    \n    def evaluate(self):\n        self.model.eval()\n        \n        rouge1_scores = []\n        rouge2_scores = []\n        rougeL_scores = []\n        bleu_scores = []\n        combined_scores = []\n        \n        generated_examples = []\n        \n        with torch.no_grad():\n            for i in tqdm(range(min(len(self.test_env.questions), 100))):  # Evaluate on at most 100 examples\n                # Get test example\n                question = self.test_env.questions[i]\n                reference = self.test_env.answers[i]\n                \n                # Generate answer\n                inputs = self.tokenizer(f\"answer: {question}\", return_tensors='pt', truncation=True, max_length=self.max_length)\n                inputs = {k: v.to(device) for k, v in inputs.items()}\n                \n                generated_ids = self.model.generate(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    max_new_tokens=self.max_new_tokens,\n                    num_beams=4,\n                    early_stopping=True\n                )\n                \n                prediction = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n                \n                # Calculate scores\n                scores = self.test_env.calculate_reward(prediction, reference)\n                \n                # Record scores\n                rouge1_scores.append(scores['rouge1'])\n                rouge2_scores.append(scores['rouge2'])\n                rougeL_scores.append(scores['rougeL'])\n                bleu_scores.append(scores['bleu'])\n                combined_scores.append(scores['combined'])\n                \n                # Save a few examples\n                if len(generated_examples) < 5:\n                    generated_examples.append({\n                        'question': question,\n                        'reference': reference,\n                        'prediction': prediction\n                    })\n        \n        # Calculate average scores\n        avg_metrics = {\n            'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n            'rouge2': sum(rouge2_scores) / len(rouge2_scores),\n            'rougeL': sum(rougeL_scores) / len(rougeL_scores),\n            'bleu': sum(bleu_scores) / len(bleu_scores),\n            'combined': sum(combined_scores) / len(combined_scores)\n        }\n        \n        # Print metrics\n        print(\"\\n=== Evaluation Metrics ===\")\n        print(f\"ROUGE-1: {avg_metrics['rouge1']:.4f}\")\n        print(f\"ROUGE-2: {avg_metrics['rouge2']:.4f}\")\n        print(f\"ROUGE-L: {avg_metrics['rougeL']:.4f}\")\n        print(f\"BLEU: {avg_metrics['bleu']:.4f}\")\n        print(f\"Combined Score: {avg_metrics['combined']:.4f}\")\n        \n        # Print examples\n        print(\"\\n=== Generated Examples ===\")\n        for i, example in enumerate(generated_examples):\n            print(f\"Example {i+1}:\")\n            print(f\"Question: {example['question']}\")\n            print(f\"Reference: {example['reference']}\")\n            print(f\"Prediction: {example['prediction']}\")\n            print(\"----------------------------\")\n        \n        self.model.train()\n        return avg_metrics\n    \n    def plot_learning_curves(self):\n        plt.figure(figsize=(15, 10))\n        \n        # Plot training loss\n        plt.subplot(2, 3, 1)\n        plt.plot(self.train_losses)\n        plt.title('Training Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        \n        # Plot ROUGE-1\n        plt.subplot(2, 3, 2)\n        plt.plot([m['rouge1'] for m in self.metrics])\n        plt.title('ROUGE-1 Score')\n        plt.xlabel('Evaluation')\n        plt.ylabel('Score')\n        \n        # Plot ROUGE-2\n        plt.subplot(2, 3, 3)\n        plt.plot([m['rouge2'] for m in self.metrics])\n        plt.title('ROUGE-2 Score')\n        plt.xlabel('Evaluation')\n        plt.ylabel('Score')\n        \n        # Plot ROUGE-L\n        plt.subplot(2, 3, 4)\n        plt.plot([m['rougeL'] for m in self.metrics])\n        plt.title('ROUGE-L Score')\n        plt.xlabel('Evaluation')\n        plt.ylabel('Score')\n        \n        # Plot BLEU\n        plt.subplot(2, 3, 5)\n        plt.plot([m['bleu'] for m in self.metrics])\n        plt.title('BLEU Score')\n        plt.xlabel('Evaluation')\n        plt.ylabel('Score')\n        \n        # Plot Combined Score\n        plt.subplot(2, 3, 6)\n        plt.plot([m['combined'] for m in self.metrics])\n        plt.title('Combined Score')\n        plt.xlabel('Evaluation')\n        plt.ylabel('Score')\n        \n        plt.tight_layout()\n        plt.savefig('learning_curves_seq2seq_grpo.png')\n        plt.close()\n    \n    def generate_answer(self, question):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = self.tokenizer(f\"answer: {question}\", return_tensors='pt', truncation=True, max_length=self.max_length)\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            \n            generated_ids = self.model.generate(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                max_new_tokens=self.max_new_tokens,\n                num_beams=4,\n                early_stopping=True\n            )\n            \n            answer = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        self.model.train()\n        return answer\n\ndef main():\n    # Define the data file path (update this with your actual file path)\n    file_path = \"/kaggle/input/comprehensive-medical-q-a-dataset/train.csv\"  # Replace with your actual CSV path\n    \n    # Load and preprocess data\n    X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path, sample_size=300)\n    \n    print(f\"Training on {len(X_train)} examples, testing on {len(X_test)} examples\")\n    \n    # Initialize the T5 model\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    \n    # Create trainer\n    trainer = QAGRPOTrainer(\n        model=model,\n        tokenizer=tokenizer,\n        train_questions=X_train,\n        train_answers=y_train,\n        test_questions=X_test,\n        test_answers=y_test,\n        max_length=128,\n        max_new_tokens=64\n    )\n    \n    # Train model\n    print(\"Training model with GRPO...\")\n    trainer.train(epochs=30, eval_freq=1)\n    \n    # Plot learning curves\n    trainer.plot_learning_curves()\n    \n    # Final evaluation\n    print(\"\\n=== Final Model Evaluation ===\")\n    metrics = trainer.evaluate()\n\n\n    import matplotlib.pyplot as plt\n    import matplotlib.image as mpimg\n    img = mpimg.imread('learning_curves_seq2seq_grpo.png')\n    plt.figure(figsize=(10, 8))\n    plt.imshow(img)\n    plt.axis('off')  \n    plt.show()\n    \n    \n    # Test with example questions\n    test_questions = [\n        \"What is machine learning?\",\n        \"How does a neural network work?\",\n        \"Explain the concept of reinforcement learning.\"\n    ]\n    \n    print(\"\\n=== Test Questions ===\")\n    for question in test_questions:\n        answer = trainer.generate_answer(question)\n        print(f\"Question: {question}\")\n        print(f\"Generated Answer: {answer}\")\n        print(\"----------------------------\")\n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}