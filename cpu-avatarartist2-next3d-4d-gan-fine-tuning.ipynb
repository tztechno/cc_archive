{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":288228911,"sourceType":"kernelVersion"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CPU AvatarArtist2: Next3D (4D GAN) Fine-tuning**","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **What is AvatarArtist?**\nAvatarArtist is a cutting-edge AI system that creates high-quality 3D avatars from text descriptions or 2D images. It can transform a simple photo or text prompt into a fully-realized 3D character in various artistic styles.\n\nhttps://kumapowerliu.github.io/AvatarArtist/","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"---\n\n## **Step 2 Pipeline Explanation: Next3D Fine-tuning**\n\n## Overview\nThis pipeline fine-tunes a **Next3D** model (3D-aware GAN) on your style-transferred avatar images to generate new faces in that artistic style while maintaining 3D consistency.\n\n---\n\n## Pipeline Architecture\n\n### 1. **Data Loading**\n```python\nStyleFaceDataset(\n    image_dir=\"./output_styled\",  # Your stylized images from Step 1\n    mesh_dir=None,                 # Optional 3DMM parameters\n    resolution=512                 # Image size\n)\n```\n\n**Purpose**: Loads your style-transferred images and optionally 3D face parameters (shape, expression, pose).\n\n---\n\n### 2. **Generator Architecture (TriplaneGenerator)**\n\n#### Components:\n\n**a) Mapping Network**\n- Converts random noise `z` (512-dim) → intermediate latent `w` (512-dim)\n- 8 fully-connected layers with LeakyReLU activation\n- Inspired by StyleGAN2\n\n**b) 3DMM Conditioning Module** (Optional)\n- Encodes face shape (80-dim) and expression (64-dim) parameters\n- Fuses with latent code to control facial attributes\n- Enables explicit control over identity and expressions\n\n**c) Triplane Backbone**\n- Generates 3D feature representations (tri-planes)\n- Uses StyleGAN2-style synthesis blocks\n- Output: 64×64 feature maps with 96 channels\n\n**d) Super-Resolution Module**\n- Upsamples 64×64 → 512×512\n- Three upsampling stages (2× each)\n- Final output: RGB image in [-1, 1] range\n\n---\n\n### 3. **Discriminator Architecture**\n- Progressive downsampling (StyleGAN2-inspired)\n- 5 discriminator blocks: 512→256→128→64→32\n- Adaptive pooling to 4×4 before final classification\n- Output: Real/Fake score\n\n---\n\n## Key Training Components\n\n### 4. **Loss Functions**\n\n#### **a) Adversarial Loss (Logistic Loss)**\n```python\nd_loss = softplus(d_fake) + softplus(-d_real)  # Discriminator\ng_loss = softplus(-d_fake)                      # Generator\n```\n- Non-saturating GAN loss\n- More stable than traditional GAN loss\n\n#### **b) R1 Regularization**\n```python\nlambda_r1 = 1.0  # Gradient penalty weight\n```\n- Prevents discriminator gradients from exploding\n- Stabilizes training\n- Applied to real images only\n\n#### **c) Density Regularization**\n```python\nlambda_den = 0.1  # Density penalty weight\n```\n- Regularizes triplane features\n- Prevents overly complex 3D representations\n\n#### **d) Adaptive Data Augmentation (ADA)**\n```python\ntarget_accuracy = 0.6\nadjustment_speed = 0.001\n```\n- Automatically adjusts augmentation strength\n- Prevents discriminator from overfitting\n- Includes: horizontal flip, 90° rotations\n\n---\n\n## Parameter Settings\n\n### **Critical Parameters**\n\n| Parameter | Default Value | Purpose | Tuning Guide |\n|-----------|--------------|---------|--------------|\n| `lr_g` | 0.0002 | Generator learning rate | Lower for stability (0.0001) |\n| `lr_d` | 0.0002 | Discriminator learning rate | Lower for stability (0.0001) |\n| `lambda_r1` | 1.0 | R1 regularization strength | Increase if D gradients explode (2.0-5.0) |\n| `lambda_den` | 0.1 | Density regularization | Increase for smoother 3D (0.2-0.5) |\n| `batch_size` | CPU: 1, GPU: 4 | Batch size | GPU 12GB: 2-4, GPU 24GB: 4-8 |\n| `num_epochs` | CPU: 10, GPU: 100 | Training epochs | More data = fewer epochs needed |\n| `grad_clip` | 1.0 | Gradient clipping threshold | Lower if NaN occurs (0.5) |\n\n### **Device-Specific Settings**\n\n**CPU Mode:**\n```python\nBATCH_SIZE = 1\nNUM_EPOCHS = 10  # For testing\nnum_workers = 0\n```\n⚠️ **Warning**: Training on CPU is **50-100× slower** than GPU\n\n**GPU Mode (Recommended):**\n```python\nBATCH_SIZE = 4  # Adjust based on VRAM\nNUM_EPOCHS = 100\nnum_workers = 2\n```\n\n---\n\n## Training Process\n\n### **Step-by-Step Execution**\n\n1. **Initialization**\n   - Creates generator with 96M parameters\n   - Creates discriminator with 30M parameters\n   - Initializes optimizers (Adam with β₁=0.5, β₂=0.999)\n\n2. **Training Loop** (per batch)\n   ```\n   For each batch:\n     1. Train Discriminator:\n        - Evaluate real images → d_real\n        - Generate fake images → d_fake\n        - Compute adversarial loss + R1 penalty\n        - Update discriminator weights\n     \n     2. Update ADA:\n        - Monitor discriminator accuracy\n        - Adjust augmentation strength\n     \n     3. Train Generator:\n        - Generate new fake images\n        - Compute adversarial loss + density penalty\n        - Update generator weights\n   ```\n\n3. **Safety Features**\n   - **NaN Detection**: Monitors for numerical instability\n   - **Gradient Clipping**: Prevents exploding gradients\n   - **Weight Initialization**: Proper initialization for stability\n\n---\n\n## Expected Results\n\n### **Training Metrics**\n\n**Healthy Training Signs:**\n```\nEpoch 1-10:\n  D Loss: 1.2-0.8 (decreasing)\n  G Loss: 1.5-1.0 (decreasing then stabilizing)\n  R1 Loss: 0.5-0.3\n  Density Loss: 0.1-0.05\n  ADA p: 0.0-0.3 (gradually increasing)\n  Real Accuracy: 0.6-0.7\n```\n\n**Convergence Indicators:**\n- D Loss stabilizes around 0.6-0.8\n- G Loss stabilizes around 0.8-1.2\n- ADA p settles around 0.3-0.5\n- No NaN warnings\n\n### **Output Files**\n\n```\nnext3d_checkpoints/\n├── checkpoint_0010.pt  # Every 10 epochs\n├── checkpoint_0020.pt\n├── ...\n├── checkpoint_0100.pt\n└── final_model.pt      # Final trained model\n```\n\n**Checkpoint Contents:**\n- Generator weights\n- Discriminator weights\n- Optimizer states\n- ADA parameters\n- Current epoch\n\n---\n\n## Common Issues & Solutions\n\n### **Problem 1: NaN Loss**\n**Symptoms:**\n```\nWARNING: NaN or Inf detected in d_real\nCRITICAL: Too many NaN occurrences!\n```\n\n**Solutions:**\n1. Reduce learning rates to 0.0001\n2. Decrease batch size to 1\n3. Lower gradient clipping to 0.5\n4. Check input data for NaN values\n5. Restart training from scratch\n\n---\n\n### **Problem 2: Mode Collapse**\n**Symptoms:**\n- G Loss stuck at high value (>2.0)\n- Generated images all look similar\n\n**Solutions:**\n1. Increase R1 regularization (λ_r1 = 2.0-5.0)\n2. Enable ADA (should happen automatically)\n3. Use more diverse training data\n4. Reduce discriminator learning rate\n\n---\n\n### **Problem 3: Discriminator Overfitting**\n**Symptoms:**\n- D Loss → 0 quickly\n- G Loss keeps increasing\n- Real Accuracy > 0.9\n\n**Solutions:**\n1. ADA will automatically increase (check `ada_p`)\n2. Manually increase λ_r1\n3. Add more augmentations\n\n---\n\n### **Problem 4: Slow Training**\n**On CPU:**\n- Each epoch: ~30-60 minutes (with 100 images)\n- **Recommendation**: Use Google Colab GPU or cloud GPU\n\n**On GPU:**\n- Each epoch: 1-3 minutes\n- Can complete 100 epochs in 2-5 hours\n\n---\n\n## Usage After Training\n\n### **Generate New Faces**\n```python\n# Load trained model\ncheckpoint = torch.load(\"next3d_checkpoints/final_model.pt\")\ngenerator.load_state_dict(checkpoint['generator'])\n\n# Generate\nz = torch.randn(1, 512)  # Random latent code\noutput = generator(z)\nimage = output['image']  # Generated face in avatar style\n```\n\n### **Control Generation** (if using 3DMM)\n```python\n# Control shape and expression\nshape = torch.randn(1, 80) * 0.5  # Identity variation\nexp = torch.randn(1, 64) * 0.3    # Expression variation\noutput = generator(z, shape, exp)\n```\n\n---\n\n## Performance Expectations\n\n| Hardware | Batch Size | Time/Epoch | 100 Epochs |\n|----------|-----------|------------|------------|\n| CPU (8 cores) | 1 | 30-60 min | 50-100 hours |\n| GPU (GTX 1080) | 2 | 3-5 min | 5-8 hours |\n| GPU (RTX 3090) | 8 | 1-2 min | 2-3 hours |\n| GPU (A100) | 16 | 30-60 sec | 1-2 hours |\n\n---\n\n## Summary\n\nThis pipeline:\n✅ Trains a 3D-aware GAN on your avatar-style images  \n✅ Maintains 3D consistency and view synthesis capability  \n✅ Includes stability features (ADA, R1, gradient clipping)  \n✅ Can optionally use 3DMM parameters for explicit control  \n✅ Produces checkpoints every 10 epochs for evaluation  \n\n**Best Practice**: Start with 10-20 epochs on GPU to verify training stability before committing to full 100-epoch training.\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n!pip install diffusers transformers accelerate\n!pip install controlnet-aux opencv-python pillow\n!pip install mediapipe==0.10.13","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:24:47.417495Z","iopub.execute_input":"2025-12-24T11:24:47.417924Z","iopub.status.idle":"2025-12-24T11:24:47.533492Z","shell.execute_reply.started":"2025-12-24T11:24:47.417891Z","shell.execute_reply":"2025-12-24T11:24:47.532024Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\npaths=[]\nfor dirname, _, filenames in os.walk('/kaggle/input/cpu-avatarartist1-2d-domain-transfer/output_styled'):\n    for filename in filenames:\n        paths+=[(os.path.join(dirname, filename))]\nprint(paths[0:6])\n\nos.makedirs(\"output_styled\", exist_ok=True)\nfor path in paths:\n    shutil.copy(path, \"output_styled\")\n\nfor dirname, _, filenames in os.walk('./output_styled'):\n    for filename in filenames:\n        print(filename)\n        \nimport os\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import Dict, Tuple, Optional\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport pickle\n\n\n# ==================== Dataset ====================\n\nclass StyleFaceDataset(Dataset):\n    \"\"\"Dataset for style-transferred face images + 3DMM parameters\"\"\"\n    \n    def __init__(\n        self,\n        image_dir: str,\n        mesh_dir: Optional[str] = None,\n        resolution: int = 512,\n        use_3dmm: bool = True\n    ):\n        \"\"\"\n        Args:\n            image_dir: Directory for style-transferred images (output_styled)\n            mesh_dir: Directory for 3DMM parameters (optional)\n            resolution: Image resolution\n            use_3dmm: Whether to use 3DMM parameters\n        \"\"\"\n        self.image_dir = Path(image_dir)\n        self.mesh_dir = Path(mesh_dir) if mesh_dir else None\n        self.resolution = resolution\n        self.use_3dmm = use_3dmm\n        \n        # List image files\n        self.image_files = sorted(list(self.image_dir.glob(\"*.jpg\")) + \n                                  list(self.image_dir.glob(\"*.png\")))\n        \n        # Transformations\n        self.transform = transforms.Compose([\n            transforms.Resize((resolution, resolution)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n        \n        print(f\"Dataset initialized: {len(self.image_files)} images found.\")\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n    def __getitem__(self, idx):\n        # Load image\n        img_path = self.image_files[idx]\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        data = {'image': image, 'index': idx}\n        \n        # Load 3DMM parameters (if they exist)\n        if self.use_3dmm and self.mesh_dir:\n            mesh_file = self.mesh_dir / f\"{img_path.stem}.pkl\"\n            if mesh_file.exists():\n                with open(mesh_file, 'rb') as f:\n                    mesh_params = pickle.load(f)\n                data['shape'] = torch.FloatTensor(mesh_params.get('shape', np.zeros(80)))\n                data['exp'] = torch.FloatTensor(mesh_params.get('exp', np.zeros(64)))\n                data['pose'] = torch.FloatTensor(mesh_params.get('pose', np.zeros(6)))\n            else:\n                # Dummy parameters\n                data['shape'] = torch.zeros(80)\n                data['exp'] = torch.zeros(64)\n                data['pose'] = torch.zeros(6)\n        \n        return data\n\n\n# ==================== Next3D Model Components ====================\n\nclass TriplaneGenerator(nn.Module):\n    \"\"\"Next3D Tri-plane Generator (Simplified version)\"\"\"\n    \n    def __init__(\n        self,\n        z_dim: int = 512,\n        w_dim: int = 512,\n        triplane_channels: int = 96,\n        triplane_resolution: int = 256,\n        use_3dmm: bool = True,\n        shape_dim: int = 80,\n        exp_dim: int = 64\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        self.w_dim = w_dim\n        self.use_3dmm = use_3dmm\n        \n        # Mapping Network (z -> w)\n        self.mapping = MappingNetwork(z_dim, w_dim)\n        \n        # 3DMM Conditioning Module (Dynamic components)\n        if use_3dmm:\n            self.shape_encoder = nn.Linear(shape_dim, w_dim)\n            self.exp_encoder = nn.Linear(exp_dim, w_dim)\n            self.condition_fusion = nn.Linear(w_dim * 3, w_dim)\n        \n        # Tri-plane generation network\n        self.triplane_generator = TriplaneBackbone(\n            w_dim=w_dim,\n            channels=triplane_channels,\n            resolution=triplane_resolution\n        )\n        \n        # Super-resolution module (Low-res -> High-res)\n        self.superres = SuperResolutionModule(\n            triplane_channels, \n            output_resolution=512\n        )\n        \n        # Weight initialization\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, m):\n        \"\"\"Proper weight initialization\"\"\"\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Conv2d):\n            nn.init.normal_(m.weight, 0, 0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n    \n    def forward(\n        self,\n        z: torch.Tensor,\n        shape: Optional[torch.Tensor] = None,\n        exp: Optional[torch.Tensor] = None,\n        c: Optional[torch.Tensor] = None\n    ):\n        \"\"\"\n        Args:\n            z: Latent code [B, z_dim]\n            shape: 3DMM shape parameters [B, shape_dim]\n            exp: 3DMM expression parameters [B, exp_dim]\n            c: Camera parameters [B, 25]\n        \n        Returns:\n            output: Dictionary containing generated image and tri-plane\n        \"\"\"\n        # Mapping network\n        w = self.mapping(z)\n        \n        # 3DMM Conditioning\n        if self.use_3dmm and shape is not None and exp is not None:\n            shape_feat = self.shape_encoder(shape)\n            exp_feat = self.exp_encoder(exp)\n            w_conditioned = self.condition_fusion(\n                torch.cat([w, shape_feat, exp_feat], dim=1)\n            )\n        else:\n            w_conditioned = w\n        \n        # Tri-plane generation\n        triplane_features = self.triplane_generator(w_conditioned)\n        \n        # Rendering (Simplified: Actual version uses volume rendering)\n        # Placeholder: generating image directly from features\n        image = self.superres(triplane_features)\n        \n        return {\n            'image': image,\n            'triplane': triplane_features,  # Used for density regularization\n            'w': w_conditioned\n        }\n\n\nclass MappingNetwork(nn.Module):\n    \"\"\"StyleGAN2-style Mapping Network\"\"\"\n    \n    def __init__(self, z_dim: int = 512, w_dim: int = 512, num_layers: int = 8):\n        super().__init__()\n        layers = []\n        for i in range(num_layers):\n            in_dim = z_dim if i == 0 else w_dim\n            layers.extend([\n                nn.Linear(in_dim, w_dim),\n                nn.LeakyReLU(0.2)\n            ])\n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, z):\n        # Normalize input to improve numerical stability\n        z = F.normalize(z, dim=1)\n        return self.net(z)\n\n\nclass TriplaneBackbone(nn.Module):\n    \"\"\"Backbone for Tri-plane generation\"\"\"\n    \n    def __init__(self, w_dim: int, channels: int, resolution: int):\n        super().__init__()\n        self.w_dim = w_dim\n        self.channels = channels\n        self.resolution = resolution\n        \n        # Synthesis network inspired by StyleGAN2\n        self.const = nn.Parameter(torch.randn(1, channels, 4, 4) * 0.01)\n        \n        self.blocks = nn.ModuleList([\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n        ])\n        \n        # Output: 64x64 with channels aligned for super-resolution\n        self.to_features = nn.Conv2d(channels, channels, 1)\n    \n    def forward(self, w):\n        B = w.shape[0]\n        x = self.const.repeat(B, 1, 1, 1)\n        \n        for block in self.blocks:\n            x = block(x, w)\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        \n        # Final output: [B, channels, 64, 64]\n        features = self.to_features(x)\n        return features\n\n\nclass StyleBlock(nn.Module):\n    \"\"\"StyleGAN2-style Synthesis Block\"\"\"\n    \n    def __init__(self, in_ch, out_ch, w_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.style1 = nn.Linear(w_dim, in_ch)\n        self.style2 = nn.Linear(w_dim, out_ch)\n        self.noise1 = NoiseInjection()\n        self.noise2 = NoiseInjection()\n        self.activation = nn.LeakyReLU(0.2)\n    \n    def forward(self, x, w):\n        # First conv + style modulation\n        s1 = self.style1(w).unsqueeze(-1).unsqueeze(-1)\n        # Stabilize style modulation (add a small value)\n        x = self.conv1(x * (s1 + 1.0))\n        x = self.noise1(x)\n        x = self.activation(x)\n        \n        # Second conv + style modulation\n        s2 = self.style2(w).unsqueeze(-1).unsqueeze(-1)\n        x = self.conv2(x * (s2 + 1.0))\n        x = self.noise2(x)\n        x = self.activation(x)\n        \n        return x\n\n\nclass NoiseInjection(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, x):\n        noise = torch.randn(x.shape[0], 1, x.shape[2], x.shape[3], device=x.device)\n        # Keep noise influence small\n        return x + self.weight * noise * 0.1\n\n\nclass SuperResolutionModule(nn.Module):\n    \"\"\"Super-resolution module: 64x64 -> 512x512\"\"\"\n    \n    def __init__(self, in_channels, output_resolution=512):\n        super().__init__()\n        # 64x64 -> 512x512 requires 3 upsampling steps (2^3 = 8x)\n        self.conv_blocks = nn.Sequential(\n            # 64x64 -> 128x128\n            nn.Conv2d(in_channels, 128, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            # 128x128 -> 256x256\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            # 256x256 -> 512x512\n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            # Final conv to RGB\n            nn.Conv2d(32, 3, 3, padding=1),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        return self.conv_blocks(x)\n\n\nclass Discriminator(nn.Module):\n    \"\"\"StyleGAN2-style Discriminator\"\"\"\n    \n    def __init__(self, resolution: int = 512, channels: int = 3):\n        super().__init__()\n        \n        # Progressive discriminator blocks\n        self.blocks = nn.ModuleList([\n            DiscriminatorBlock(channels, 64),\n            DiscriminatorBlock(64, 128),\n            DiscriminatorBlock(128, 256),\n            DiscriminatorBlock(256, 512),\n            DiscriminatorBlock(512, 512),\n        ])\n        \n        # Adaptive pooling to ensure consistent output size\n        self.pool = nn.AdaptiveAvgPool2d((4, 4))\n        \n        self.final = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512 * 4 * 4, 1)\n        )\n        \n        # Weight initialization\n        self.apply(self._init_weights)\n    \n    def _init_weights(self, m):\n        \"\"\"Proper weight initialization\"\"\"\n        if isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.Conv2d):\n            nn.init.normal_(m.weight, 0, 0.02)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        for block in self.blocks:\n            x = block(x)\n        x = self.pool(x)  # Ensure 4x4 spatial size\n        return self.final(x)\n\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.activation = nn.LeakyReLU(0.2)\n        self.downsample = nn.AvgPool2d(2)\n    \n    def forward(self, x):\n        x = self.activation(self.conv1(x))\n        x = self.activation(self.conv2(x))\n        x = self.downsample(x)\n        return x\n\n\n# ==================== Loss Functions ====================\n\nclass AdaptiveAugmentation:\n    \"\"\"ADA (Adaptive Discriminator Augmentation)\"\"\"\n    \n    def __init__(self, target_accuracy=0.6, adjustment_speed=0.001):\n        self.target = target_accuracy\n        self.speed = adjustment_speed\n        self.p = 0.0\n        self.accuracy_ema = 0.5\n    \n    def update(self, real_acc):\n        \"\"\"Adjust augmentation strength based on Discriminator accuracy\"\"\"\n        self.accuracy_ema = 0.99 * self.accuracy_ema + 0.01 * real_acc\n        \n        if self.accuracy_ema > self.target:\n            self.p = min(1.0, self.p + self.speed)\n        else:\n            self.p = max(0.0, self.p - self.speed)\n    \n    def __call__(self, images):\n        \"\"\"Apply random augmentations\"\"\"\n        if self.p == 0.0:\n            return images\n        \n        B, C, H, W = images.shape\n        \n        # Apply random transformations\n        if torch.rand(1).item() < self.p:\n            # Horizontal flip\n            if torch.rand(1).item() < 0.5:\n                images = torch.flip(images, [3])\n            \n            # 90-degree rotations\n            if torch.rand(1).item() < 0.25:\n                k = torch.randint(1, 4, (1,)).item()\n                images = torch.rot90(images, k, [2, 3])\n        \n        return images\n\n\ndef r1_regularization(d_real, real_images):\n    \"\"\"R1 Gradient Penalty\"\"\"\n    grad_real = torch.autograd.grad(\n        outputs=d_real.sum(),\n        inputs=real_images,\n        create_graph=True,\n        only_inputs=True\n    )[0]\n    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()\n    return grad_penalty\n\n\ndef density_regularization(triplane, lambda_den=0.1):\n    \"\"\"Density Regularization for Tri-plane\"\"\"\n    # Regularize the density in the tri-plane\n    density_loss = torch.abs(triplane).mean()\n    return lambda_den * density_loss\n\n\ndef check_nan(tensor, name=\"tensor\"):\n    \"\"\"Helper function for NaN checks\"\"\"\n    if torch.isnan(tensor).any() or torch.isinf(tensor).any():\n        print(f\"WARNING: NaN or Inf detected in {name}\")\n        return True\n    return False\n\n\n# ==================== Training Loop ====================\n\nclass Next3DTrainer:\n    \"\"\"Trainer for Next3D Fine-tuning\"\"\"\n    \n    def __init__(\n        self,\n        generator: nn.Module,\n        discriminator: nn.Module,\n        device: str = 'cpu',\n        lr_g: float = 0.0002,  # Learning rate reduced by 1/10\n        lr_d: float = 0.0002,  # Learning rate reduced by 1/10\n        lambda_r1: float = 1.0,  # Weakened R1 regularization\n        lambda_den: float = 0.1,  # Weakened density regularization\n        batch_size: int = 1\n    ):\n        self.G = generator.to(device)\n        self.D = discriminator.to(device)\n        self.device = device\n        self.lambda_r1 = lambda_r1\n        self.lambda_den = lambda_den\n        \n        # Optimizers (more stable betas)\n        self.opt_g = torch.optim.Adam(self.G.parameters(), lr=lr_g, betas=(0.5, 0.999), eps=1e-8)\n        self.opt_d = torch.optim.Adam(self.D.parameters(), lr=lr_d, betas=(0.5, 0.999), eps=1e-8)\n        \n        # ADA\n        self.ada = AdaptiveAugmentation()\n        \n        # Gradient clipping threshold\n        self.grad_clip = 1.0\n        \n        # NaN detection counter\n        self.nan_count = 0\n        \n        print(f\"Trainer initialized on device: {device}\")\n        print(f\"  Learning rates: G={lr_g}, D={lr_d}\")\n        print(f\"  Regularization: R1={lambda_r1}, Density={lambda_den}\")\n        print(f\"  Generator parameters: {sum(p.numel() for p in self.G.parameters()):,}\")\n        print(f\"  Discriminator parameters: {sum(p.numel() for p in self.D.parameters()):,}\")\n    \n    def train_step(self, real_data: Dict[str, torch.Tensor]):\n        \"\"\"Single training step\"\"\"\n        real_images = real_data['image'].to(self.device)\n        B = real_images.shape[0]\n        \n        # ==================== Train Discriminator ====================\n        self.opt_d.zero_grad()\n        \n        # Real images\n        real_images_grad = real_images.clone().detach().requires_grad_(True)\n        real_aug = self.ada(real_images_grad)\n        d_real = self.D(real_aug)\n        \n        if check_nan(d_real, \"d_real\"):\n            return self._return_nan_metrics()\n        \n        # Fake images\n        z = torch.randn(B, self.G.z_dim, device=self.device) * 0.5  # Lower variance\n        \n        # Use 3DMM parameters if available\n        shape = real_data.get('shape', None)\n        exp = real_data.get('exp', None)\n        if shape is not None:\n            shape = shape.to(self.device)\n            exp = exp.to(self.device)\n        \n        with torch.no_grad():\n            fake_output = self.G(z, shape, exp)\n        fake_images = fake_output['image'].detach()\n        \n        if check_nan(fake_images, \"fake_images\"):\n            return self._return_nan_metrics()\n        \n        fake_aug = self.ada(fake_images)\n        d_fake = self.D(fake_aug)\n        \n        if check_nan(d_fake, \"d_fake\"):\n            return self._return_nan_metrics()\n        \n        # Adversarial loss (logistic loss)\n        d_loss = F.softplus(d_fake).mean() + F.softplus(-d_real).mean()\n        \n        # R1 regularization\n        r1_loss = r1_regularization(d_real, real_images_grad)\n        \n        if check_nan(r1_loss, \"r1_loss\"):\n            r1_loss = torch.tensor(0.0, device=self.device)\n        \n        d_loss_total = d_loss + self.lambda_r1 * r1_loss\n        \n        if check_nan(d_loss_total, \"d_loss_total\"):\n            return self._return_nan_metrics()\n        \n        d_loss_total.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(self.D.parameters(), self.grad_clip)\n        \n        self.opt_d.step()\n        \n        # Update ADA\n        real_acc = (d_real.detach() > 0).float().mean().item()\n        self.ada.update(real_acc)\n        \n        # ==================== Train Generator ====================\n        self.opt_g.zero_grad()\n        \n        z = torch.randn(B, self.G.z_dim, device=self.device) * 0.5  # Lower variance\n        fake_output = self.G(z, shape, exp)\n        fake_images = fake_output['image']\n        triplane = fake_output['triplane']\n        \n        if check_nan(fake_images, \"g_fake_images\"):\n            return self._return_nan_metrics()\n        \n        fake_aug = self.ada(fake_images)\n        d_fake = self.D(fake_aug)\n        \n        if check_nan(d_fake, \"g_d_fake\"):\n            return self._return_nan_metrics()\n        \n        # Adversarial loss\n        g_loss = F.softplus(-d_fake).mean()\n        \n        # Density regularization\n        den_loss = density_regularization(triplane, self.lambda_den)\n        \n        if check_nan(den_loss, \"den_loss\"):\n            den_loss = torch.tensor(0.0, device=self.device)\n        \n        g_loss_total = g_loss + den_loss\n        \n        if check_nan(g_loss_total, \"g_loss_total\"):\n            return self._return_nan_metrics()\n        \n        g_loss_total.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(self.G.parameters(), self.grad_clip)\n        \n        self.opt_g.step()\n        \n        return {\n            'd_loss': d_loss.item(),\n            'g_loss': g_loss.item(),\n            'r1_loss': r1_loss.item(),\n            'den_loss': den_loss.item(),\n            'ada_p': self.ada.p,\n            'real_acc': real_acc\n        }\n    \n    def _return_nan_metrics(self):\n        \"\"\"Return values when NaN occurs\"\"\"\n        self.nan_count += 1\n        if self.nan_count > 10:\n            print(\"\\n\" + \"=\"*60)\n            print(\"CRITICAL: Too many NaN occurrences!\")\n            print(\"Suggestions:\")\n            print(\"1. Reduce learning rate further (e.g., 0.0001)\")\n            print(\"2. Check your input data for NaN/Inf values\")\n            print(\"3. Use smaller batch size\")\n            print(\"4. Restart training from scratch\")\n            print(\"=\"*60 + \"\\n\")\n        \n        return {\n            'd_loss': 0.0,\n            'g_loss': 0.0,\n            'r1_loss': 0.0,\n            'den_loss': 0.0,\n            'ada_p': self.ada.p,\n            'real_acc': 0.0\n        }\n    \n    def save_checkpoint(self, path: str, epoch: int):\n        \"\"\"Save training checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'generator': self.G.state_dict(),\n            'discriminator': self.D.state_dict(),\n            'opt_g': self.opt_g.state_dict(),\n            'opt_d': self.opt_d.state_dict(),\n            'ada_p': self.ada.p,\n        }, path)\n        print(f\"Checkpoint saved: {path}\")\n    \n    def load_checkpoint(self, path: str):\n        \"\"\"Load training checkpoint\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.G.load_state_dict(checkpoint['generator'])\n        self.D.load_state_dict(checkpoint['discriminator'])\n        self.opt_g.load_state_dict(checkpoint['opt_g'])\n        self.opt_d.load_state_dict(checkpoint['opt_d'])\n        self.ada.p = checkpoint.get('ada_p', 0.0)\n        print(f\"Checkpoint loaded: {path}\")\n        return checkpoint['epoch']\n\n\n# ==================== Main Training Function ====================\n\ndef train_next3d(\n    data_dir: str = \"./output_styled\",\n    mesh_dir: Optional[str] = None,\n    output_dir: str = \"./next3d_checkpoints\",\n    pretrained_path: Optional[str] = None,\n    batch_size: int = 1,\n    num_epochs: int = 100,\n    save_every: int = 10,\n    device: str = 'cpu'\n):\n    \"\"\"\n    Main function for Next3D fine-tuning\n    \n    Args:\n        data_dir: Directory for styled images (output_styled)\n        mesh_dir: Directory for 3DMM parameters\n        output_dir: Output path for checkpoints\n        pretrained_path: Path to pre-trained model\n        batch_size: Batch size (CPU: 1-2, GPU 12GB: 2, GPU 24GB: 4-8)\n        num_epochs: Number of epochs\n        save_every: Interval for saving checkpoints\n        device: Target device ('cpu' or 'cuda')\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dataset\n    dataset = StyleFaceDataset(\n        image_dir=data_dir,\n        mesh_dir=mesh_dir,\n        resolution=512,\n        use_3dmm=(mesh_dir is not None)\n    )\n    \n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0 if device == 'cpu' else 2,  # CPU: 0, GPU: 2-4\n        pin_memory=False  # CPU doesn't need pin_memory\n    )\n    \n    # Model\n    generator = TriplaneGenerator(\n        z_dim=512,\n        w_dim=512,\n        use_3dmm=(mesh_dir is not None)\n    )\n    \n    discriminator = Discriminator(resolution=512)\n    \n    # Load pre-trained model\n    if pretrained_path and os.path.exists(pretrained_path):\n        print(f\"Loading pretrained model: {pretrained_path}\")\n        checkpoint = torch.load(pretrained_path, map_location=device)\n        generator.load_state_dict(checkpoint.get('generator', checkpoint))\n        print(\"Pretrained model loaded!\")\n    \n    # Trainer\n    trainer = Next3DTrainer(\n        generator=generator,\n        discriminator=discriminator,\n        device=device,\n        lr_g=0.0002,  # Reduced learning rate\n        lr_d=0.0002,  # Reduced learning rate\n        lambda_r1=1.0,  # Weakened R1\n        lambda_den=0.1,  # Weakened density loss\n        batch_size=batch_size\n    )\n    \n    # Training Loop\n    print(f\"\\n{'='*60}\")\n    print(f\"Starting Training on {device.upper()}\")\n    if device == 'cpu':\n        print(\"WARNING: Training on CPU will be significantly slower!\")\n        print(\"Consider using GPU for faster training.\")\n    print(f\"{'='*60}\\n\")\n    \n    for epoch in range(num_epochs):\n        pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        epoch_metrics = {\n            'd_loss': 0, 'g_loss': 0, \n            'r1_loss': 0, 'den_loss': 0\n        }\n        \n        valid_batches = 0\n        \n        for i, batch in enumerate(pbar):\n            metrics = trainer.train_step(batch)\n            \n            # Count non-NaN batches\n            if metrics['d_loss'] != 0.0 or metrics['g_loss'] != 0.0:\n                valid_batches += 1\n                # Aggregate metrics\n                for k in epoch_metrics:\n                    epoch_metrics[k] += metrics[k]\n            \n            # Update progress bar\n            if i % 10 == 0:\n                pbar.set_postfix({\n                    'D': f\"{metrics['d_loss']:.4f}\",\n                    'G': f\"{metrics['g_loss']:.4f}\",\n                    'ADA': f\"{metrics['ada_p']:.3f}\"\n                })\n        \n        # End of epoch statistics\n        if valid_batches > 0:\n            print(f\"\\nEpoch {epoch+1} Completed (Valid batches: {valid_batches}/{len(dataloader)}):\")\n            print(f\"  D Loss: {epoch_metrics['d_loss']/valid_batches:.4f}\")\n            print(f\"  G Loss: {epoch_metrics['g_loss']/valid_batches:.4f}\")\n            print(f\"  R1 Loss: {epoch_metrics['r1_loss']/valid_batches:.4f}\")\n            print(f\"  Density Loss: {epoch_metrics['den_loss']/valid_batches:.4f}\")\n        else:\n            print(f\"\\nEpoch {epoch+1}: All batches failed with NaN!\")\n        \n        # Save checkpoint\n        if (epoch + 1) % save_every == 0:\n            save_path = os.path.join(output_dir, f\"checkpoint_{epoch+1:04d}.pt\")\n            trainer.save_checkpoint(save_path, epoch+1)\n    \n    # Save final model\n    final_path = os.path.join(output_dir, \"final_model.pt\")\n    trainer.save_checkpoint(final_path, num_epochs)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"Training Complete!\")\n    print(f\"Final model saved at: {final_path}\")\n    print(f\"{'='*60}\\n\")\n\n\nif __name__ == \"__main__\":\n    # Settings\n    DATA_DIR = \"./output_styled\"  # Output from step 1\n    MESH_DIR = None  # 3DMM parameters (if available)\n    OUTPUT_DIR = \"./next3d_checkpoints\"\n    PRETRAINED_PATH = None  # Path to Next3D pre-trained on FFHQ\n    \n    # CPU/GPU auto-detection\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    # Recommended settings based on device\n    if DEVICE == 'cpu':\n        BATCH_SIZE = 1  # Minimal batch size for CPU\n        NUM_EPOCHS = 10  # Fewer epochs for testing on CPU\n        print(\"Running on CPU - using minimal settings\")\n    else:\n        BATCH_SIZE = 4  # GPU settings (adjust based on VRAM)\n        NUM_EPOCHS = 100\n        print(\"Running on GPU - using standard settings\")\n    \n    # Execute training\n    train_next3d(\n        data_dir=DATA_DIR,\n        mesh_dir=MESH_DIR,\n        output_dir=OUTPUT_DIR,\n        pretrained_path=PRETRAINED_PATH,\n        batch_size=BATCH_SIZE,\n        num_epochs=NUM_EPOCHS,\n        device=DEVICE\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef show_image(image_dir):\n    image_paths = [\n        os.path.join(image_dir, f)\n        for f in sorted(os.listdir(image_dir))\n        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n    ][:6]  \n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    axes = axes.flatten()\n    for ax, img_path in zip(axes, image_paths):\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis(\"off\")\n        ax.set_title(os.path.basename(img_path), fontsize=9)\n    for ax in axes[len(image_paths):]:\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nshow_image('output_styled')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}