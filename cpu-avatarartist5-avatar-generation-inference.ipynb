{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":288323840,"sourceType":"kernelVersion"},{"sourceId":288328029,"sourceType":"kernelVersion"},{"sourceId":288334056,"sourceType":"kernelVersion"},{"sourceId":288342624,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CPU AvatarArtist5: Avatar Generation & Inference**","metadata":{}},{"cell_type":"markdown","source":"https://kumapowerliu.github.io/AvatarArtist/","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **Step 5 Pipeline Explanation: 3D Avatar Generation & Inference**\n\n## Overview\nThis is the **final inference pipeline** that brings everything together. It takes your **trained DiT model from Step 4** and a **2D avatar image**, then generates a complete **3D avatar** that can be viewed from any angle, animated, and exported as a 3D mesh file.\n\n---\n\n## Pipeline Flow\n\n```\nInput: 2D Avatar Image (512√ó512)\n   ‚Üì\n[DiT Model] Diffusion Sampling (1000 steps ‚Üí 50 steps)\n   ‚Üì\nOutput: 3D Triplane (96√ó256√ó256)\n   ‚Üì\n[Triplane Decoder] Volume Rendering\n   ‚Üì\nOutput: 3D Volume (64√ó64√ó64 density grid)\n   ‚Üì\n[Marching Cubes] Surface Extraction\n   ‚Üì\nOutput: 3D Mesh (vertices + faces)\n   ‚Üì\nSave: OBJ file, Visualizations\n```\n\n---\n\n## Key Components\n\n### **1. Model Loading & Initialization**\n\n```python\nAvatarGenerator(\n    model_path=\"dit_checkpoints/best_model.pt\",\n    device='cuda',\n    model_config=None  # Auto-detected\n)\n```\n\n**Auto-Detection Logic:**\n- Counts model parameters\n- **< 50M params** ‚Üí Small model (384-dim, 6 layers)\n- **> 50M params** ‚Üí Standard model (768-dim, 12 layers)\n\n**Loaded Components:**\n- **DiT Model**: Trained diffusion transformer\n- **Diffusion Scheduler**: 1000-step noise schedule\n- **Mesh Generator**: Triplane ‚Üí Volume ‚Üí Mesh converter\n- **Image Preprocessor**: Resize, normalize to [-1, 1]\n\n---\n\n### **2. Diffusion Sampling Process**\n\n#### **What Happens:**\n```python\n# Start: Random noise [1, 96, 256, 256]\nx_T = torch.randn(1, 96, 256, 256)\n\n# Iteratively denoise\nfor t in [1000, 980, 960, ..., 20, 0]:  # 50 steps\n    noise_pred = model(x_t, t, condition_image)\n    x_{t-1} = remove_noise(x_t, noise_pred)\n\n# Final: Clean triplane [1, 96, 256, 256]\n```\n\n**Key Parameters:**\n\n| Parameter | Default | Fast | High Quality | Purpose |\n|-----------|---------|------|--------------|---------|\n| `num_sampling_steps` | 50 | 25 | 100-250 | Denoising iterations |\n| `seed` | None | 42 | Random | Reproducibility |\n\n**Time vs Quality Trade-off:**\n- **25 steps**: 5-10 seconds, preview quality\n- **50 steps**: 15-20 seconds, good quality (recommended)\n- **100 steps**: 30-40 seconds, high quality\n- **250 steps**: 1-2 minutes, maximum quality\n\n---\n\n### **3. Triplane Structure**\n\n#### **Understanding Triplanes:**\n\nA triplane is a **3D representation using three 2D planes**:\n\n```\nTriplane [96 channels, 256√ó256]\n‚îú‚îÄ XY Plane [32 channels]: Front view features\n‚îú‚îÄ XZ Plane [32 channels]: Top view features\n‚îî‚îÄ YZ Plane [32 channels]: Side view features\n```\n\n**Why Three Planes?**\n- **Efficient**: 3√ó256√ó256 << 256√ó256√ó256 (3D volume)\n- **Complete**: Any 3D point can be projected onto all three planes\n- **Expressive**: Captures 3D structure with 2D operations\n\n**Channel Decomposition:**\n```python\nStatic Channels (67): Identity features\n  - Face shape\n  - Bone structure\n  - Skin texture\n  - Hair style (unchanging)\n\nDynamic Channels (29): Expression/Pose features\n  - Facial expressions\n  - Eye gaze\n  - Mouth movement\n  - Head rotation\n```\n\n---\n\n### **4. Triplane ‚Üí Volume Conversion**\n\n#### **Process:**\n\n```python\ntriplane_to_volume(triplane, grid_size=64)\n```\n\n**Step-by-Step:**\n\n1. **Create 3D Grid**\n   ```python\n   # Generate 64√ó64√ó64 sampling points in [-1, 1]¬≥\n   coords = create_grid(64)  # [262,144 points]\n   ```\n\n2. **Sample from Each Plane**\n   ```python\n   # For each 3D point (x, y, z):\n   feat_xy = sample_plane(xy_plane, (x, y))\n   feat_xz = sample_plane(xz_plane, (x, z))\n   feat_yz = sample_plane(yz_plane, (y, z))\n   ```\n\n3. **Aggregate Features**\n   ```python\n   density = feat_xy + feat_xz + feat_yz\n   density = sigmoid(density)  # ‚Üí [0, 1]\n   ```\n\n4. **Reshape to Volume**\n   ```python\n   volume = density.reshape(64, 64, 64)\n   ```\n\n**Volume Interpretation:**\n- **Value 0.0**: Empty space (air)\n- **Value 0.5**: Surface boundary\n- **Value 1.0**: Solid material (inside head)\n\n---\n\n### **5. Mesh Extraction (Marching Cubes)**\n\n#### **Algorithm:**\n\n**Marching Cubes** is a classic algorithm that converts volume density into triangle meshes:\n\n```python\nextract_mesh(volume, threshold=0.5)\n```\n\n**How It Works:**\n\n1. **Scan the Volume**\n   - Process each 2√ó2√ó2 cube of voxels\n   - 8 corner vertices per cube\n\n2. **Classify Corners**\n   ```\n   Corner > threshold ‚Üí Inside surface\n   Corner < threshold ‚Üí Outside surface\n   ```\n\n3. **Generate Triangles**\n   - Look up triangle configuration (256 possible cases)\n   - Place vertices on edges where surface crosses\n   - Connect vertices to form triangles\n\n4. **Output Mesh**\n   ```python\n   vertices: [N, 3]  # 3D coordinates\n   faces: [M, 3]     # Triangle indices\n   ```\n\n**Threshold Selection:**\n\n| Threshold | Effect | Use Case |\n|-----------|--------|----------|\n| Low (30th percentile) | Thicker mesh, more detail | High-quality models |\n| Medium (50th percentile) | Balanced | Default |\n| High (70th percentile) | Thinner mesh, cleaner | Simplified models |\n\n**Adaptive Strategy:**\n```python\n# Try multiple thresholds automatically\nfor percentile in [50, 60, 70, 80]:\n    threshold = np.percentile(volume, percentile)\n    try:\n        vertices, faces = marching_cubes(volume, threshold)\n        break  # Success!\n    except:\n        continue  # Try next threshold\n```\n\n---\n\n## Output Files Explained\n\n### **Generated Files:**\n\n```\navatar_results/\n‚îú‚îÄ‚îÄ input_image.png              # Original 2D image\n‚îú‚îÄ‚îÄ triplane_visualization.png   # Three projection planes\n‚îú‚îÄ‚îÄ volume_slices.png            # 3D volume cross-sections\n‚îî‚îÄ‚îÄ avatar_mesh.obj              # 3D mesh (importable)\n```\n\n---\n\n### **1. input_image.png**\n- **Content**: Your original 2D avatar image\n- **Purpose**: Reference for comparison\n\n---\n\n### **2. triplane_visualization.png**\n\n**Shows three 2D feature maps:**\n\n```\n[XY Plane]  [XZ Plane]  [YZ Plane]\n   Front       Top         Side\n```\n\n**What Each Plane Represents:**\n- **Bright areas**: High activation (important features)\n- **Dark areas**: Low activation (background/empty)\n\n**Quality Indicators:**\n\n‚úÖ **Good Triplane:**\n- Clear facial structure visible\n- Symmetrical patterns (left-right balance)\n- Smooth gradients (no noise)\n- Distinct features (eyes, nose, mouth regions)\n\n‚ùå **Poor Triplane:**\n- Random noise patterns\n- No recognizable structure\n- All black or all white (collapsed)\n- Checkerboard artifacts\n\n**Example Interpretation:**\n```\nXY Plane: Should show face outline, eye positions\nXZ Plane: Should show head top-to-bottom structure\nYZ Plane: Should show side profile of face\n```\n\n---\n\n### **3. volume_slices.png**\n\n**Shows three cross-sections of the 3D volume:**\n\n```\n[Slice X]   [Slice Y]   [Slice Z]\n  Sagittal    Coronal    Transverse\n```\n\n**What to Look For:**\n\n‚úÖ **Healthy Volume:**\n- **Slice X (Left-Right)**: Oval shape (head from side)\n- **Slice Y (Front-Back)**: Circular shape (head from front)\n- **Slice Z (Top-Bottom)**: Circular shape (head from top)\n- Smooth boundaries, no holes\n- Clear distinction between inside (bright) and outside (dark)\n\n‚ùå **Problem Volume:**\n- Irregular shapes\n- Multiple disconnected regions\n- Uniform gray (no structure)\n- Sharp discontinuities\n\n**Density Interpretation:**\n```\nWhite (1.0): Solid (inside head)\nGray (0.5):  Surface boundary\nBlack (0.0): Empty space\n```\n\n---\n\n### **4. avatar_mesh.obj**\n\n**Standard 3D file format containing:**\n```obj\nv -0.5 0.3 0.2    # Vertex positions\nv 0.1 -0.2 0.4\n...\nf 1 2 3           # Triangle faces (vertex indices)\nf 4 5 6\n...\n```\n\n**Usage:**\n- Import into **Blender**, **Maya**, **3ds Max**\n- View in online viewers (e.g., https://3dviewer.net/)\n- Use in game engines (**Unity**, **Unreal**)\n- 3D print preparation\n\n**Typical Mesh Stats:**\n\n| Quality | Vertices | Faces | File Size | Detail Level |\n|---------|----------|-------|-----------|--------------|\n| Low | 5,000 | 10,000 | 500 KB | Preview |\n| Medium | 15,000 | 30,000 | 1.5 MB | Standard |\n| High | 50,000 | 100,000 | 5 MB | High detail |\n\n---\n\n## Diagnostic Features\n\n### **Model Diagnostics**\n\n```python\ngenerator.diagnose_model()\n```\n\n**Checks Performed:**\n\n1. **Parameter Count**\n   ```\n   Total parameters: 40,123,456\n   Trainable parameters: 40,123,456\n   ```\n\n2. **NaN Detection in Weights**\n   ```\n   ‚úì No NaN values in model weights\n   ```\n   OR\n   ```\n   ‚ö†Ô∏è WARNING: NaN found in model.blocks.3.mlp.0.weight\n   ```\n\n3. **Test Forward Pass**\n   ```\n   Output shape: [1, 96, 256, 256]\n   Output stats: min=-2.34, mean=0.12, max=3.45\n   ‚úì Model forward pass successful\n   ```\n\n**Interpretation:**\n\n| Check Result | Meaning | Action |\n|--------------|---------|--------|\n| No NaN in weights | ‚úÖ Model trained correctly | Proceed |\n| NaN in weights | ‚ùå Training failed | Retrain Step 4 |\n| Normal output range | ‚úÖ Model producing valid results | Proceed |\n| NaN in output | ‚ùå Model broken | Check checkpoint |\n| Output all zeros | ‚ö†Ô∏è Model not learning | Retrain longer |\n\n---\n\n## Common Issues & Solutions\n\n### **Problem 1: NaN in Triplane**\n\n**Symptoms:**\n```\n‚ö†Ô∏è WARNING: Triplane contains NaN values!\nNaN count: 12,543\n```\n\n**Causes:**\n1. Model didn't train properly (Step 4 loss didn't converge)\n2. Using wrong checkpoint\n3. Numerical instability during sampling\n\n**Solutions:**\n```python\n# 1. Try different checkpoint\nMODEL_PATH = \"dit_checkpoints/checkpoint_0080.pt\"  # Earlier epoch\n\n# 2. Reduce sampling steps\nNUM_SAMPLING_STEPS = 25  # Less chance for instability\n\n# 3. Use different seed\nSEED = 123  # Try multiple seeds\n\n# 4. If all fail ‚Üí Retrain Step 4 with:\n#    - Lower learning rate (lr=5e-5)\n#    - More epochs (num_epochs=150)\n#    - Check training loss < 0.05\n```\n\n---\n\n### **Problem 2: Empty or Incorrect Mesh**\n\n**Symptoms:**\n```\n‚ùå ERROR: RuntimeError in marching_cubes\n```\nOR\n```\nExtracted mesh: 0 vertices, 0 faces\n```\n\n**Causes:**\n1. Volume is all zeros or all ones (no surface)\n2. Threshold too high or too low\n3. Triplane didn't capture 3D structure\n\n**Solutions:**\n\n**A. Check Volume Statistics:**\n```python\nprint(f\"Volume min: {volume.min()}\")\nprint(f\"Volume max: {volume.max()}\")\nprint(f\"Volume mean: {volume.mean()}\")\n```\n\n| Statistics | Interpretation | Action |\n|------------|----------------|--------|\n| min=max | Uniform volume, no structure | Retrain model |\n| All > 0.9 | All solid | Lower threshold |\n| All < 0.1 | All empty | Raise threshold |\n| Good range [0, 1] | Normal | Adjust threshold |\n\n**B. Manual Threshold:**\n```python\n# Try lower threshold\nvertices, faces = mesh_generator.extract_mesh(volume, threshold=0.3)\n\n# Try higher threshold\nvertices, faces = mesh_generator.extract_mesh(volume, threshold=0.7)\n```\n\n**C. Use Fallback Volume:**\nIf the code creates a \"fallback spherical volume\", it means:\n- Triplane was invalid (NaN/Inf)\n- Model output is not meaningful\n- **Must retrain Step 4**\n\n---\n\n### **Problem 3: Mesh Doesn't Resemble Input**\n\n**Symptoms:**\n- Mesh looks random/blobby\n- No facial features visible\n- Generic sphere instead of face\n\n**Causes:**\n1. **Model underfitted** (Step 4 trained too few epochs)\n2. **Model overfitted** (memorized training set, can't generalize)\n3. **Input image very different** from training style\n\n**Solutions:**\n\n**A. Check Training Loss:**\n```\nIf Step 4 final loss > 0.08:\n  ‚Üí Underfitted, train 50-100 more epochs\n  \nIf train loss < 0.02 but val loss > 0.10:\n  ‚Üí Overfitted, use earlier checkpoint or get more data\n```\n\n**B. Use Training Set Image:**\n```python\n# Test with an image from training data\nTEST_IMAGE = \"dit_training_data/images/image_000001.png\"\n```\nIf this works but new images don't ‚Üí Model overfitted\n\n**C. Check Input Similarity:**\n- Input must match **training style** from Step 1\n- If using realistic photo instead of avatar ‚Üí Won't work\n- Solution: Run Step 1 style transfer on new image first\n\n---\n\n### **Problem 4: Slow Generation**\n\n**Expected Times:**\n\n| Device | 50 Steps | 100 Steps | 250 Steps |\n|--------|----------|-----------|-----------|\n| CPU | 10-20 min | 20-40 min | 1-2 hours |\n| GTX 1080 | 30 sec | 60 sec | 2-3 min |\n| RTX 3090 | 15 sec | 30 sec | 60-90 sec |\n| A100 | 10 sec | 20 sec | 40-50 sec |\n\n**Speed Optimization:**\n\n```python\n# 1. Reduce steps (quality vs speed)\nNUM_SAMPLING_STEPS = 25  # 2√ó faster, slight quality loss\n\n# 2. Use smaller grid (faster mesh)\nvolume = mesh_generator.triplane_to_volume(triplane, grid_size=48)  # vs 64\n\n# 3. Half precision (2√ó faster, GPU only)\nmodel = model.half()\nimage_tensor = image_tensor.half()\n```\n\n---\n\n### **Problem 5: Out of Memory**\n\n**Symptoms:**\n```\nRuntimeError: CUDA out of memory\n```\n\n**Solutions:**\n\n```python\n# 1. Reduce grid size\ngrid_size = 48  # vs 64 (uses 50% less memory)\n\n# 2. Use CPU for mesh generation\nvolume = mesh_generator.triplane_to_volume(\n    triplane.cpu(), \n    grid_size=64\n)\n\n# 3. Clear cache after generation\ntorch.cuda.empty_cache()\n```\n\n---\n\n## Result Quality Assessment\n\n### **Excellent Results:**\n\n‚úÖ **Triplane Visualization:**\n- Clear three-plane structure\n- Recognizable facial features in all views\n- Smooth gradients, no artifacts\n\n‚úÖ **Volume Slices:**\n- Closed, continuous shapes\n- Symmetric (left-right balance)\n- Clear inside/outside boundary\n\n‚úÖ **3D Mesh:**\n- 15,000+ vertices\n- Smooth surface\n- Resembles input image from front view\n- Realistic from all angles\n\n---\n\n### **Poor Results:**\n\n‚ùå **Triplane:**\n- Random noise\n- No structure\n- All uniform color\n\n‚ùå **Volume:**\n- Disconnected regions\n- No clear surface\n- Uniform gray\n\n‚ùå **Mesh:**\n- < 1,000 vertices\n- Irregular shape\n- No resemblance to input\n\n**‚Üí Action: Retrain Step 4 or use earlier checkpoint**\n\n---\n\n## Advanced Usage\n\n### **Batch Generation**\n\n```python\n# Generate multiple avatars\nimages = [\n    \"avatar1.png\",\n    \"avatar2.png\",\n    \"avatar3.png\"\n]\n\nfor i, img_path in enumerate(images):\n    results = generator.generate_from_image(\n        image_path=img_path,\n        num_sampling_steps=50,\n        seed=42 + i\n    )\n    generator.visualize_results(\n        results, \n        save_dir=f\"avatar_results_{i}\"\n    )\n```\n\n### **Expression Control** (Future Enhancement)\n\n```python\n# Separate static and dynamic\nstatic = triplane[:67]   # Identity (fixed)\ndynamic = triplane[67:]  # Expression (modify)\n\n# Amplify expression\ndynamic_amplified = dynamic * 1.5\n\n# Create new triplane\ntriplane_new = torch.cat([static, dynamic_amplified], dim=0)\n\n# Generate volume with new expression\nvolume_new = mesh_generator.triplane_to_volume(\n    triplane_new.unsqueeze(0), \n    grid_size=64\n)\n```\n\n### **Multi-View Rendering** (Future Enhancement)\n\n```python\n# Generate from different angles\nfor angle in [0, 45, 90, 135, 180]:\n    # Rotate triplane or camera\n    # Render view\n    # Save image\n```\n\n---\n\n## Integration with 3D Software\n\n### **Blender Import:**\n\n```python\n# In Blender Python console:\nimport bpy\n\nbpy.ops.import_scene.obj(filepath=\"avatar_results/avatar_mesh.obj\")\n\n# Add material\nmesh = bpy.context.active_object\nmat = bpy.data.materials.new(name=\"AvatarMaterial\")\nmesh.data.materials.append(mat)\n\n# Add smooth shading\nbpy.ops.object.shade_smooth()\n```\n\n### **Unity Import:**\n\n1. Copy `avatar_mesh.obj` to `Assets/Models/`\n2. Drag into scene\n3. Add materials and textures\n4. Attach animation controller\n\n---\n\n## Summary\n\nThis pipeline:\n\n‚úÖ **Takes** a 2D avatar image  \n‚úÖ **Generates** a 3D triplane using trained DiT model  \n‚úÖ **Converts** triplane to 3D volume  \n‚úÖ **Extracts** surface mesh using Marching Cubes  \n‚úÖ **Outputs** OBJ file + visualizations  \n‚úÖ **Includes** extensive diagnostics and error handling\n\n**Quality Indicators:**\n- **Triplane**: Shows clear 3-plane structure\n- **Volume**: Closed shapes in all three slices\n- **Mesh**: 15,000+ vertices, smooth surface\n- **Resemblance**: Front view matches input image\n\n**Performance:**\n- **GPU (RTX 3090)**: ~15 seconds for 50-step generation\n- **CPU**: ~10-15 minutes (not recommended)\n\n**Use Cases:**\n- Create 3D avatars from 2D concept art\n- Generate game assets\n- VR/AR avatar creation\n- 3D printing preparation\n- Animation and rigging base\n\n**Final Note**: Quality depends entirely on Step 4 training. If results are poor, check Step 4 training metrics (loss should be < 0.05) and consider retraining with more data or epochs.\n\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n!pip install diffusers transformers accelerate\n!pip install controlnet-aux opencv-python pillow\n!pip install mediapipe==0.10.9","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-12-23T02:53:16.646178Z","iopub.execute_input":"2025-12-23T02:53:16.646528Z","iopub.status.idle":"2025-12-23T02:55:20.4819Z","shell.execute_reply.started":"2025-12-23T02:53:16.6465Z","shell.execute_reply":"2025-12-23T02:55:20.480172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#final_model_path = '/kaggle/input/avatarartist-next3d-4d-gan-fine-tuning/next3d_checkpoints/final_model.pt'\ngd_path = '/kaggle/input/cpu-avatarartist1-2d-domain-transfer'\ndit_checkpoints = '/kaggle/input/cpu-avatarartist2-next3d-4d-gan-fine-tuning/next3d_checkpoints'\ndit_training_data = '/kaggle/input/cpu-avatarartist3-triplane-decomposition/dit_training_data'\nbest_model_path = '/kaggle/input/cpu-avatarartist4-diffusion-transformer-train/dit_checkpoints/best_model.pt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n3D Avatar Generation & Inference Script\nGenerates a 3D avatar from a 2D image using a trained DiT model.\n\nFeatures:\n1. 2D Image ‚Üí 3D Triplane prediction\n2. Triplane ‚Üí 3D Mesh generation\n3. Expression animation generation\n4. Visualization and saving of results\n\"\"\"\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport json\nimport cv2\nfrom einops import rearrange, repeat\nimport math\n\n\n# ==================== DiT Model Imports (from Training Script) ====================\n\nclass TimestepEmbedding(nn.Module):\n    \"\"\"Sinusoidal Embedding for time steps\"\"\"\n\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.SiLU(),\n            nn.Linear(dim * 4, dim)\n        )\n\n    def forward(self, timesteps):\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n        emb = timesteps[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n        return self.mlp(emb)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Split image into patches and embed them\"\"\"\n\n    def __init__(self, img_size=256, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = rearrange(x, 'b d h w -> b (h w) d')\n        return x\n\n\nclass DiTBlock(nn.Module):\n    \"\"\"DiT Transformer Block\"\"\"\n\n    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, dim)\n        )\n\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(dim, 6 * dim)\n        )\n\n    def forward(self, x, c):\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \\\n            self.adaLN_modulation(c).chunk(6, dim=-1)\n\n        h = self.norm1(x)\n        h = h * (1 + scale_msa.unsqueeze(1)) + shift_msa.unsqueeze(1)\n        h, _ = self.attn(h, h, h)\n        x = x + gate_msa.unsqueeze(1) * h\n\n        h = self.norm2(x)\n        h = h * (1 + scale_mlp.unsqueeze(1)) + shift_mlp.unsqueeze(1)\n        h = self.mlp(h)\n        x = x + gate_mlp.unsqueeze(1) * h\n\n        return x\n\n\nclass FinalLayer(nn.Module):\n    \"\"\"Final Output Layer\"\"\"\n\n    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n        super().__init__()\n        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False)\n        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(hidden_size, 2 * hidden_size)\n        )\n\n    def forward(self, x, c):\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=-1)\n        x = self.norm_final(x) * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n        x = self.linear(x)\n        return x\n\n\nclass DiffusionTransformer(nn.Module):\n    \"\"\"Diffusion Transformer (DiT)\"\"\"\n\n    def __init__(\n        self,\n        img_size: int = 256,\n        patch_size: int = 16,\n        in_channels: int = 96,\n        condition_channels: int = 3,\n        hidden_size: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0\n    ):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.out_channels = in_channels\n        self.num_patches = (img_size // patch_size) ** 2\n\n        self.x_embedder = PatchEmbed(img_size, patch_size, in_channels, hidden_size)\n        self.c_embedder = PatchEmbed(img_size, patch_size, condition_channels, hidden_size)\n        self.t_embedder = TimestepEmbedding(hidden_size)\n\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, hidden_size))\n\n        self.blocks = nn.ModuleList([\n            DiTBlock(hidden_size, num_heads, mlp_ratio)\n            for _ in range(depth)\n        ])\n\n        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        def _basic_init(module):\n            if isinstance(module, nn.Linear):\n                torch.nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n        self.apply(_basic_init)\n        nn.init.normal_(self.pos_embed, std=0.02)\n        nn.init.constant_(self.final_layer.linear.weight, 0)\n        nn.init.constant_(self.final_layer.linear.bias, 0)\n\n    def unpatchify(self, x):\n        p = self.patch_size\n        h = w = int(x.shape[1] ** 0.5)\n        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)', h=h, p1=p, p2=p)\n        return x\n\n    def forward(self, x, t, c):\n        x = self.x_embedder(x) + self.pos_embed\n        c_embed = self.c_embedder(c)\n        t_embed = self.t_embedder(t)\n\n        x = x + c_embed\n        c = t_embed\n\n        for block in self.blocks:\n            x = block(x, c)\n\n        x = self.final_layer(x, c)\n        x = self.unpatchify(x)\n\n        return x\n\n\nclass GaussianDiffusion:\n    \"\"\"DDPM Gaussian Diffusion Implementation\"\"\"\n\n    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, device='cuda'):\n        self.timesteps = timesteps\n        self.device = device\n\n        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        self.posterior_variance = (\n            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n\n    @torch.no_grad()\n    def p_sample(self, model, x, t, condition):\n        predicted_noise = model(x, t, condition)\n\n        alpha = self.alphas[t][:, None, None, None]\n        alpha_cumprod = self.alphas_cumprod[t][:, None, None, None]\n        beta = self.betas[t][:, None, None, None]\n\n        pred_x0 = (x - torch.sqrt(1 - alpha_cumprod) * predicted_noise) / torch.sqrt(alpha_cumprod)\n\n        if t[0] > 0:\n            noise = torch.randn_like(x)\n            sigma = torch.sqrt(self.posterior_variance[t])[:, None, None, None]\n        else:\n            noise = 0\n            sigma = 0\n\n        mean = (1 / torch.sqrt(alpha)) * (x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise)\n\n        return mean + sigma * noise\n\n    @torch.no_grad()\n    def sample(self, model, shape, condition, num_steps=50):\n        \"\"\"\n        Fast sampling (DDIM-style skip)\n\n        Args:\n            num_steps: Number of sampling steps (fewer steps are faster)\n        \"\"\"\n        batch_size = shape[0]\n        device = condition.device\n\n        # Start from random noise\n        x = torch.randn(shape, device=device)\n\n        # Sample with skips\n        step_size = self.timesteps // num_steps\n        timesteps = list(range(0, self.timesteps, step_size))[::-1]\n\n        for i in tqdm(timesteps, desc='Generating 3D Avatar'):\n            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n            x = self.p_sample(model, x, t, condition)\n\n        return x\n\n\n# ==================== 3D Mesh Generation ====================\n\nclass TriplaneToMesh:\n    \"\"\"Generates 3D mesh from triplane representations\"\"\"\n\n    def __init__(self, resolution: int = 256):\n        self.resolution = resolution\n\n    def triplane_to_volume(self, triplane: torch.Tensor, grid_size: int = 64):\n        \"\"\"\n        Generates a 3D volume from a triplane with NaN handling\n\n        Args:\n            triplane: [B, C, H, W] triplane tensor\n            grid_size: Resolution of the 3D grid\n\n        Returns:\n            volume: [B, grid_size, grid_size, grid_size] density field\n        \"\"\"\n        B, C, H, W = triplane.shape\n        device = triplane.device\n\n        # === DIAGNOSTIC: Check triplane for NaN/Inf ===\n        if torch.isnan(triplane).any():\n            print(f\"‚ö†Ô∏è  WARNING: Triplane contains NaN values!\")\n            print(f\"   NaN count: {torch.isnan(triplane).sum().item()}\")\n            # Replace NaN with zeros\n            triplane = torch.nan_to_num(triplane, nan=0.0)\n            \n        if torch.isinf(triplane).any():\n            print(f\"‚ö†Ô∏è  WARNING: Triplane contains Inf values!\")\n            # Replace Inf with zeros\n            triplane = torch.nan_to_num(triplane, posinf=0.0, neginf=0.0)\n\n        # Clamp to reasonable range\n        triplane = torch.clamp(triplane, -10.0, 10.0)\n        \n        print(f\"[Volume] Triplane stats: min={triplane.min().item():.4f}, \"\n              f\"mean={triplane.mean().item():.4f}, max={triplane.max().item():.4f}\")\n\n        # Generate 3D grid coordinates\n        coords = torch.linspace(-1, 1, grid_size, device=device)\n        grid = torch.stack(torch.meshgrid(coords, coords, coords, indexing='ij'), dim=-1)\n        grid = grid.reshape(-1, 3)  # [N, 3]\n\n        # Sample features from each plane\n        xy_coords = grid[:, [0, 1]].unsqueeze(0).unsqueeze(0)  # XY plane\n        xz_coords = grid[:, [0, 2]].unsqueeze(0).unsqueeze(0)  # XZ plane\n        yz_coords = grid[:, [1, 2]].unsqueeze(0).unsqueeze(0)  # YZ plane\n\n        # Split triplane into three components\n        C_per_plane = C // 3\n        plane_xy = triplane[:, :C_per_plane]\n        plane_xz = triplane[:, C_per_plane:2*C_per_plane]\n        plane_yz = triplane[:, 2*C_per_plane:]\n\n        # Feature sampling with error handling\n        try:\n            feat_xy = F.grid_sample(plane_xy, xy_coords, align_corners=False, mode='bilinear', padding_mode='zeros')\n            feat_xz = F.grid_sample(plane_xz, xz_coords, align_corners=False, mode='bilinear', padding_mode='zeros')\n            feat_yz = F.grid_sample(plane_yz, yz_coords, align_corners=False, mode='bilinear', padding_mode='zeros')\n        except Exception as e:\n            print(f\"‚ö†Ô∏è  Error during grid sampling: {e}\")\n            # Fallback: create simple volume\n            return self._create_fallback_volume(B, grid_size, device)\n\n        # Aggregate features\n        features = feat_xy + feat_xz + feat_yz  # [B, C_per_plane, 1, N]\n        \n        # Check for NaN after sampling\n        if torch.isnan(features).any():\n            print(f\"‚ö†Ô∏è  WARNING: Features contain NaN after sampling!\")\n            features = torch.nan_to_num(features, nan=0.0)\n\n        # Convert to density\n        density = features.mean(dim=1).squeeze(1)  # [B, N]\n        \n        # Normalize to [0, 1] range more carefully\n        density_min = density.min()\n        density_max = density.max()\n        \n        if density_max - density_min > 1e-8:\n            # Normalize to [0, 1]\n            density = (density - density_min) / (density_max - density_min + 1e-8)\n        else:\n            print(f\"‚ö†Ô∏è  WARNING: Density range is too small, using sigmoid\")\n            density = torch.sigmoid(density)\n        \n        # Final NaN check\n        if torch.isnan(density).any():\n            print(f\"‚ö†Ô∏è  WARNING: Density contains NaN, replacing with fallback\")\n            return self._create_fallback_volume(B, grid_size, device)\n\n        # Reshape back to 3D grid\n        volume = density.reshape(B, grid_size, grid_size, grid_size)\n        \n        print(f\"[Volume] Density stats: min={volume.min().item():.4f}, \"\n              f\"mean={volume.mean().item():.4f}, max={volume.max().item():.4f}\")\n\n        return volume\n\n    def _create_fallback_volume(self, B, grid_size, device):\n        \"\"\"Create a simple spherical volume as fallback\"\"\"\n        print(\"üîÑ Creating fallback spherical volume...\")\n        \n        coords = torch.linspace(-1, 1, grid_size, device=device)\n        x, y, z = torch.meshgrid(coords, coords, coords, indexing='ij')\n        \n        # Create a sphere\n        radius = torch.sqrt(x**2 + y**2 + z**2)\n        volume = torch.exp(-radius * 3)  # Gaussian sphere\n        volume = volume.unsqueeze(0).repeat(B, 1, 1, 1)\n        \n        return volume\n\n    def extract_mesh(self, volume: torch.Tensor, threshold: float = None):\n        \"\"\"\n        Extract mesh from volume using Marching Cubes\n        \"\"\"\n        from skimage import measure\n        import numpy as np\n\n        volume_np = volume.cpu().numpy()\n\n        # Volume statistics\n        vmin, vmax = volume_np.min(), volume_np.max()\n        vmean = volume_np.mean()\n\n        print(f\"[Mesh] volume stats: min={vmin:.4f}, mean={vmean:.4f}, max={vmax:.4f}\")\n\n        # Check for NaN\n        if np.isnan(volume_np).any():\n            print(\"‚ùå ERROR: Volume contains NaN values!\")\n            print(\"   This usually means the diffusion model didn't train properly.\")\n            print(\"   Try using a different checkpoint or retraining the model.\")\n            raise ValueError(\"Cannot extract mesh from NaN volume\")\n\n        # Adaptive iso-value\n        if threshold is None:\n            # Try multiple thresholds\n            percentiles = [50, 60, 70, 80]\n            for p in percentiles:\n                threshold = np.percentile(volume_np, p)\n                print(f\"[Mesh] trying {p}th percentile threshold = {threshold:.4f}\")\n                \n                try:\n                    vertices, faces, normals, values = measure.marching_cubes(\n                        volume_np,\n                        level=threshold\n                    )\n                    print(f\"‚úì Success with {p}th percentile!\")\n                    break\n                except RuntimeError:\n                    continue\n            else:\n                # All failed, try mean\n                threshold = vmean\n                print(f\"[Mesh] trying mean threshold = {threshold:.4f}\")\n                vertices, faces, normals, values = measure.marching_cubes(\n                    volume_np,\n                    level=threshold\n                )\n        else:\n            vertices, faces, normals, values = measure.marching_cubes(\n                volume_np,\n                level=threshold\n            )\n\n        # Normalize to [-1, 1]\n        vertices = vertices / volume_np.shape[0] * 2 - 1\n\n        print(f\"‚úì Extracted mesh: {len(vertices)} vertices, {len(faces)} faces\")\n        \n        return vertices, faces\n\n    def save_obj(self, vertices, faces, filepath: str):\n        \"\"\"Save in OBJ format\"\"\"\n        with open(filepath, 'w') as f:\n            for v in vertices:\n                f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n            for face in faces:\n                f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n        print(f\"‚úì Mesh saved: {filepath}\")\n\n\n# ==================== Avatar Generator ====================\n\nclass AvatarGenerator:\n    \"\"\"Main class for 3D Avatar Generation\"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        device: str = 'cuda',\n        model_config: Dict = None\n    ):\n        \"\"\"\n        Args:\n            model_path: Path to trained model\n            device: Computing device\n            model_config: Model configuration (can be auto-detected)\n        \"\"\"\n        self.device = device\n\n        print(\"=\" * 60)\n        print(\"Avatar Generator Initialization\")\n        print(\"=\" * 60)\n\n        # Load Model\n        print(f\"\\nLoading model: {model_path}\")\n        self.load_model(model_path, model_config)\n\n        # Diffusion setup\n        self.diffusion = GaussianDiffusion(\n            timesteps=1000,\n            beta_start=0.0001,\n            beta_end=0.02,\n            device=device\n        )\n\n        # Mesh generator setup\n        self.mesh_generator = TriplaneToMesh()\n\n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n\n        print(\"\\n‚úì Initialization complete!\")\n        print(\"=\" * 60 + \"\\n\")\n\n    def load_model(self, model_path: str, model_config: Optional[Dict] = None):\n        \"\"\"Loads the model from a checkpoint\"\"\"\n        checkpoint = torch.load(model_path, map_location=self.device)\n\n        # Infer model settings if not provided\n        if model_config is None:\n            state_dict = checkpoint.get('model', checkpoint)\n\n            # Guess config based on parameter count\n            num_params = sum(p.numel() for p in state_dict.values())\n\n            if num_params < 50_000_000:  # Under 50M\n                model_config = {\n                    'hidden_size': 384,\n                    'depth': 6,\n                    'num_heads': 6,\n                }\n                print(\"  Detected: SMALL model\")\n            else:\n                model_config = {\n                    'hidden_size': 768,\n                    'depth': 12,\n                    'num_heads': 12,\n                }\n                print(\"  Detected: STANDARD model\")\n\n        # Construct model\n        self.model = DiffusionTransformer(\n            img_size=256,\n            patch_size=16,\n            in_channels=96,  # Default\n            condition_channels=3,\n            **model_config\n        ).to(self.device)\n\n        # Load weights\n        self.model.load_state_dict(checkpoint.get('model', checkpoint))\n        self.model.eval()\n\n        print(f\"  Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n\n    @torch.no_grad()\n    def generate_from_image(\n        self,\n        image_path: str,\n        num_sampling_steps: int = 50,\n        seed: Optional[int] = None\n    ) -> Dict:\n        \"\"\"\n        Generates 3D avatar from a 2D image\n\n        Args:\n            image_path: Input image path\n            num_sampling_steps: Number of diffusion sampling steps\n            seed: Random seed\n\n        Returns:\n            results: Dictionary containing results\n        \"\"\"\n        if seed is not None:\n            torch.manual_seed(seed)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed(seed)\n\n        print(f\"\\nGenerating 3D avatar from: {image_path}\")\n\n        # Load image\n        image = Image.open(image_path).convert('RGB')\n        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n        \n        # Check input\n        print(f\"[Input] Image tensor stats: min={image_tensor.min().item():.4f}, \"\n              f\"mean={image_tensor.mean().item():.4f}, max={image_tensor.max().item():.4f}\")\n\n        # Generate triplane\n        print(\"Generating triplane...\")\n        triplane = self.diffusion.sample(\n            self.model,\n            shape=(1, 96, 256, 256),\n            condition=image_tensor,\n            num_steps=num_sampling_steps\n        )\n        \n        # Check triplane output\n        print(f\"[Triplane] Generated triplane stats: min={triplane.min().item():.4f}, \"\n              f\"mean={triplane.mean().item():.4f}, max={triplane.max().item():.4f}\")\n        \n        if torch.isnan(triplane).any():\n            print(\"‚ö†Ô∏è  WARNING: Triplane contains NaN! Model may not be properly trained.\")\n            print(\"   Attempting to fix...\")\n            triplane = torch.nan_to_num(triplane, nan=0.0)\n        \n        if torch.isinf(triplane).any():\n            print(\"‚ö†Ô∏è  WARNING: Triplane contains Inf values!\")\n            triplane = torch.nan_to_num(triplane, posinf=1.0, neginf=-1.0)\n\n        # Generate volume\n        print(\"Converting to 3D volume...\")\n        volume = self.mesh_generator.triplane_to_volume(triplane, grid_size=64)\n\n        return {\n            'image': image,\n            'triplane': triplane[0],\n            'volume': volume[0],\n        }\n\n    def visualize_results(\n        self,\n        results: Dict,\n        save_dir: str = \"/content/avatar_results\"\n    ):\n        \"\"\"Visualize and save results\"\"\"\n        os.makedirs(save_dir, exist_ok=True)\n\n        # 1. Input Image\n        results['image'].save(os.path.join(save_dir, \"input_image.png\"))\n\n        # 2. Triplane Visualization\n        self.visualize_triplane(results['triplane'], save_dir)\n\n        # 3. Volume Visualization\n        self.visualize_volume(results['volume'], save_dir)\n\n        # 4. Mesh Generation and Saving\n        print(\"\\nExtracting mesh...\")\n        vertices, faces = self.mesh_generator.extract_mesh(results['volume'])\n        obj_path = os.path.join(save_dir, \"avatar_mesh.obj\")\n        self.mesh_generator.save_obj(vertices, faces, obj_path)\n\n        print(f\"\\n‚úì Results saved to: {save_dir}\")\n        return save_dir\n\n    def visualize_triplane(self, triplane: torch.Tensor, save_dir: str):\n        \"\"\"Visualize the triplane components\"\"\"\n        C, H, W = triplane.shape\n\n        # Split into three planes\n        C_per_plane = C // 3\n        plane_xy = triplane[:C_per_plane].mean(0).cpu().numpy()\n        plane_xz = triplane[C_per_plane:2*C_per_plane].mean(0).cpu().numpy()\n        plane_yz = triplane[2*C_per_plane:].mean(0).cpu().numpy()\n\n        # Normalization\n        def normalize(p): return (p - p.min()) / (p.max() - p.min() + 1e-8)\n        plane_xy = normalize(plane_xy)\n        plane_xz = normalize(plane_xz)\n        plane_yz = normalize(plane_yz)\n\n        # Plotting\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(plane_xy, cmap='viridis')\n        axes[0].set_title('XY Plane')\n        axes[0].axis('off')\n\n        axes[1].imshow(plane_xz, cmap='viridis')\n        axes[1].set_title('XZ Plane')\n        axes[1].axis('off')\n\n        axes[2].imshow(plane_yz, cmap='viridis')\n        axes[2].set_title('YZ Plane')\n        axes[2].axis('off')\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, \"triplane_visualization.png\"), dpi=150, bbox_inches='tight')\n        plt.close()\n\n        print(\"‚úì Triplane visualized\")\n\n    def visualize_volume(self, volume: torch.Tensor, save_dir: str):\n        \"\"\"Visualize slices of the 3D volume\"\"\"\n        volume_np = volume.cpu().numpy()\n\n        # Middle slice\n        mid = volume_np.shape[0] // 2\n\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        axes[0].imshow(volume_np[mid, :, :], cmap='gray')\n        axes[0].set_title('Slice X')\n        axes[0].axis('off')\n\n        axes[1].imshow(volume_np[:, mid, :], cmap='gray')\n        axes[1].set_title('Slice Y')\n        axes[1].axis('off')\n\n        axes[2].imshow(volume_np[:, :, mid], cmap='gray')\n        axes[2].set_title('Slice Z')\n        axes[2].axis('off')\n\n        plt.tight_layout()\n        plt.savefig(os.path.join(save_dir, \"volume_slices.png\"), dpi=150, bbox_inches='tight')\n        plt.close()\n\n        print(\"‚úì Volume visualized\")\n\n    def diagnose_model(self):\n        \"\"\"Run diagnostics on the model\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Model Diagnostics\")\n        print(\"=\"*60)\n        \n        # Check model parameters\n        total_params = sum(p.numel() for p in self.model.parameters())\n        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n        \n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        \n        # Check for NaN in weights\n        nan_count = 0\n        for name, param in self.model.named_parameters():\n            if torch.isnan(param).any():\n                print(f\"‚ö†Ô∏è  WARNING: NaN found in {name}\")\n                nan_count += 1\n        \n        if nan_count == 0:\n            print(\"‚úì No NaN values in model weights\")\n        else:\n            print(f\"‚ùå Found NaN in {nan_count} parameter tensors!\")\n        \n        # Test forward pass with dummy data\n        print(\"\\nTesting forward pass with dummy data...\")\n        try:\n            dummy_x = torch.randn(1, 96, 256, 256).to(self.device)\n            dummy_t = torch.tensor([500]).to(self.device)\n            dummy_c = torch.randn(1, 3, 256, 256).to(self.device)\n            \n            output = self.model(dummy_x, dummy_t, dummy_c)\n            \n            print(f\"Output shape: {output.shape}\")\n            print(f\"Output stats: min={output.min().item():.4f}, \"\n                  f\"mean={output.mean().item():.4f}, max={output.max().item():.4f}\")\n            \n            if torch.isnan(output).any():\n                print(\"‚ùå Model output contains NaN!\")\n            else:\n                print(\"‚úì Model forward pass successful\")\n        except Exception as e:\n            print(f\"‚ùå Error during forward pass: {e}\")\n        \n        print(\"=\"*60 + \"\\n\")\n\n\n# ==================== Main Execution ====================\n\ndef main():\n    \"\"\"Main execution entry point\"\"\"\n\n    # ========== Kaggle Environment Path Settings ==========\n\n    # Trained model checkpoint\n    MODEL_PATH = best_model_path\n    # Alternative: MODEL_PATH = \"/kaggle/working/dit_checkpoints/final_model.pt\"\n\n    # Test image (use one from input data)\n    TEST_IMAGE = f'{gd_path}/output_styled/styled_Chris Evans42_1217.jpg'\n\n    # Output directory\n    OUTPUT_DIR = \"avatar_results\"\n\n    # ========== Path Verification ==========\n\n    print(\"=\" * 60)\n    print(\"Path Verification\")\n    print(\"=\" * 60)\n    print(f\"Model: {MODEL_PATH}\")\n    print(f\"  Exists: {os.path.exists(MODEL_PATH)}\")\n    if os.path.exists(MODEL_PATH):\n        size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n        print(f\"  Size: {size_mb:.1f} MB\")\n\n    print(f\"\\nTest Image: {TEST_IMAGE}\")\n    print(f\"  Exists: {os.path.exists(TEST_IMAGE)}\")\n\n    print(f\"\\nOutput: {OUTPUT_DIR}\")\n    print(\"=\" * 60 + \"\\n\")\n\n    if not os.path.exists(MODEL_PATH):\n        print(\"‚ùå Model not found!\")\n        print(\"\\nAvailable models:\")\n        checkpoint_dir = f'{gd_path}/dit_checkpoints'\n        if os.path.exists(checkpoint_dir):\n            for f in os.listdir(checkpoint_dir):\n                if f.endswith('.pt'):\n                    path = os.path.join(checkpoint_dir, f)\n                    size = os.path.getsize(path) / (1024 * 1024)\n                    print(f\"  {path} ({size:.1f} MB)\")\n        return\n\n    if not os.path.exists(TEST_IMAGE):\n        print(\"‚ùå Test image not found!\")\n        print(\"\\nSearching for alternative images...\")\n        data_dir = f'{gd_path}/dit_training_data'\n        img_dir = os.path.join(data_dir, \"images\")\n        if os.path.exists(img_dir):\n            images = sorted(os.listdir(img_dir))[:5]\n            print(f\"Available images: {images}\")\n            if images:\n                TEST_IMAGE = os.path.join(img_dir, images[0])\n                print(f\"Using: {TEST_IMAGE}\")\n        else:\n            return\n\n    # ========== Configuration ==========\n\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    NUM_SAMPLING_STEPS = 50  # Fast generation (use 50 for higher quality)\n    SEED = 42\n\n    # ========== Avatar Generation Process ==========\n\n    try:\n        # Initialize generator\n        generator = AvatarGenerator(\n            model_path=MODEL_PATH,\n            device=DEVICE\n        )\n\n        # Run diagnostics\n        generator.diagnose_model()\n\n        # Generation\n        results = generator.generate_from_image(\n            image_path=TEST_IMAGE,\n            num_sampling_steps=NUM_SAMPLING_STEPS,\n            seed=SEED\n        )\n\n        # Visualize and save\n        output_dir = generator.visualize_results(results, OUTPUT_DIR)\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"Generation Complete!\")\n        print(\"=\" * 60)\n        print(f\"Output directory: {output_dir}\")\n        print(\"\\nGenerated files:\")\n        for f in sorted(os.listdir(output_dir)):\n            print(f\"  - {f}\")\n        print(\"=\" * 60)\n\n        # Generate download links for Kaggle environment\n        try:\n            from IPython.display import FileLink\n            print(\"\\nüì• Download links:\")\n            print(FileLink(os.path.join(output_dir, \"avatar_mesh.obj\")))\n        except:\n            pass\n\n    except Exception as e:\n        print(f\"\\n‚ùå Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef show_image(image_dir):\n    image_paths = [\n        os.path.join(image_dir, f)\n        for f in sorted(os.listdir(image_dir))\n        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n    ][:6]  \n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    axes = axes.flatten()\n    for ax, img_path in zip(axes, image_paths):\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis(\"off\")\n        ax.set_title(os.path.basename(img_path), fontsize=9)\n    for ax in axes[len(image_paths):]:\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_image(f'{gd_path}/output_styled')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}