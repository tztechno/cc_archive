{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 49349,
          "databundleVersionId": 5447706,
          "sourceType": "competition"
        },
        {
          "sourceId": 91498,
          "databundleVersionId": 11655853,
          "sourceType": "competition"
        },
        {
          "sourceId": 289936912,
          "sourceType": "kernelVersion"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tztechno/cc_archive/blob/main/Gaussian_Splat_w_DINO%2BALIKED_OK_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "\n"
      ],
      "metadata": {
        "id": "oYZFSP6JIQq1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fountain: Gaussian Splat w/ DINO+ALIKED**"
      ],
      "metadata": {
        "id": "lUpuiNI_IQq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia\n",
        "!pip install pycolmap\n",
        "!pip install lightglue\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "wJX7spq5E7qY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cec56c9c-9a58-449b-f325-43e8af4e453f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.8.2-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia)\n",
            "  Downloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from kornia) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->kornia) (3.0.3)\n",
            "Downloading kornia-0.8.2-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kornia_rs, kornia\n",
            "Successfully installed kornia-0.8.2 kornia_rs-0.1.10\n",
            "Collecting pycolmap\n",
            "  Downloading pycolmap-3.13.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycolmap) (2.0.2)\n",
            "Downloading pycolmap-3.13.0-cp312-cp312-manylinux_2_28_x86_64.whl (20.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycolmap\n",
            "Successfully installed pycolmap-3.13.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement lightglue (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for lightglue\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "INSTALL_DIR = \"/content/packages\"\n",
        "os.makedirs(INSTALL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Building LightGlue only...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Install LightGlue only\n",
        "# Dependencies (including pycolmap) will not be installed\n",
        "print(\"\\nInstalling LightGlue (no dependencies)...\")\n",
        "subprocess.run([\n",
        "    sys.executable, '-m', 'pip', 'install',\n",
        "    '--target', INSTALL_DIR,\n",
        "    '--no-deps',  # No dependencies\n",
        "    'git+https://github.com/cvg/LightGlue.git'\n",
        "], check=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ Build Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nPackages built to: {INSTALL_DIR}\")\n",
        "print(\"\\nNote: pycolmap will be installed on-the-fly in the main notebook.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cne_GHxTLUpN",
        "outputId": "cf54b5ec-b8e7-44c1-a390-5bb8932f5321"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Building LightGlue only...\n",
            "============================================================\n",
            "\n",
            "Installing LightGlue (no dependencies)...\n",
            "\n",
            "============================================================\n",
            "✓ Build Complete!\n",
            "============================================================\n",
            "\n",
            "Packages built to: /content/packages\n",
            "\n",
            "Note: pycolmap will be installed on-the-fly in the main notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_environment():\n",
        "    \"\"\"\n",
        "    Setup environment with clean NumPy installation at the beginning\n",
        "    \"\"\"\n",
        "    print(\"Setting up environment for Kaggle...\")\n",
        "    WORK_DIR = '/kaggle/working/gaussian_splatting'\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 0: Clean NumPy installation BEFORE importing anything\n",
        "    # ========================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"STEP 0: Fixing NumPy compatibility (clean install)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        # Uninstall NumPy completely\n",
        "        print(\"Uninstalling NumPy 2.x...\")\n",
        "        subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'uninstall', '-y', 'numpy'\n",
        "        ], check=True, capture_output=True)\n",
        "        print(\"✓ NumPy uninstalled\")\n",
        "\n",
        "        # Install NumPy 1.x\n",
        "        print(\"Installing NumPy 1.x...\")\n",
        "        subprocess.run([\n",
        "            sys.executable, '-m', 'pip', 'install', 'numpy<2'\n",
        "        ], check=True, capture_output=True)\n",
        "        print(\"✓ NumPy 1.x installed\")\n",
        "\n",
        "        # Reinstall key packages that depend on NumPy\n",
        "        print(\"Reinstalling NumPy-dependent packages...\")\n",
        "        packages_to_reinstall = [\n",
        "            'scikit-learn',\n",
        "            'scipy',\n",
        "            'matplotlib',\n",
        "            'pandas'\n",
        "        ]\n",
        "\n",
        "        for pkg in packages_to_reinstall:\n",
        "            try:\n",
        "                subprocess.run([\n",
        "                    sys.executable, '-m', 'pip', 'install', '--force-reinstall',\n",
        "                    '--no-deps', pkg\n",
        "                ], check=True, capture_output=True)\n",
        "                print(f\"✓ Reinstalled {pkg}\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                print(f\"⚠ Failed to reinstall {pkg} (may not be critical)\")\n",
        "\n",
        "        # Verify NumPy version\n",
        "        result = subprocess.run([\n",
        "            sys.executable, '-c', 'import numpy; print(numpy.__version__)'\n",
        "        ], capture_output=True, text=True)\n",
        "        numpy_version = result.stdout.strip()\n",
        "        print(f\"\\n✓ NumPy version now: {numpy_version}\")\n",
        "\n",
        "        if numpy_version.startswith('1.'):\n",
        "            print(\"✓ NumPy fix successful!\")\n",
        "        else:\n",
        "            print(f\"⚠ Warning: NumPy version is {numpy_version}, expected 1.x\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"⚠ NumPy fix encountered issues: {e}\")\n",
        "        print(\"Continuing anyway...\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 1: System packages and dependencies\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: Installing system packages\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Virtual display setup\n",
        "    try:\n",
        "        print(\"Setting up virtual display...\")\n",
        "        subprocess.run(['apt-get', 'update', '-qq'], check=True, capture_output=True)\n",
        "        subprocess.run(['apt-get', 'install', '-y', '-qq', 'xvfb'],\n",
        "                      check=True, capture_output=True)\n",
        "\n",
        "        os.environ['QT_QPA_PLATFORM'] = 'offscreen'\n",
        "        os.environ['DISPLAY'] = ':99'\n",
        "        subprocess.Popen(['Xvfb', ':99', '-screen', '0', '1024x768x24'],\n",
        "                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        print(\"✓ Virtual display setup\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Virtual display skipped: {e}\")\n",
        "\n",
        "    # Install COLMAP\n",
        "    print(\"\\nInstalling COLMAP...\")\n",
        "    try:\n",
        "        subprocess.run(['apt-get', 'install', '-y', '-qq', 'colmap'],\n",
        "                       check=True, capture_output=True)\n",
        "        print(\"✓ COLMAP installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"⚠ COLMAP warning: {e}\")\n",
        "\n",
        "    # Install build dependencies\n",
        "    print(\"\\nInstalling build dependencies...\")\n",
        "    try:\n",
        "        subprocess.run([\n",
        "            'apt-get', 'install', '-y', '-qq',\n",
        "            'build-essential', 'cmake', 'git', 'libopenblas-dev'\n",
        "        ], check=True, capture_output=True)\n",
        "        print(\"✓ Build dependencies installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"⚠ Build dependencies warning: {e}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 2: Clone Gaussian Splatting repository\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: Cloning repository\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if not os.path.exists(WORK_DIR):\n",
        "        print(f\"Cloning to {WORK_DIR}...\")\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                'git', 'clone', '--recursive',\n",
        "                'https://github.com/graphdeco-inria/gaussian-splatting.git',\n",
        "                WORK_DIR\n",
        "            ], check=True)\n",
        "            print(\"✓ Repository cloned\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"Primary repository failed, trying alternative...\")\n",
        "            try:\n",
        "                subprocess.run([\n",
        "                    'git', 'clone', '--recursive',\n",
        "                    'https://github.com/tztechno/gaussian-splatting.git',\n",
        "                    WORK_DIR\n",
        "                ], check=True)\n",
        "                print(\"✓ Alternative repository cloned\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"✗ Both repositories failed: {e}\")\n",
        "                raise\n",
        "    else:\n",
        "        print(f\"✓ Repository exists at {WORK_DIR}\")\n",
        "        try:\n",
        "            subprocess.run(['git', 'submodule', 'update', '--init', '--recursive'],\n",
        "                          cwd=WORK_DIR, check=True, capture_output=True)\n",
        "            print(\"✓ Submodules updated\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(\"⚠ Submodule update failed\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 3: Install Python packages\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: Installing Python packages\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    pip_packages = [\n",
        "        'torch', 'torchvision', 'torchaudio',\n",
        "        'plyfile', 'tqdm', 'opencv-python', 'pillow',\n",
        "        'imageio', 'imageio-ffmpeg', 'tensorboard'\n",
        "    ]\n",
        "\n",
        "    for package in pip_packages:\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', '-q', package\n",
        "            ], check=True, capture_output=True)\n",
        "            print(f\"✓ Installed {package}\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"⚠ Failed to install {package}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 4: Verify CUDA\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 4: Verifying CUDA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"PyTorch: {torch.__version__}\")\n",
        "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"CUDA version: {torch.version.cuda}\")\n",
        "            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    except ImportError:\n",
        "        print(\"⚠ PyTorch not available\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 5: Build submodules\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 5: Building submodules\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    submodules = [\n",
        "        ('diff-gaussian-rasterization',\n",
        "         'https://github.com/graphdeco-inria/diff-gaussian-rasterization.git'),\n",
        "        ('simple-knn',\n",
        "         'https://github.com/camenduru/simple-knn.git')\n",
        "    ]\n",
        "\n",
        "    for submodule_name, fallback_url in submodules:\n",
        "        print(f\"\\n{'-'*70}\")\n",
        "        print(f\"Building {submodule_name}...\")\n",
        "        print(f\"{'-'*70}\")\n",
        "\n",
        "        submodule_dir = os.path.join(WORK_DIR, 'submodules', submodule_name)\n",
        "\n",
        "        # Check/clone submodule\n",
        "        if not os.path.exists(submodule_dir) or not os.listdir(submodule_dir):\n",
        "            print(f\"Cloning {submodule_name}...\")\n",
        "            try:\n",
        "                subprocess.run(['git', 'clone', fallback_url, submodule_dir],\n",
        "                              check=True)\n",
        "                print(f\"✓ Cloned {submodule_name}\")\n",
        "            except subprocess.CalledProcessError:\n",
        "                print(f\"✗ Failed to clone {submodule_name}\")\n",
        "                continue\n",
        "\n",
        "        # Try installation methods in order\n",
        "        methods = [\n",
        "            (\"pip install\", lambda: subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', submodule_dir\n",
        "            ], check=True, capture_output=True)),\n",
        "\n",
        "            (\"setup.py install\", lambda: subprocess.run([\n",
        "                sys.executable, 'setup.py', 'install'\n",
        "            ], cwd=submodule_dir, check=True, capture_output=True)),\n",
        "\n",
        "            (\"git install\", lambda: subprocess.run([\n",
        "                sys.executable, '-m', 'pip', 'install', f'git+{fallback_url}'\n",
        "            ], check=True, capture_output=True))\n",
        "        ]\n",
        "\n",
        "        for method_name, method_func in methods:\n",
        "            try:\n",
        "                print(f\"Trying {method_name}...\")\n",
        "                method_func()\n",
        "                print(f\"✓ {submodule_name} installed via {method_name}\")\n",
        "                break\n",
        "            except subprocess.CalledProcessError:\n",
        "                print(f\"✗ {method_name} failed\")\n",
        "        else:\n",
        "            print(f\"⚠ All methods failed for {submodule_name}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 6: Verify installations\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 6: Verifying installations\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_good = True\n",
        "\n",
        "    try:\n",
        "        import diff_gaussian_rasterization\n",
        "        print(\"✓ diff_gaussian_rasterization available\")\n",
        "    except ImportError:\n",
        "        print(\"✗ diff_gaussian_rasterization NOT FOUND\")\n",
        "        all_good = False\n",
        "\n",
        "    try:\n",
        "        import simple_knn\n",
        "        print(\"✓ simple_knn available\")\n",
        "    except ImportError:\n",
        "        print(\"✗ simple_knn NOT FOUND\")\n",
        "        all_good = False\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    if all_good:\n",
        "        print(\"✓✓✓ SETUP COMPLETE - Ready to run! ✓✓✓\")\n",
        "    else:\n",
        "        print(\"⚠⚠⚠ SETUP COMPLETED WITH WARNINGS ⚠⚠⚠\")\n",
        "        print(\"Some modules may be missing. Training might fail.\")\n",
        "    print(f\"Working directory: {WORK_DIR}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return WORK_DIR\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    setup_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOkDLHNSFukk",
        "outputId": "b1e22b08-8ebf-40eb-ac6e-07991c09358a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up environment for Kaggle...\n",
            "======================================================================\n",
            "STEP 0: Fixing NumPy compatibility (clean install)\n",
            "======================================================================\n",
            "Uninstalling NumPy 2.x...\n",
            "✓ NumPy uninstalled\n",
            "Installing NumPy 1.x...\n",
            "✓ NumPy 1.x installed\n",
            "Reinstalling NumPy-dependent packages...\n",
            "✓ Reinstalled scikit-learn\n",
            "✓ Reinstalled scipy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#サイズの異なる画像を扱う\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "rCW0OvQaIeOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "# Configuration\n",
        "# IMAGE_PATH: Path to the image folder\n",
        "# WORK_DIR: Working directory for Gaussian Splatting repository\n",
        "# OUTPUT_DIR: Directory for the final video output\n",
        "# COLMAP_DIR: Directory for COLMAP data\n",
        "\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/your_folder/fountain2\"\n",
        "WORK_DIR = '/content/gaussian_splatting'\n",
        "OUTPUT_DIR = '/content/output'\n",
        "COLMAP_DIR = '/content/colmap_data'\n",
        "\n",
        "ORIGINAL=IMAGE_DIR\n",
        "RESIZED='/content/resized'"
      ],
      "metadata": {
        "id": "NlvpKXz1IudB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image_sizes(image_dir, output_dir=None, target_size=1200, mode='fit'):\n",
        "    \"\"\"\n",
        "    Resizes all images in a directory while maintaining aspect ratio.\n",
        "\n",
        "    Args:\n",
        "        image_dir: Directory containing input images.\n",
        "        output_dir: Directory to save the processed images. Defaults to image_dir.\n",
        "        target_size: The desired maximum size for the longer side (or minimum size for the shorter side).\n",
        "        mode: Resizing mode - 'fit' (fit within target), 'fill' (fill target), or 'pad' (fit with padding).\n",
        "    \"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = image_dir\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Normalizing image sizes (mode: {mode}) while maintaining aspect ratio...\")\n",
        "\n",
        "    size_stats = {}\n",
        "    converted_count = 0\n",
        "\n",
        "    for img_file in sorted(os.listdir(image_dir)):\n",
        "        if not img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            continue\n",
        "\n",
        "        input_path = os.path.join(image_dir, img_file)\n",
        "        output_path = os.path.join(output_dir, img_file)\n",
        "\n",
        "        try:\n",
        "            img = Image.open(input_path)\n",
        "            original_size = img.size  # (width, height)\n",
        "            original_aspect = original_size[0] / original_size[1]\n",
        "\n",
        "            # Record original size for statistics\n",
        "            size_key = f\"{original_size[0]}x{original_size[1]}\"\n",
        "            if size_key not in size_stats:\n",
        "                size_stats[size_key] = 0\n",
        "            size_stats[size_key] += 1\n",
        "\n",
        "            # Resize while maintaining aspect ratio\n",
        "            if mode == 'fit':\n",
        "                # Fit within target (長辺をtarget_sizeに合わせてリサイズ)\n",
        "                if original_size[0] > original_size[1]:  # 横長\n",
        "                    new_width = target_size\n",
        "                    new_height = int(target_size / original_aspect)\n",
        "                else:  # 縦長 or 正方形\n",
        "                    new_height = target_size\n",
        "                    new_width = int(target_size * original_aspect)\n",
        "\n",
        "            elif mode == 'fill':\n",
        "                # Fill target (短辺をtarget_sizeに合わせてリサイズ)\n",
        "                if original_size[0] > original_size[1]:  # 横長\n",
        "                    new_height = target_size\n",
        "                    new_width = int(target_size * original_aspect)\n",
        "                else:  # 縦長 or 正方形\n",
        "                    new_width = target_size\n",
        "                    new_height = int(target_size / original_aspect)\n",
        "\n",
        "            elif mode == 'pad':\n",
        "                # Fit with padding (短辺をtarget_sizeに合わせて、余白を追加)\n",
        "                if original_size[0] > original_size[1]:  # 横長\n",
        "                    new_width = target_size\n",
        "                    new_height = int(target_size / original_aspect)\n",
        "                else:  # 縦長 or 正方形\n",
        "                    new_height = target_size\n",
        "                    new_width = int(target_size * original_aspect)\n",
        "\n",
        "                # 余白を追加して正方形にする\n",
        "                img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "                img_square = Image.new('RGB', (target_size, target_size), (255, 255, 255))\n",
        "                offset = ((target_size - new_width) // 2, (target_size - new_height) // 2)\n",
        "                img_square.paste(img_resized, offset)\n",
        "                img = img_square\n",
        "                print(f\"  ✓ {img_file}: {original_size} → {new_width}x{new_height} (padded to {target_size}x{target_size})\")\n",
        "                img.save(output_path, quality=95)\n",
        "                converted_count += 1\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown mode: {mode}. Use 'fit', 'fill', or 'pad'.\")\n",
        "\n",
        "            # リサイズ実行\n",
        "            img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "            img_resized.save(output_path, quality=95)\n",
        "            converted_count += 1\n",
        "\n",
        "            print(f\"  ✓ {img_file}: {original_size} → {new_width}x{new_height} (aspect ratio: {original_aspect:.2f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error processing {img_file}: {e}\")\n",
        "\n",
        "    print(f\"\\nConversion complete: {converted_count} images\")\n",
        "    print(f\"Original size distribution: {size_stats}\")\n",
        "    return converted_count\n",
        "\n",
        "\n",
        "\n",
        "normalize_image_sizes(ORIGINAL, RESIZED, target_size=1000, mode='fit'):"
      ],
      "metadata": {
        "id": "IxSbne6AMqVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# Cell 1: Setup (Revised Version)\n",
        "# =========================================================\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Environment settings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Setting up environment...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# =========================================================\n",
        "# Kaggle defaults (Packages with C-binaries)\n",
        "# =========================================================\n",
        "print(\"\\n[1/5] Loading Kaggle default packages...\")\n",
        "import gc\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import h5py\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "\n",
        "print(f\"✓ Defaults loaded: numpy {np.__version__}, torch {torch.__version__}\")\n",
        "print(f\"  CUDA Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# =========================================================\n",
        "# pycolmap (With C-binaries - Install on-the-fly)\n",
        "# =========================================================\n",
        "print(\"\\n[2/5] Installing pycolmap...\")\n",
        "try:\n",
        "    import pycolmap\n",
        "    print(\"✓ pycolmap already available\")\n",
        "except ImportError:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'pycolmap'],\n",
        "                  check=True)\n",
        "    import pycolmap\n",
        "    print(\"✓ pycolmap installed\")\n",
        "\n",
        "# =========================================================\n",
        "# LightGlue (Pure Python - Using pre-built package)\n",
        "# =========================================================\n",
        "print(\"\\n[3/5] Loading LightGlue...\")\n",
        "PACKAGES_PATH = '/content/lightglue-package-builder/packages'\n",
        "\n",
        "if os.path.exists(PACKAGES_PATH):\n",
        "    sys.path.insert(0, PACKAGES_PATH)\n",
        "    print(f\"✓ Using pre-built path: {PACKAGES_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠️  Pre-built not found, installing via pip...\")\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q',\n",
        "                    'git+https://github.com/cvg/LightGlue.git'], check=True)\n",
        "\n",
        "from lightglue import ALIKED, LightGlue\n",
        "print(\"✓ LightGlue loaded\")\n",
        "\n",
        "# =========================================================\n",
        "# transformers\n",
        "# =========================================================\n",
        "print(\"\\n[4/5] Loading transformers...\")\n",
        "try:\n",
        "    from transformers import AutoImageProcessor, AutoModel\n",
        "    print(\"✓ transformers loaded\")\n",
        "except ImportError:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install', '-q', 'transformers'],\n",
        "                  check=True)\n",
        "    from transformers import AutoImageProcessor, AutoModel\n",
        "    print(\"✓ transformers installed\")\n",
        "\n",
        "# =========================================================\n",
        "# COLMAP (System Package)\n",
        "# =========================================================\n",
        "print(\"\\n[5/5] Installing COLMAP system binary...\")\n",
        "subprocess.run(['apt-get', 'update', '-qq'], capture_output=True)\n",
        "subprocess.run(['apt-get', 'install', '-y', '-qq', 'colmap'], capture_output=True)\n",
        "print(\"✓ COLMAP binary installed\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✓ All packages ready!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T05:47:41.852314Z",
          "iopub.execute_input": "2026-01-05T05:47:41.852542Z",
          "iopub.status.idle": "2026-01-05T05:48:39.581278Z",
          "shell.execute_reply.started": "2026-01-05T05:47:41.852524Z",
          "shell.execute_reply": "2026-01-05T05:48:39.580428Z"
        },
        "id": "6xPRQY5IIQq3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import h5py\n",
        "import sqlite3\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import kornia as K\n",
        "import kornia.feature as KF\n",
        "from lightglue import ALIKED, LightGlue\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import pycolmap\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CONFIG:\n",
        "    GLOBAL_TOPK = 200\n",
        "    RATIO_THR = 1.2\n",
        "    MATCH_THRESH = 10\n",
        "    N_KEYPOINTS = 2048\n",
        "    exhaustive_if_less = 20\n",
        "    min_matches = 15\n",
        "    max_num_keypoints = 8192\n",
        "    image_size = 1024\n",
        "    colmap_camera_model = 'SIMPLE_RADIAL'"
      ],
      "metadata": {
        "trusted": true,
        "id": "IsqmL7haIQq3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# COLMAP Database Utilities\n",
        "# =========================================================\n",
        "class COLMAPDatabase:\n",
        "    @staticmethod\n",
        "    def connect(database_path):\n",
        "        return COLMAPDatabase(database_path)\n",
        "\n",
        "    def __init__(self, database_path):\n",
        "        self.connection = sqlite3.connect(database_path)\n",
        "        self.cursor = self.connection.cursor()\n",
        "\n",
        "    def create_tables(self):\n",
        "        self.cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS cameras (\n",
        "                camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "                model INTEGER NOT NULL,\n",
        "                width INTEGER NOT NULL,\n",
        "                height INTEGER NOT NULL,\n",
        "                params BLOB,\n",
        "                prior_focal_length INTEGER NOT NULL\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS images (\n",
        "                image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "                name TEXT NOT NULL UNIQUE,\n",
        "                camera_id INTEGER NOT NULL,\n",
        "                prior_qw REAL,\n",
        "                prior_qx REAL,\n",
        "                prior_qy REAL,\n",
        "                prior_qz REAL,\n",
        "                prior_tx REAL,\n",
        "                prior_ty REAL,\n",
        "                prior_tz REAL,\n",
        "                CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < 2147483647),\n",
        "                FOREIGN KEY(camera_id) REFERENCES cameras(camera_id)\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS keypoints (\n",
        "                image_id INTEGER PRIMARY KEY NOT NULL,\n",
        "                rows INTEGER NOT NULL,\n",
        "                cols INTEGER NOT NULL,\n",
        "                data BLOB,\n",
        "                FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS matches (\n",
        "                pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "                rows INTEGER NOT NULL,\n",
        "                cols INTEGER NOT NULL,\n",
        "                data BLOB\n",
        "            )\n",
        "        \"\"\")\n",
        "        self.cursor.execute(\"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\")\n",
        "\n",
        "    def add_camera(self, model, width, height, params, prior_focal_length=1):\n",
        "        params_blob = np.array(params, dtype=np.float64).tobytes()\n",
        "        self.cursor.execute(\n",
        "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "            (None, model, width, height, params_blob, prior_focal_length)\n",
        "        )\n",
        "        return self.cursor.lastrowid\n",
        "\n",
        "    def add_image(self, name, camera_id, prior_q=None, prior_t=None):\n",
        "        if prior_q is None:\n",
        "            prior_q = [1.0, 0.0, 0.0, 0.0]\n",
        "        if prior_t is None:\n",
        "            prior_t = [0.0, 0.0, 0.0]\n",
        "\n",
        "        self.cursor.execute(\n",
        "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "            (None, name, camera_id, *prior_q, *prior_t)\n",
        "        )\n",
        "        return self.cursor.lastrowid\n",
        "\n",
        "    def add_keypoints(self, image_id, keypoints):\n",
        "        if keypoints.dtype != np.float32:\n",
        "            keypoints = keypoints.astype(np.float32)\n",
        "\n",
        "        self.cursor.execute(\n",
        "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
        "            (image_id, keypoints.shape[0], keypoints.shape[1], keypoints.tobytes())\n",
        "        )\n",
        "\n",
        "    def add_matches(self, image_id1, image_id2, matches):\n",
        "        pair_id = self.image_ids_to_pair_id(image_id1, image_id2)\n",
        "\n",
        "        if matches.dtype != np.uint32:\n",
        "            matches = matches.astype(np.uint32)\n",
        "\n",
        "        self.cursor.execute(\n",
        "            \"INSERT OR REPLACE INTO matches VALUES (?, ?, ?, ?)\",\n",
        "            (pair_id, matches.shape[0], matches.shape[1], matches.tobytes())\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def image_ids_to_pair_id(image_id1, image_id2):\n",
        "        if image_id1 > image_id2:\n",
        "            image_id1, image_id2 = image_id2, image_id1\n",
        "        return image_id1 * 2147483648 + image_id2\n",
        "\n",
        "    def commit(self):\n",
        "        self.connection.commit()\n",
        "\n",
        "    def close(self):\n",
        "        self.connection.close()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# H5 to Database Import\n",
        "# =========================================================\n",
        "CAMERA_MODEL_IDS = {\n",
        "    'SIMPLE_PINHOLE': 0,\n",
        "    'PINHOLE': 1,\n",
        "    'SIMPLE_RADIAL': 2,\n",
        "    'RADIAL': 3,\n",
        "    'OPENCV': 4,\n",
        "    'OPENCV_FISHEYE': 5,\n",
        "}\n",
        "\n",
        "def create_camera(db, image_path, camera_model):\n",
        "    \"\"\"Create camera entry\"\"\"\n",
        "    img = Image.open(image_path)\n",
        "    width, height = img.size\n",
        "\n",
        "    # Simple radial model: f, cx, cy, k\n",
        "    focal = max(width, height) * 1.2\n",
        "    params = [focal, width/2, height/2, 0.0]\n",
        "\n",
        "    model_id = CAMERA_MODEL_IDS.get(camera_model.upper(), 2)\n",
        "    camera_id = db.add_camera(model_id, width, height, params)\n",
        "\n",
        "    return camera_id"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T05:48:39.582671Z",
          "iopub.execute_input": "2026-01-05T05:48:39.58323Z",
          "iopub.status.idle": "2026-01-05T05:48:39.596493Z",
          "shell.execute_reply.started": "2026-01-05T05:48:39.58321Z",
          "shell.execute_reply": "2026-01-05T05:48:39.595782Z"
        },
        "id": "vNwfOXztIQq4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def import_into_colmap(image_dir, feature_dir, database_path):\n",
        "    \"\"\"COLMAP Database Import - Multiple Cameras Support\"\"\"\n",
        "    print(\"\\n=== Creating COLMAP Database ===\")\n",
        "\n",
        "    if os.path.exists(database_path):\n",
        "        os.remove(database_path)\n",
        "\n",
        "    # Create empty database structure\n",
        "    conn = sqlite3.connect(database_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create all tables\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cameras (\n",
        "            camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "            model INTEGER NOT NULL,\n",
        "            width INTEGER NOT NULL,\n",
        "            height INTEGER NOT NULL,\n",
        "            params BLOB,\n",
        "            prior_focal_length INTEGER NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS images (\n",
        "            image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "            name TEXT NOT NULL UNIQUE,\n",
        "            camera_id INTEGER NOT NULL,\n",
        "            prior_qw REAL,\n",
        "            prior_qx REAL,\n",
        "            prior_qy REAL,\n",
        "            prior_qz REAL,\n",
        "            prior_tx REAL,\n",
        "            prior_ty REAL,\n",
        "            prior_tz REAL,\n",
        "            FOREIGN KEY(camera_id) REFERENCES cameras(camera_id)\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS keypoints (\n",
        "            image_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB,\n",
        "            FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS matches (\n",
        "            pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
        "            pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB,\n",
        "            config INTEGER NOT NULL,\n",
        "            F BLOB,\n",
        "            E BLOB,\n",
        "            H BLOB,\n",
        "            qvec BLOB,\n",
        "            tvec BLOB\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    # Load keypoints file\n",
        "    kpts_file = os.path.join(feature_dir, 'keypoints.h5')\n",
        "    matches_file = os.path.join(feature_dir, 'matches.h5')\n",
        "\n",
        "    # Create cameras based on image sizes\n",
        "    size_to_camera = {}  # (width, height) -> camera_id\n",
        "    fname_to_id = {}\n",
        "    image_id = 1\n",
        "\n",
        "    with h5py.File(kpts_file, 'r') as f:\n",
        "        print(f\"Importing {len(f.keys())} images...\")\n",
        "\n",
        "        for filename in tqdm(f.keys(), desc=\"Adding images\"):\n",
        "            # Get image size\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            try:\n",
        "                img = Image.open(image_path)\n",
        "                width, height = img.size\n",
        "                img.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Cannot open {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Get or create camera for this size\n",
        "            size_key = (width, height)\n",
        "            if size_key not in size_to_camera:\n",
        "                focal = max(width, height) * 1.2\n",
        "                params = np.array([focal, width/2, height/2, 0.0], dtype=np.float64)\n",
        "\n",
        "                cursor.execute(\n",
        "                    \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                    (None, 2, width, height, params.tobytes(), 1)  # 2 = SIMPLE_RADIAL\n",
        "                )\n",
        "                camera_id = cursor.lastrowid\n",
        "                size_to_camera[size_key] = camera_id\n",
        "            else:\n",
        "                camera_id = size_to_camera[size_key]\n",
        "\n",
        "            # Add image\n",
        "            cursor.execute(\n",
        "                \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "                (image_id, filename, camera_id, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
        "            )\n",
        "            fname_to_id[filename] = image_id\n",
        "\n",
        "            # Add keypoints\n",
        "            kpts = f[filename][()].astype(np.float32)\n",
        "            if len(kpts.shape) == 1:\n",
        "                kpts = kpts.reshape(-1, 2)\n",
        "\n",
        "            cursor.execute(\n",
        "                \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
        "                (image_id, kpts.shape[0], 2, kpts.tobytes())\n",
        "            )\n",
        "\n",
        "            image_id += 1\n",
        "\n",
        "    print(f\"\\nCreated {len(size_to_camera)} camera(s) for different image sizes:\")\n",
        "    for size, cam_id in sorted(size_to_camera.items()):\n",
        "        print(f\"  Camera {cam_id}: {size[0]}x{size[1]}\")\n",
        "\n",
        "    # Add matches\n",
        "    total_matches = 0\n",
        "    total_match_count = 0\n",
        "    with h5py.File(matches_file, 'r') as f:\n",
        "        print(f\"\\nProcessing matches...\")\n",
        "        for key1 in tqdm(f.keys(), desc=\"Adding matches\"):\n",
        "            if key1 not in fname_to_id:\n",
        "                continue\n",
        "            for key2 in f[key1].keys():\n",
        "                if key2 not in fname_to_id:\n",
        "                    continue\n",
        "\n",
        "                id1, id2 = fname_to_id[key1], fname_to_id[key2]\n",
        "                if id1 >= id2:\n",
        "                    continue\n",
        "\n",
        "                matches = f[key1][key2][()].astype(np.uint32)\n",
        "                if matches.shape[0] == 0:\n",
        "                    continue\n",
        "\n",
        "                pair_id = id1 * 2147483648 + id2\n",
        "                cursor.execute(\n",
        "                    \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
        "                    (pair_id, matches.shape[0], 2, matches.tobytes())\n",
        "                )\n",
        "                total_matches += 1\n",
        "                total_match_count += matches.shape[0]\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"\\n✓ Database created: {database_path}\")\n",
        "    print(f\"  Cameras: {len(size_to_camera)}\")\n",
        "    print(f\"  Images: {len(fname_to_id)}\")\n",
        "    print(f\"  Match pairs: {total_matches}\")\n",
        "    print(f\"  Total matches: {total_match_count}\")\n",
        "\n",
        "    return fname_to_id"
      ],
      "metadata": {
        "trusted": true,
        "id": "APfi4SJrIQq4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_torch_image(fname, device=torch.device('cuda')):\n",
        "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
        "    return img\n",
        "\n",
        "\n",
        "def extract_dino_embeddings(fnames, device=torch.device('cuda')):\n",
        "    print(\"\\n=== Stage 1: Extracting DINO Global Features ===\")\n",
        "\n",
        "    processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
        "    model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
        "    model = model.eval().to(device)\n",
        "\n",
        "    global_descs = []\n",
        "    for img_path in tqdm(fnames, desc=\"DINO extraction\"):\n",
        "        timg = load_torch_image(img_path, device)\n",
        "        with torch.inference_mode():\n",
        "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            dino_feat = F.normalize(\n",
        "                outputs.last_hidden_state[:,1:].max(dim=1)[0],\n",
        "                dim=1, p=2\n",
        "            )\n",
        "        global_descs.append(dino_feat.detach().cpu())\n",
        "\n",
        "    global_descs = torch.cat(global_descs, dim=0)\n",
        "    print(f\"Extracted global features: {global_descs.shape}\")\n",
        "\n",
        "    del model, processor\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return global_descs\n",
        "\n",
        "\n",
        "def build_topk_pairs(global_feats, device):\n",
        "    print(\"\\n=== Building Top-K Pairs from Global Features ===\")\n",
        "\n",
        "    g = global_feats.to(device)\n",
        "    sim = g @ g.T\n",
        "    sim.fill_diagonal_(-1)\n",
        "\n",
        "    N = sim.size(0)\n",
        "    k = min(CONFIG.GLOBAL_TOPK, N - 1)\n",
        "    k = max(k, 1)\n",
        "\n",
        "    topk_indices = torch.topk(sim, k, dim=1).indices.cpu()\n",
        "\n",
        "    pairs = set()\n",
        "    for i, neighbors in enumerate(topk_indices):\n",
        "        for j in neighbors:\n",
        "            j = j.item()\n",
        "            if i < j:\n",
        "                pairs.add((i, j))\n",
        "\n",
        "    pairs = sorted(list(pairs))\n",
        "    print(f\"Initial pairs from global features: {len(pairs)}\")\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def extract_aliked_features(fnames, device=torch.device('cuda')):\n",
        "    print(\"\\n=== Stage 2: Extracting ALIKED Local Features ===\")\n",
        "\n",
        "    dtype = torch.float32\n",
        "    extractor = ALIKED(\n",
        "        model_name=\"aliked-n16\",\n",
        "        max_num_keypoints=CONFIG.max_num_keypoints,\n",
        "        detection_threshold=0.01,\n",
        "        resize=CONFIG.image_size\n",
        "    ).eval().to(device, dtype)\n",
        "\n",
        "    keypoints_dict = {}\n",
        "    descriptors_dict = {}\n",
        "\n",
        "    for img_path in tqdm(fnames, desc=\"ALIKED extraction\"):\n",
        "        key = os.path.basename(img_path)\n",
        "        image = load_torch_image(img_path, device=device).to(dtype)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            feats = extractor.extract(image)\n",
        "            kpts = feats['keypoints'].reshape(-1, 2).detach().cpu()\n",
        "            descs = feats['descriptors'].reshape(-1, 128).detach().cpu()\n",
        "            descs = F.normalize(descs, dim=1).half()\n",
        "\n",
        "        keypoints_dict[key] = kpts.numpy()\n",
        "        descriptors_dict[key] = descs\n",
        "\n",
        "    print(f\"Extracted features for {len(keypoints_dict)} images\")\n",
        "\n",
        "    del extractor\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return keypoints_dict, descriptors_dict\n",
        "\n",
        "\n",
        "def verify_pairs_with_local_features(pairs, fnames, descriptors_dict, device):\n",
        "    print(\"\\n=== Verifying Pairs with Local Features ===\")\n",
        "\n",
        "    verified_pairs = []\n",
        "\n",
        "    for i, j in tqdm(pairs, desc=\"Local verification\"):\n",
        "        key1 = os.path.basename(fnames[i])\n",
        "        key2 = os.path.basename(fnames[j])\n",
        "\n",
        "        desc1 = descriptors_dict[key1].to(device)\n",
        "        desc2 = descriptors_dict[key2].to(device)\n",
        "\n",
        "        if desc1.size(0) == 0 or desc2.size(0) == 0:\n",
        "            continue\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            sim = desc1 @ desc2.T\n",
        "            nn1 = torch.argmax(sim, dim=1)\n",
        "            nn2 = torch.argmax(sim, dim=0)\n",
        "            mutual = torch.arange(len(nn1), device=device) == nn2[nn1]\n",
        "            n_matches = mutual.sum().item()\n",
        "\n",
        "        if n_matches >= CONFIG.MATCH_THRESH:\n",
        "            verified_pairs.append((i, j))\n",
        "\n",
        "    print(f\"Verified pairs: {len(verified_pairs)}\")\n",
        "    return verified_pairs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T05:48:39.620344Z",
          "iopub.execute_input": "2026-01-05T05:48:39.620528Z",
          "iopub.status.idle": "2026-01-05T05:48:39.635341Z",
          "shell.execute_reply.started": "2026-01-05T05:48:39.620512Z",
          "shell.execute_reply": "2026-01-05T05:48:39.634554Z"
        },
        "id": "paYlPoohIQq4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def match_with_lightglue(verified_pairs, fnames, keypoints_dict, descriptors_dict,\n",
        "                         output_dir, device=torch.device('cuda')):\n",
        "    \"\"\"Perform detailed matching using LightGlue - Fully Corrected Version\"\"\"\n",
        "    print(\"\\n=== Stage 3: Matching with LightGlue ===\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    lg_matcher = KF.LightGlueMatcher(\n",
        "        \"aliked\", {\n",
        "            \"width_confidence\": -1,\n",
        "            \"depth_confidence\": -1,\n",
        "            \"mp\": True if 'cuda' in str(device) else False\n",
        "        }\n",
        "    ).eval().to(device).half()\n",
        "\n",
        "    print(\"Loaded LightGlue model\")\n",
        "\n",
        "    # Save keypoints\n",
        "    kpts_h5_path = os.path.join(output_dir, 'keypoints.h5')\n",
        "    with h5py.File(kpts_h5_path, 'w') as f:\n",
        "        for img_path in fnames:\n",
        "            key = os.path.basename(img_path)\n",
        "            f.create_dataset(key, data=keypoints_dict[key])\n",
        "\n",
        "    # Save matches\n",
        "    matches_h5_path = os.path.join(output_dir, 'matches.h5')\n",
        "    matched_pairs = 0\n",
        "    skipped_pairs = 0\n",
        "    total_matches = 0\n",
        "\n",
        "    with h5py.File(matches_h5_path, 'w') as f_match:\n",
        "        for i, j in tqdm(verified_pairs, desc=\"LightGlue matching\"):\n",
        "            key1 = os.path.basename(fnames[i])\n",
        "            key2 = os.path.basename(fnames[j])\n",
        "\n",
        "            kp1 = torch.from_numpy(keypoints_dict[key1]).to(device).half()\n",
        "            kp2 = torch.from_numpy(keypoints_dict[key2]).to(device).half()\n",
        "            desc1 = descriptors_dict[key1].to(device)\n",
        "            desc2 = descriptors_dict[key2].to(device)\n",
        "\n",
        "            if len(kp1) == 0 or len(kp2) == 0:\n",
        "                skipped_pairs += 1\n",
        "                continue\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                try:\n",
        "                    dists, idxs = lg_matcher(\n",
        "                        desc1, desc2,\n",
        "                        KF.laf_from_center_scale_ori(kp1[None]),\n",
        "                        KF.laf_from_center_scale_ori(kp2[None])\n",
        "                    )\n",
        "\n",
        "                    # Check if matches were found\n",
        "                    if idxs.numel() == 0:\n",
        "                        skipped_pairs += 1\n",
        "                        continue\n",
        "\n",
        "                    # ★★★ Fix: Removed [0] ★★★\n",
        "                    matches = idxs.cpu().numpy()  # (num_matches, 2)\n",
        "\n",
        "                    # Check match count\n",
        "                    num_matches = matches.shape[0]\n",
        "\n",
        "                    if num_matches >= CONFIG.min_matches:\n",
        "                        grp = f_match.require_group(key1)\n",
        "                        grp.create_dataset(key2, data=matches)\n",
        "                        matched_pairs += 1\n",
        "                        total_matches += num_matches\n",
        "                    else:\n",
        "                        skipped_pairs += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError matching {key1}-{key2}: {e}\")\n",
        "                    skipped_pairs += 1\n",
        "                    continue\n",
        "\n",
        "    del lg_matcher\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"\\nMatching complete:\")\n",
        "    print(f\"  Matched pairs: {matched_pairs}\")\n",
        "    print(f\"  Skipped pairs: {skipped_pairs}\")\n",
        "    print(f\"  Total matches: {total_matches}\")\n",
        "    print(f\"  Average matches per pair: {total_matches/matched_pairs:.1f}\" if matched_pairs > 0 else \"\")\n",
        "    print(f\"  Success rate: {matched_pairs/len(verified_pairs)*100:.1f}%\")\n",
        "\n",
        "    print(f\"\\nSaved keypoints to: {kpts_h5_path}\")\n",
        "    print(f\"Saved matches to: {matches_h5_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-05T05:48:39.636081Z",
          "iopub.execute_input": "2026-01-05T05:48:39.636374Z",
          "iopub.status.idle": "2026-01-05T05:48:39.655904Z",
          "shell.execute_reply.started": "2026-01-05T05:48:39.636356Z",
          "shell.execute_reply": "2026-01-05T05:48:39.655232Z"
        },
        "id": "33nQw3f2IQq5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cu8BVjF-SJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_colmap_sequential(database_path, image_dir, output_dir):\n",
        "    \"\"\"Run COLMAP mapper with manual initial pair\"\"\"\n",
        "    print(\"\\n=== Stage 4: Running COLMAP Reconstruction ===\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    env = os.environ.copy()\n",
        "    env['QT_QPA_PLATFORM'] = 'offscreen'\n",
        "\n",
        "    from datetime import datetime, timezone\n",
        "    print(f\"🚀 Starting mapper at {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    print(\"💡 Using manual initial pair: images 11-20 (5859 matches)\")\n",
        "    print()\n",
        "\n",
        "    cmd_mapper = [\n",
        "        'colmap', 'mapper',\n",
        "        '--database_path', database_path,\n",
        "        '--image_path', image_dir,\n",
        "        '--output_path', output_dir,\n",
        "        # 手動指定なし（自動）\n",
        "        '--Mapper.ba_refine_focal_length', '1',\n",
        "        '--Mapper.ba_refine_principal_point', '1',\n",
        "        '--Mapper.ba_refine_extra_params', '1',\n",
        "        # 標準的な設定\n",
        "        '--Mapper.init_min_num_inliers', '50',\n",
        "        '--Mapper.init_max_error', '8',\n",
        "        '--Mapper.init_min_tri_angle', '4',\n",
        "    ]\n",
        "\n",
        "    import subprocess\n",
        "    process = subprocess.Popen(\n",
        "        cmd_mapper,\n",
        "        env=env,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True,\n",
        "        bufsize=1,\n",
        "        universal_newlines=True\n",
        "    )\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    for line in iter(process.stdout.readline, ''):\n",
        "        if line:\n",
        "            print(line.rstrip(), flush=True)\n",
        "\n",
        "    process.stdout.close()\n",
        "    return_code = process.wait(timeout=3600)\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    if return_code == 0:\n",
        "        print(f\"\\n✅ COLMAP reconstruction saved to: {output_dir}\")\n",
        "        print(f\"🕐 Completed at {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    else:\n",
        "        print(f\"\\n❌ COLMAP mapper failed with return code {return_code}\")\n",
        "        raise subprocess.CalledProcessError(return_code, cmd_mapper)"
      ],
      "metadata": {
        "id": "qKD1nLUR1y-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def import_into_colmap(image_dir, feature_dir, database_path):\n",
        "    \"\"\"Import with camera grouping\"\"\"\n",
        "    print(\"\\n=== Creating COLMAP Database ===\")\n",
        "\n",
        "    if os.path.exists(database_path):\n",
        "        os.remove(database_path)\n",
        "\n",
        "    import cv2\n",
        "\n",
        "    conn = sqlite3.connect(database_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Create tables\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS cameras (\n",
        "            camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "            model INTEGER NOT NULL,\n",
        "            width INTEGER NOT NULL,\n",
        "            height INTEGER NOT NULL,\n",
        "            params BLOB,\n",
        "            prior_focal_length INTEGER NOT NULL\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS images (\n",
        "            image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
        "            name TEXT NOT NULL UNIQUE,\n",
        "            camera_id INTEGER NOT NULL,\n",
        "            prior_qw REAL,\n",
        "            prior_qx REAL,\n",
        "            prior_qy REAL,\n",
        "            prior_qz REAL,\n",
        "            prior_tx REAL,\n",
        "            prior_ty REAL,\n",
        "            prior_tz REAL,\n",
        "            FOREIGN KEY(camera_id) REFERENCES cameras(camera_id)\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS keypoints (\n",
        "            image_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB,\n",
        "            FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS matches (\n",
        "            pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    cursor.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
        "            pair_id INTEGER PRIMARY KEY NOT NULL,\n",
        "            rows INTEGER NOT NULL,\n",
        "            cols INTEGER NOT NULL,\n",
        "            data BLOB,\n",
        "            config INTEGER NOT NULL,\n",
        "            F BLOB,\n",
        "            E BLOB,\n",
        "            H BLOB,\n",
        "            qvec BLOB,\n",
        "            tvec BLOB\n",
        "        )\n",
        "    \"\"\")\n",
        "\n",
        "    kpts_file = os.path.join(feature_dir, 'keypoints.h5')\n",
        "    matches_file = os.path.join(feature_dir, 'matches.h5')\n",
        "\n",
        "    # Camera grouping function\n",
        "    def get_camera_group(width, height, tolerance=100):\n",
        "        \"\"\"Group similar resolutions together\"\"\"\n",
        "        w_group = round(width / tolerance) * tolerance\n",
        "        h_group = round(height / tolerance) * tolerance\n",
        "        return (w_group, h_group)\n",
        "\n",
        "    # Add cameras and images\n",
        "    size_to_camera = {}\n",
        "    fname_to_id = {}\n",
        "    image_id = 1\n",
        "\n",
        "    with h5py.File(kpts_file, 'r') as f:\n",
        "        print(f\"Importing {len(f.keys())} images...\")\n",
        "\n",
        "        for filename in tqdm(f.keys(), desc=\"Adding images\"):\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            try:\n",
        "                img = Image.open(image_path)\n",
        "                width, height = img.size\n",
        "                img.close()\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            # Get grouped camera key\n",
        "            size_key = get_camera_group(width, height, tolerance=50)\n",
        "\n",
        "            if size_key not in size_to_camera:\n",
        "                # Use group representative values\n",
        "                focal = max(size_key[0], size_key[1])  # 1.2倍を削除\n",
        "                params = np.array([focal, size_key[0]/2, size_key[1]/2, 0.0], dtype=np.float64)\n",
        "                cursor.execute(\n",
        "                    \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                    (None, 2, size_key[0], size_key[1], params.tobytes(), 1)\n",
        "                )\n",
        "                size_to_camera[size_key] = cursor.lastrowid\n",
        "                print(f\"  Created camera group: {size_key[0]}x{size_key[1]}, focal={focal:.0f}\")\n",
        "\n",
        "            camera_id = size_to_camera[size_key]\n",
        "            cursor.execute(\n",
        "                \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "                (image_id, filename, camera_id, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
        "            )\n",
        "            fname_to_id[filename] = image_id\n",
        "\n",
        "            kpts = f[filename][()].astype(np.float32)\n",
        "            if len(kpts.shape) == 1:\n",
        "                kpts = kpts.reshape(-1, 2)\n",
        "            cursor.execute(\n",
        "                \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
        "                (image_id, kpts.shape[0], 2, kpts.tobytes())\n",
        "            )\n",
        "            image_id += 1\n",
        "\n",
        "    print(f\"\\n✅ Grouped into {len(size_to_camera)} camera(s) (from ~36 individual sizes)\")\n",
        "\n",
        "    # Geometric verification\n",
        "    verified_count = 0\n",
        "\n",
        "    with h5py.File(kpts_file, 'r') as f_kpts:\n",
        "        with h5py.File(matches_file, 'r') as f_matches:\n",
        "            print(f\"\\n🔧 Processing matches with geometric verification...\")\n",
        "\n",
        "            for key1 in tqdm(f_matches.keys(), desc=\"Verifying\"):\n",
        "                if key1 not in fname_to_id:\n",
        "                    continue\n",
        "\n",
        "                for key2 in f_matches[key1].keys():\n",
        "                    if key2 not in fname_to_id:\n",
        "                        continue\n",
        "\n",
        "                    id1, id2 = fname_to_id[key1], fname_to_id[key2]\n",
        "                    if id1 >= id2:\n",
        "                        continue\n",
        "\n",
        "                    matches = f_matches[key1][key2][()].astype(np.uint32)\n",
        "                    if matches.shape[0] < 15:\n",
        "                        continue\n",
        "\n",
        "                    kpts1 = f_kpts[key1][()].astype(np.float64)\n",
        "                    kpts2 = f_kpts[key2][()].astype(np.float64)\n",
        "\n",
        "                    if len(kpts1.shape) == 1:\n",
        "                        kpts1 = kpts1.reshape(-1, 2)\n",
        "                    if len(kpts2.shape) == 1:\n",
        "                        kpts2 = kpts2.reshape(-1, 2)\n",
        "\n",
        "                    pts1 = kpts1[matches[:, 0]]\n",
        "                    pts2 = kpts2[matches[:, 1]]\n",
        "\n",
        "                    try:\n",
        "                        F, mask = cv2.findFundamentalMat(\n",
        "                            pts1, pts2,\n",
        "                            cv2.FM_RANSAC,\n",
        "                            3.0, 0.999\n",
        "                        )\n",
        "\n",
        "                        if F is None or mask is None:\n",
        "                            continue\n",
        "\n",
        "                        inliers = matches[mask.ravel() == 1]\n",
        "\n",
        "                        if len(inliers) < 15:\n",
        "                            continue\n",
        "\n",
        "                        pair_id = id1 * 2147483648 + id2\n",
        "\n",
        "                        cursor.execute(\n",
        "                            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
        "                            (pair_id, len(inliers), 2, inliers.astype(np.uint32).tobytes())\n",
        "                        )\n",
        "\n",
        "                        cursor.execute(\n",
        "                            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
        "                            (pair_id, len(inliers), 2, inliers.astype(np.uint32).tobytes(),\n",
        "                             2, F.astype(np.float64).tobytes(),\n",
        "                             None, None, None, None)\n",
        "                        )\n",
        "\n",
        "                        verified_count += 1\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "    print(f\"\\n✓ Database created: {database_path}\")\n",
        "    print(f\"  Camera groups: {len(size_to_camera)}\")\n",
        "    print(f\"  Images: {len(fname_to_id)}\")\n",
        "    print(f\"  ✅ Geometrically verified pairs: {verified_count}\")\n",
        "\n",
        "    return fname_to_id"
      ],
      "metadata": {
        "id": "7gQdy_QYTtjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline(image_dir, output_base_dir):\n",
        "    \"\"\"Complete pipeline\"\"\"\n",
        "\n",
        "    from datetime import datetime, timezone\n",
        "\n",
        "    print(f\"\\n🚀 Pipeline started at {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Get images\n",
        "    img_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']\n",
        "    fnames = []\n",
        "    for ext in img_extensions:\n",
        "        fnames.extend(glob.glob(os.path.join(image_dir, ext)))\n",
        "    fnames = sorted(fnames)\n",
        "    print(f\"\\n📸 Found {len(fnames)} images\")\n",
        "\n",
        "    if len(fnames) == 0:\n",
        "        raise ValueError(\"No images found!\")\n",
        "\n",
        "    # Create directories\n",
        "    feature_dir = os.path.join(output_base_dir, 'features')\n",
        "    colmap_dir = os.path.join(output_base_dir, 'colmap')\n",
        "    sparse_dir = os.path.join(colmap_dir, 'sparse')\n",
        "    os.makedirs(feature_dir, exist_ok=True)\n",
        "    os.makedirs(colmap_dir, exist_ok=True)\n",
        "\n",
        "    # Stages 1-3: Feature extraction and matching\n",
        "    print(f\"\\n⏰ Stage 1 started: {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    global_feats = extract_dino_embeddings(fnames, device)\n",
        "\n",
        "    print(f\"\\n⏰ Stage 2 started: {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    initial_pairs = build_topk_pairs(global_feats, device)\n",
        "    keypoints_dict, descriptors_dict = extract_aliked_features(fnames, device)\n",
        "\n",
        "    print(f\"\\n⏰ Stage 3 started: {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    verified_pairs = verify_pairs_with_local_features(\n",
        "        initial_pairs, fnames, descriptors_dict, device\n",
        "    )\n",
        "    match_with_lightglue(\n",
        "        verified_pairs, fnames, keypoints_dict, descriptors_dict,\n",
        "        feature_dir, device\n",
        "    )\n",
        "\n",
        "    from datetime import datetime, timezone\n",
        "    print()\n",
        "    print(datetime.now(timezone.utc))\n",
        "\n",
        "    print(f\"\\n⏰ Stage 4 started: {datetime.now(timezone.utc).strftime('%H:%M:%S UTC')}\")\n",
        "    # Stage 4: COLMAP Database + Reconstruction\n",
        "    database_path = os.path.join(colmap_dir, 'database.db')\n",
        "    #import_into_colmap(image_dir, feature_dir, database_path)\n",
        "    import_into_colmap(image_dir, feature_dir, database_path)\n",
        "    run_colmap_sequential(database_path, image_dir, sparse_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"✅ Pipeline Complete!\")\n",
        "    print(f\"🕐 Finished at {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "# Execute\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/your_folder/fountain2\"\n",
        "OUTPUT_DIR = \"/content/output\"\n",
        "\n",
        "main_pipeline(IMAGE_DIR, OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "-bfvkO7NS-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_cameras_to_pinhole(input_file, output_file):\n",
        "    \"\"\"Convert camera model to PINHOLE format, typically from OPENCV\"\"\"\n",
        "    print(f\"Reading camera file: {input_file}\")\n",
        "\n",
        "    with open(input_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    converted_count = 0\n",
        "    with open(output_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            # Write comments and empty lines directly\n",
        "            if line.startswith('#') or line.strip() == '':\n",
        "                f.write(line)\n",
        "            else:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) >= 4:\n",
        "                    cam_id = parts[0]\n",
        "                    model = parts[1]\n",
        "                    width = parts[2]\n",
        "                    height = parts[3]\n",
        "                    params = parts[4:]\n",
        "\n",
        "                    # Convert to PINHOLE format\n",
        "                    if model == \"PINHOLE\":\n",
        "                        f.write(line)\n",
        "                    elif model == \"OPENCV\":\n",
        "                        # OPENCV: fx, fy, cx, cy, k1, k2, p1, p2 (only need first four for PINHOLE)\n",
        "                        fx = params[0]\n",
        "                        fy = params[1]\n",
        "                        cx = params[2]\n",
        "                        cy = params[3]\n",
        "                        # PINHOLE: fx, fy, cx, cy\n",
        "                        f.write(f\"{cam_id} PINHOLE {width} {height} {fx} {fy} {cx} {cy}\\n\")\n",
        "                        converted_count += 1\n",
        "                    else:\n",
        "                        # Convert other models by estimating PINHOLE parameters\n",
        "                        # Set focal length to the max of width/height, and principal point to the center\n",
        "                        fx = fy = max(float(width), float(height))\n",
        "                        cx = float(width) / 2\n",
        "                        cy = float(height) / 2\n",
        "                        f.write(f\"{cam_id} PINHOLE {width} {height} {fx} {fy} {cx} {cy}\\n\")\n",
        "                        converted_count += 1\n",
        "                else:\n",
        "                    # Write lines that don't match the expected format\n",
        "                    f.write(line)\n",
        "\n",
        "    print(f\"Converted {converted_count} cameras to PINHOLE format\")\n",
        "\n",
        "\n",
        "\n",
        "def prepare_gaussian_splatting_data(image_dir, colmap_model_dir):\n",
        "    \"\"\"Prepare data for Gaussian Splatting, structuring it in the expected format\"\"\"\n",
        "    print(\"Preparing data for Gaussian Splatting...\")\n",
        "\n",
        "    # Assumes WORK_DIR is defined globally or passed\n",
        "    data_dir = f\"{WORK_DIR}/data/video\"\n",
        "    os.makedirs(f\"{data_dir}/sparse/0\", exist_ok=True)\n",
        "    os.makedirs(f\"{data_dir}/images\", exist_ok=True)\n",
        "\n",
        "    # Copy images\n",
        "    print(\"Copying images...\")\n",
        "    img_count = 0\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            shutil.copy(\n",
        "                os.path.join(image_dir, img_file),\n",
        "                f\"{data_dir}/images/{img_file}\"\n",
        "            )\n",
        "            img_count += 1\n",
        "    print(f\"Copied {img_count} images\")\n",
        "\n",
        "    # Convert and copy camera file to PINHOLE format\n",
        "    print(\"Converting camera model to PINHOLE format...\")\n",
        "    convert_cameras_to_pinhole(\n",
        "        os.path.join(colmap_model_dir, 'cameras.txt'),\n",
        "        f\"{data_dir}/sparse/0/cameras.txt\"\n",
        "    )\n",
        "\n",
        "    # Copy other files\n",
        "    for filename in ['images.txt', 'points3D.txt']:\n",
        "        src = os.path.join(colmap_model_dir, filename)\n",
        "        dst = f\"{data_dir}/sparse/0/{filename}\"\n",
        "        if os.path.exists(src):\n",
        "            shutil.copy(src, dst)\n",
        "            print(f\"Copied {filename}\")\n",
        "        else:\n",
        "            print(f\"Warning: {filename} not found\")\n",
        "\n",
        "    print(f\"Data preparation complete: {data_dir}\")\n",
        "    return data_dir\n",
        "\n",
        "\n",
        "\n",
        "def train_gaussian_splatting(data_dir, iterations=3000):\n",
        "    \"\"\"Train the Gaussian Splatting model\"\"\"\n",
        "    print(f\"Training Gaussian Splatting model for {iterations} iterations...\")\n",
        "\n",
        "    # Assumes WORK_DIR is defined globally or passed\n",
        "    model_path = f\"{WORK_DIR}/output/video\"\n",
        "\n",
        "    cmd = [\n",
        "        sys.executable, 'train.py',\n",
        "        '-s', data_dir,\n",
        "        '-m', model_path,\n",
        "        '--iterations', str(iterations),\n",
        "        '--eval' # Optionally run an evaluation phase\n",
        "    ]\n",
        "\n",
        "    # Execute the training script from the WORK_DIR\n",
        "    subprocess.run(cmd, cwd=WORK_DIR, check=True)\n",
        "\n",
        "    return model_path\n",
        "\n",
        "\n",
        "\n",
        "def render_video(model_path, output_video_path, iteration=3000):\n",
        "    \"\"\"Generate video from the trained model by rendering a sequence of views\"\"\"\n",
        "    print(\"Rendering video...\")\n",
        "\n",
        "    # Execute rendering\n",
        "    cmd = [\n",
        "        sys.executable, 'render.py',\n",
        "        '-m', model_path,\n",
        "        '--iteration', str(iteration)\n",
        "    ]\n",
        "\n",
        "    # Execute the rendering script from the WORK_DIR\n",
        "    subprocess.run(cmd, cwd=WORK_DIR, check=True)\n",
        "\n",
        "    # Find the rendering directory\n",
        "    possible_dirs = [\n",
        "        f\"{model_path}/test/ours_{iteration}/renders\",\n",
        "        f\"{model_path}/train/ours_{iteration}/renders\",\n",
        "    ]\n",
        "\n",
        "    render_dir = None\n",
        "    for test_dir in possible_dirs:\n",
        "        if os.path.exists(test_dir):\n",
        "            render_dir = test_dir\n",
        "            print(f\"Rendering directory found: {render_dir}\")\n",
        "            break\n",
        "\n",
        "    if render_dir and os.path.exists(render_dir):\n",
        "        # Sort rendered PNG images for correct video sequence\n",
        "        render_imgs = sorted([f for f in os.listdir(render_dir) if f.endswith('.png')])\n",
        "\n",
        "        if render_imgs:\n",
        "            print(f\"Found {len(render_imgs)} rendered images\")\n",
        "\n",
        "            # Create video with ffmpeg\n",
        "            # -y: overwrite output file without asking\n",
        "            # -framerate 30: set input framerate to 30 FPS\n",
        "            # -pattern_type glob -i: use glob pattern to specify input images\n",
        "            # -c:v libx264: use h.264 video codec\n",
        "            # -pix_fmt yuv420p: use a pixel format compatible with most players\n",
        "            # -crf 18: Constant Rate Factor (lower is higher quality, 18 is generally high quality)\n",
        "            subprocess.run([\n",
        "                'ffmpeg', '-y',\n",
        "                '-framerate', '30',\n",
        "                '-pattern_type', 'glob',\n",
        "                '-i', f\"{render_dir}/*.png\",\n",
        "                '-c:v', 'libx264',\n",
        "                '-pix_fmt', 'yuv420p',\n",
        "                '-crf', '18',\n",
        "                output_video_path\n",
        "            ], check=True)\n",
        "\n",
        "            print(f\"Video saved: {output_video_path}\")\n",
        "            return True\n",
        "\n",
        "    print(\"Error: Rendering directory not found or no images rendered\")\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def create_gif(video_path, gif_path):\n",
        "    \"\"\"Create an animated GIF from an MP4 video file\"\"\"\n",
        "    print(\"Creating animated GIF...\")\n",
        "\n",
        "    # ffmpeg command to create a GIF\n",
        "    # -vf: video filter graph\n",
        "    # setpts=8*PTS: slows down the video by a factor of 8 (8x original duration)\n",
        "    # fps=10: set output frame rate to 10 FPS\n",
        "    # scale=720:-1:flags=lanczos: resize to 720px width, auto height, using Lanczos resampling\n",
        "    # -loop 0: loop the GIF indefinitely\n",
        "    subprocess.run([\n",
        "        'ffmpeg', '-y',\n",
        "        '-i', video_path,\n",
        "        '-vf', 'setpts=8*PTS,fps=10,scale=720:-1:flags=lanczos',\n",
        "        '-loop', '0',\n",
        "        gif_path\n",
        "    ], check=True)\n",
        "\n",
        "    if os.path.exists(gif_path):\n",
        "        size_mb = os.path.getsize(gif_path) / (1024 * 1024)\n",
        "        print(f\"GIF creation complete: {gif_path} ({size_mb:.2f} MB)\")\n",
        "        return True\n",
        "\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "myF1NasbGgOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = train_gaussian_splatting(data_dir, iterations=1000)\n",
        "\n",
        "# Step 6: Render Video\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "output_video = f\"{OUTPUT_DIR}/gaussian_splatting_video.mp4\"\n",
        "success = render_video(model_path, output_video, iteration=1000)\n"
      ],
      "metadata": {
        "id": "JKJuclrCgMcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diagnose_specific_pair(database_path, id1, id2):\n",
        "    \"\"\"Diagnose a specific image pair\"\"\"\n",
        "    conn = sqlite3.connect(database_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    print(f\"\\n🔍 Diagnosing pair {id1}-{id2}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get match info\n",
        "    pair_id = id1 * 2147483648 + id2\n",
        "    cursor.execute(\n",
        "        \"SELECT rows, F FROM two_view_geometries WHERE pair_id=?\",\n",
        "        (pair_id,)\n",
        "    )\n",
        "    result = cursor.fetchone()\n",
        "\n",
        "    if result:\n",
        "        rows, F_blob = result\n",
        "        print(f\"  Matches: {rows}\")\n",
        "\n",
        "        if F_blob:\n",
        "            F = np.frombuffer(F_blob, dtype=np.float64).reshape(3, 3)\n",
        "            print(f\"  F matrix exists: {F.shape}\")\n",
        "            print(f\"  F matrix:\\n{F}\")\n",
        "        else:\n",
        "            print(\"  ⚠️ F matrix is NULL!\")\n",
        "    else:\n",
        "        print(f\"  ⚠️ Pair not found in two_view_geometries!\")\n",
        "\n",
        "    # Get camera info\n",
        "    cursor.execute(\"SELECT c.* FROM cameras c JOIN images i ON c.camera_id = i.camera_id WHERE i.image_id IN (?, ?)\", (id1, id2))\n",
        "    print(\"\\n  Cameras:\")\n",
        "    for row in cursor.fetchall():\n",
        "        cam_id, model, w, h, params_blob, prior = row\n",
        "        params = np.frombuffer(params_blob, dtype=np.float64)\n",
        "        print(f\"    Camera {cam_id}: {w}x{h}, model={model}, params={params}\")\n",
        "\n",
        "    conn.close()\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 実行\n",
        "diagnose_specific_pair('/content/output/colmap/database.db', 11, 20)"
      ],
      "metadata": {
        "id": "CtinWBxmWjFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "\n",
        "def diagnose_database(database_path):\n",
        "    \"\"\"Diagnose why COLMAP can't find initial pair\"\"\"\n",
        "    conn = sqlite3.connect(database_path)\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    print(\"\\n🔍 Database Diagnosis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get match statistics\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT pair_id, rows, config\n",
        "        FROM two_view_geometries\n",
        "        ORDER BY rows DESC\n",
        "        LIMIT 10\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\nTop 10 matches by count:\")\n",
        "    for pair_id, rows, config in cursor.fetchall():\n",
        "        image_id2 = pair_id % 2147483648\n",
        "        image_id1 = (pair_id - image_id2) // 2147483648\n",
        "        print(f\"  Images {image_id1}-{image_id2}: {rows} matches, config={config}\")\n",
        "\n",
        "    # Get image and camera info\n",
        "    cursor.execute(\"\"\"\n",
        "        SELECT i.image_id, i.name, i.camera_id, c.width, c.height\n",
        "        FROM images i\n",
        "        JOIN cameras c ON i.camera_id = c.camera_id\n",
        "        LIMIT 5\n",
        "    \"\"\")\n",
        "\n",
        "    print(\"\\nSample images:\")\n",
        "    for img_id, name, cam_id, w, h in cursor.fetchall():\n",
        "        print(f\"  Image {img_id}: {name}, camera {cam_id} ({w}x{h})\")\n",
        "\n",
        "    conn.close()\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# 実行\n",
        "diagnose_database('/content/output/colmap/database.db')"
      ],
      "metadata": {
        "id": "wUW7ZnT99nOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dq2Hln8-FKNu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}