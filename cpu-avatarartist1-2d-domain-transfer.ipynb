{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/stpeteishii/cpu-avatarartist1-2d-domain-transfer?scriptVersionId=288388234\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CPU AvatarArtist1: 2D Domain Transfer**","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **What is AvatarArtist?**\nAvatarArtist is a cutting-edge AI system that creates high-quality 3D avatars from text descriptions or 2D images. It can transform a simple photo or text prompt into a fully-realized 3D character in various artistic styles.\n\nhttps://kumapowerliu.github.io/AvatarArtist/","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"---\n\n## **About This Notebook**\n\n**This notebook demonstrates a simplified, CPU-compatible version of the first step (Step 1: 2D Domain Transfer) from the complete pipeline described in the paper.**\n\n### Limitations\n\n- ‚ö†Ô∏è **It is NOT possible to execute the full process and achieve the intended performance on CPU alone**\n- This notebook is intended for educational purposes and algorithm understanding\n- To obtain practical results, execution of the complete pipeline with GPU is required\n\n### Complete Pipeline\n\nThe full AvatarArtist pipeline consists of 5 steps:\n\n| Step | Description | \n|------|-------------|\n| **Step 1** | 2D Domain Transfer | \n| **Step 2** | NeXT3D & 4D-GAN Fine-tuning | \n| **Step 3** | Triplane Decomposition | \n| **Step 4** | Diffusion Transformer Training |\n| **Step 5** | Avatar Generation (Inference) | \n\n### Reference\n\nüìÑ **Original Paper**: [AvatarArtist Project Page](https://kumapowerliu.github.io/AvatarArtist/)\n\n---\n\n## üìä Pipeline Overview \n\n```\nStep 1: 2D Domain Transfer (‚Üê You are here )\n   ‚Üì\nStep 2: NeXT3D & 4D-GAN Fine-tuning\n   ‚Üì\nStep 3: Triplane Decomposition\n   ‚Üì\nStep 4: Diffusion Transformer Training\n   ‚Üì\nStep 5: Avatar Generation (Inference)\n```\n\n---\n\n\n\n## ‚öôÔ∏è System Requirements \n\n### This Notebook (Step 1 - CPU) \n| Component | Minimum | Recommended |\n|-----------|---------|-------------|\n| CPU | Multi-core processor | Intel i7/AMD Ryzen 7+ |\n| RAM | 8GB | 16GB+ |\n| Storage | 5GB | 10GB+ |\n| Time per image | 5-10 minutes | 3-5 minutes |\n\n### Full Pipeline (GPU) \n| Component | Minimum | Recommended |\n|-----------|---------|-------------|\n| GPU | NVIDIA RTX 3060 (12GB) | NVIDIA RTX 3090/4090 (24GB+) |\n| RAM | 16GB | 32GB+ |\n| Storage | 50GB | 100GB+ |\n| Time per avatar | 2-4 hours | 1-2 hours |\n\n---","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **Step 1 Pipeline Explanation: 2D Domain Transfer (Style Transfer)**\n\n## Overview\nThis is the **foundation pipeline** that transforms real-life photographs into artistic avatar styles (Pixar, anime, LEGO, etc.). It uses **Stable Diffusion + ControlNet** to maintain facial structure while completely changing the artistic style. The output becomes the training data for Step 2.\n\n---\n\n## Pipeline Purpose\n\n```\nInput: Real photograph (512√ó512)\n   ‚Üì\n[ControlNet] Extract pose/structure\n   ‚Üì\n[Stable Diffusion] Apply artistic style\n   ‚Üì\nOutput: Stylized avatar (512√ó512)\n```\n\n**Key Goal**: Generate hundreds of style-consistent avatar images that will be used to train the 3D-aware GAN (Next3D) in Step 2.\n\n---\n\n## Core Technologies\n\n### **1. Stable Diffusion**\n\n**What it is:**\n- State-of-the-art text-to-image diffusion model\n- Can generate high-quality images from text prompts\n- Version used: v1.5 (runwayml) or v2.1 (stabilityai)\n\n**How it works:**\n```\nText Prompt ‚Üí CLIP Encoder ‚Üí Latent Space ‚Üí U-Net Denoising ‚Üí VAE Decoder ‚Üí Image\n```\n\n**Role in this pipeline:**\n- Generates images in target artistic style\n- Maintains semantic meaning (face, features)\n- Produces high-quality, consistent outputs\n\n---\n\n### **2. ControlNet**\n\n**What it is:**\n- Neural network that adds spatial control to Stable Diffusion\n- Preserves structural information (pose, edges) during generation\n- Ensures face position/proportions stay consistent\n\n**Available Control Types:**\n\n| Control Type | What It Preserves | Speed | Best For |\n|--------------|-------------------|-------|----------|\n| **OpenPose** | Body/face keypoints | Slower | Full body portraits |\n| **Canny** | Edge outlines | Faster | Face-only images |\n\n**Why we need it:**\n```\nWithout ControlNet:\n  Input: Photo of person\n  Prompt: \"Pixar style face\"\n  Result: Random Pixar character (wrong pose, different person)\n\nWith ControlNet:\n  Input: Photo of person + Pose skeleton\n  Prompt: \"Pixar style face\"\n  Result: Same person, same pose, Pixar style ‚úì\n```\n\n**Example Process:**\n\n```python\n# 1. Extract control signal\nInput Image ‚Üí OpenPose Detector ‚Üí Skeleton/Keypoints\n                                    (18 body points + 70 face points)\n\n# 2. Guide generation\nSkeleton + Text Prompt ‚Üí ControlNet + SD ‚Üí Stylized Image\n                                           (same pose, new style)\n```\n\n---\n\n### **3. SDEdit (Stochastic Differential Editing)**\n\n**Concept:**\n- Add controlled noise to input image\n- Denoise with style prompt\n- Result: Modified image that maintains structure\n\n**Noise Strength Parameter:**\n\n| Strength | Effect | Use Case |\n|----------|--------|----------|\n| 0.0 | No change | Minimal style transfer |\n| 0.3 | Subtle style | Keep more original features |\n| 0.5 | Balanced | Standard transformation |\n| 0.7 | Heavy style | Maximum artistic change |\n| 1.0 | Complete regeneration | Full style replacement |\n\n**Process:**\n```\nOriginal Image (t=0)\n   ‚Üì Add noise (strength=0.5)\nNoisy Image (t=500)\n   ‚Üì Denoise with style prompt (500 steps)\nStyled Image (t=0)\n```\n\n---\n\n## Architecture Components\n\n### **Class: AvatarArtist2D**\n\n#### **Initialization Parameters:**\n\n```python\nAvatarArtist2D(\n    model_id=\"runwayml/stable-diffusion-v1-5\",\n    controlnet_model=\"lllyasviel/sd-controlnet-openpose\",\n    device=\"cpu\",  # or \"cuda\"\n    dtype=torch.float32,  # CPU requires float32\n    use_canny=False,  # True for faster edge-based control\n    hf_token=None  # Hugging Face authentication token\n)\n```\n\n**Parameter Guide:**\n\n| Parameter | Options | Purpose | Notes |\n|-----------|---------|---------|-------|\n| `model_id` | SD v1.5, v2.1 | Base diffusion model | v1.5 = no token, v2.1 = token needed |\n| `controlnet_model` | OpenPose, Canny | Control type | Auto-selected based on model_id |\n| `device` | \"cpu\", \"cuda\" | Computing device | GPU 10-50√ó faster |\n| `dtype` | float32, float16 | Precision | CPU requires float32 |\n| `use_canny` | True/False | Edge vs Pose | True = faster, False = more accurate |\n\n---\n\n### **Model Loading Process**\n\n**Step-by-Step:**\n\n1. **Load ControlNet**\n   ```python\n   controlnet = ControlNetModel.from_pretrained(\n       \"lllyasviel/sd-controlnet-openpose\"\n   )\n   ```\n   - Size: ~1.5 GB\n   - Contains: Encoder that processes control images\n\n2. **Load Stable Diffusion Pipeline**\n   ```python\n   pipe = StableDiffusionControlNetPipeline.from_pretrained(\n       \"runwayml/stable-diffusion-v1-5\",\n       controlnet=controlnet\n   )\n   ```\n   - Size: ~4-5 GB total\n   - Contains: Text encoder, U-Net, VAE decoder\n\n3. **Configure Scheduler**\n   ```python\n   pipe.scheduler = DDIMScheduler.from_config(...)\n   ```\n   - DDIM: Fast, deterministic sampling\n   - Alternative: UniPC (fewer steps needed)\n\n4. **CPU Optimizations**\n   ```python\n   pipe.enable_attention_slicing(slice_size=1)\n   pipe.enable_vae_slicing()\n   ```\n   - Reduces memory usage by ~50%\n   - Essential for CPU/low-VRAM GPUs\n   - Slight slowdown (~10%) but prevents OOM\n\n---\n\n### **Control Signal Extraction**\n\n#### **OpenPose Detection:**\n\n```python\nprocessor = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\ncontrol_image = processor(input_image)\n```\n\n**Output:**\n- Skeleton with 18 body keypoints\n- 70 facial landmarks (eyes, nose, mouth, jaw)\n- Colored visualization (different colors = different body parts)\n\n**Keypoint Categories:**\n```\nBody: Neck, shoulders, elbows, wrists, hips, knees, ankles (18 points)\nFace: Eyes, eyebrows, nose, mouth, jaw contour (70 points)\n```\n\n#### **Canny Edge Detection:**\n\n```python\nprocessor = CannyDetector()\ncontrol_image = processor(input_image)\n```\n\n**Output:**\n- Binary edge map (black/white)\n- Detects gradients > threshold\n- Faster than OpenPose (~5√ó speed)\n\n**Comparison:**\n\n| Aspect | OpenPose | Canny |\n|--------|----------|-------|\n| **Speed** | Slow (2-5 sec/image) | Fast (0.1-0.5 sec) |\n| **Accuracy** | High (semantic) | Medium (edges only) |\n| **Face Detail** | Excellent (70 points) | Good (outline) |\n| **CPU Usage** | Higher | Lower |\n| **Recommended** | GPU, high quality | CPU, speed priority |\n\n---\n\n## Processing Workflow\n\n### **Single Image Process:**\n\n```python\nprocess_single_image(\n    image_path=\"input.jpg\",\n    output_path=\"output.png\",\n    style_prompt=\"Pixar animation style\",\n    noise_strength=0.5,\n    controlnet_strength=1.0,\n    guidance_scale=7.5,\n    num_steps=50,\n    seed=42\n)\n```\n\n**Step-by-Step Execution:**\n\n1. **Load and Resize**\n   ```python\n   image = load_image(\"input.jpg\")\n   image = image.resize((512, 512))  # SD v1.5 optimal size\n   ```\n\n2. **Extract Control Signal**\n   ```python\n   control_image = processor(image)\n   # Returns skeleton/edges matching input pose\n   ```\n\n3. **Style Transfer**\n   ```python\n   output = pipe(\n       prompt=style_prompt,\n       image=control_image,  # Structural guidance\n       num_inference_steps=50,  # Denoising iterations\n       guidance_scale=7.5,  # Prompt adherence\n       controlnet_conditioning_scale=1.0,  # Control strength\n       generator=torch.Generator().manual_seed(42)\n   )\n   ```\n\n4. **Save Result**\n   ```python\n   output.images[0].save(\"output.png\")\n   ```\n\n---\n\n## Key Parameters Explained\n\n### **1. Style Prompt**\n\n**Format:**\n```python\n\"[subject] in [style], [quality modifiers]\"\n```\n\n**Examples:**\n\n```python\nSTYLE_PROMPTS = {\n    \"pixar\": \"a 3D render of a face in Pixar animation style, \"\n             \"high quality, detailed, professional lighting\",\n    \n    \"anime\": \"anime style portrait, cel shaded, vibrant colors, \"\n             \"expressive eyes, detailed\",\n    \n    \"lego\": \"LEGO minifigure face, plastic texture, \"\n            \"simplified features, toy style\",\n    \n    \"cartoon\": \"cartoon style portrait, bold lines, \"\n               \"vibrant colors, simplified features\",\n    \n    \"oil_painting\": \"oil painting portrait, classical style, \"\n                    \"rich colors, brushstrokes visible\"\n}\n```\n\n**Prompt Engineering Tips:**\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Subject** | What to generate | \"face\", \"portrait\", \"person\" |\n| **Style** | Artistic direction | \"Pixar\", \"anime\", \"watercolor\" |\n| **Quality** | Improve output | \"high quality\", \"detailed\", \"professional\" |\n| **Lighting** | Visual enhancement | \"soft lighting\", \"dramatic shadows\" |\n| **Technical** | Format/medium | \"3D render\", \"digital art\", \"oil painting\" |\n\n**Negative Prompts** (what to avoid):\n```python\nnegative_prompt = \"blurry, low quality, distorted, ugly, bad anatomy\"\n```\n\n---\n\n### **2. Noise Strength**\n\n**Controls how much to modify the image:**\n\n```python\nnoise_strength = 0.5  # Range: 0.0 - 1.0\n```\n\n| Value | Effect | Output Characteristics | Use Case |\n|-------|--------|------------------------|----------|\n| **0.1-0.2** | Minimal | Slight color/texture change | Subtle enhancement |\n| **0.3-0.4** | Light | Recognizable person + style hints | Balanced realism |\n| **0.5-0.6** | Medium | Clear style, some original features | **Recommended** |\n| **0.7-0.8** | Heavy | Full style, minimal original | Artistic freedom |\n| **0.9-1.0** | Complete | Entirely new generation | Max creativity |\n\n**Visual Comparison:**\n```\nNoise 0.2: Photo ‚Üí Photo with slight cartoon coloring\nNoise 0.5: Photo ‚Üí Clear Pixar character (same person)\nNoise 0.8: Photo ‚Üí Full Pixar character (generic features)\n```\n\n---\n\n### **3. ControlNet Strength**\n\n**Controls how strictly to follow the input structure:**\n\n```python\ncontrolnet_conditioning_scale = 1.0  # Range: 0.0 - 2.0\n```\n\n| Value | Effect | Structural Fidelity | Use Case |\n|-------|--------|---------------------|----------|\n| **0.3-0.5** | Loose | Approximate pose | Creative variation |\n| **0.7-0.9** | Moderate | Close match | Balanced |\n| **1.0** | Standard | Exact match | **Recommended** |\n| **1.2-1.5** | Strong | Very precise | Technical accuracy |\n| **1.8-2.0** | Maximum | Rigid adherence | Exact reproduction |\n\n**Trade-off:**\n- **Higher** = More faithful to original pose/structure\n- **Lower** = More creative freedom in composition\n\n---\n\n### **4. Guidance Scale**\n\n**Controls prompt following strength:**\n\n```python\nguidance_scale = 7.5  # Range: 1.0 - 20.0\n```\n\n| Value | Effect | Output Quality | Use Case |\n|-------|--------|----------------|----------|\n| **1-3** | Weak | Ignores prompt, generic | Not recommended |\n| **5-7** | Moderate | Balanced style + creativity | Artistic freedom |\n| **7.5** | Standard | Clear style adherence | **Recommended** |\n| **10-12** | Strong | Strict prompt following | Precise results |\n| **15-20** | Extreme | Over-saturated, artifacts | Experimental |\n\n**CFG (Classifier-Free Guidance) Formula:**\n```\noutput = unconditional_output + guidance_scale √ó (conditional - unconditional)\n```\n\n---\n\n### **5. Inference Steps**\n\n**Number of denoising iterations:**\n\n```python\nnum_inference_steps = 50  # Range: 10 - 150\n```\n\n| Steps | Quality | Time (GPU) | Time (CPU) | Use Case |\n|-------|---------|------------|------------|----------|\n| **10-15** | Low | 2-3 sec | 1-2 min | Preview |\n| **20-30** | Medium | 4-6 sec | 3-5 min | Fast production |\n| **50** | High | 8-10 sec | 8-12 min | **Recommended** |\n| **75-100** | Very High | 15-20 sec | 15-20 min | Maximum quality |\n\n**Diminishing Returns:**\n- Steps 10‚Üí25: Major quality improvement\n- Steps 25‚Üí50: Noticeable improvement\n- Steps 50‚Üí100: Marginal improvement\n- Steps >100: Minimal difference\n\n---\n\n## Style Presets Explained\n\n### **Pixar Style**\n\n```python\nprompt = \"a 3D render of a face in Pixar animation style, \n          high quality, detailed, professional lighting\"\n\nnoise_strength = 0.4-0.5\ncontrolnet_strength = 0.8-1.0\nguidance_scale = 7.5\n```\n\n**Characteristics:**\n- Smooth, rounded features\n- Large, expressive eyes\n- Soft lighting and shadows\n- Cartoon proportions with realism\n- Clean, polished appearance\n\n**Best For:** General-purpose avatars, friendly characters\n\n---\n\n### **Anime Style**\n\n```python\nprompt = \"anime style portrait, cel shaded, vibrant colors, \n          expressive eyes, detailed\"\n\nnoise_strength = 0.5-0.6\ncontrolnet_strength = 0.7-0.9\nguidance_scale = 8.0\n```\n\n**Characteristics:**\n- Large eyes with highlights\n- Simplified nose (small triangle)\n- Cel shading (flat colors with sharp shadows)\n- Colorful hair and features\n- Stylized proportions\n\n**Best For:** Manga-style characters, gaming avatars\n\n---\n\n### **LEGO Style**\n\n```python\nprompt = \"LEGO minifigure face, plastic texture, \n          simplified features, toy style\"\n\nnoise_strength = 0.6-0.7\ncontrolnet_strength = 1.0\nguidance_scale = 9.0\n```\n\n**Characteristics:**\n- Cylindrical head shape\n- Minimal facial features (dots, curves)\n- Solid colors\n- Plastic/toy appearance\n- High contrast\n\n**Best For:** Gaming, collectibles, fun avatars\n\n---\n\n### **Oil Painting Style**\n\n```python\nprompt = \"oil painting portrait, classical style, \n          rich colors, brushstrokes visible\"\n\nnoise_strength = 0.4-0.5\ncontrolnet_strength = 0.9\nguidance_scale = 7.0\n```\n\n**Characteristics:**\n- Visible brushstrokes\n- Textured appearance\n- Rich, deep colors\n- Artistic imperfections\n- Classical lighting\n\n**Best For:** Artistic profiles, formal portraits\n\n---\n\n## Batch Processing\n\n### **Process Multiple Images:**\n\n```python\nprocess_batch(\n    input_dir=\"./input_images\",\n    output_dir=\"./output_styled\",\n    style_prompt=STYLE_PROMPTS[\"pixar\"],\n    noise_strength=0.5,\n    controlnet_strength=1.0,\n    guidance_scale=7.5,\n    num_steps=50,\n    seed=42\n)\n```\n\n**Features:**\n- Processes all .jpg, .jpeg, .png files in input folder\n- Creates output folder automatically\n- Progress tracking for each image\n- Error handling (continues even if one image fails)\n- Consistent seed = reproducible results\n\n**Output Naming:**\n```\ninput_images/\n  ‚îú‚îÄ person1.jpg\n  ‚îú‚îÄ person2.jpg\n  ‚îî‚îÄ person3.png\n\noutput_styled/\n  ‚îú‚îÄ styled_person1.jpg\n  ‚îú‚îÄ styled_person2.jpg\n  ‚îî‚îÄ styled_person3.png\n```\n\n---\n\n## Output Quality Assessment\n\n### **Good Results:**\n\n‚úÖ **Structural Preservation:**\n- Face position matches original\n- Facial proportions correct\n- Expression maintained\n- Pose identical\n\n‚úÖ **Style Consistency:**\n- Clear artistic style visible\n- Consistent across batch\n- No realistic photo elements\n- Professional appearance\n\n‚úÖ **Quality:**\n- No artifacts or distortions\n- Sharp details\n- Proper lighting\n- Natural colors within style\n\n---\n\n### **Poor Results:**\n\n‚ùå **Common Issues:**\n\n**1. Wrong Face Shape**\n- **Cause**: ControlNet strength too low\n- **Solution**: Increase to 0.9-1.0\n\n**2. Not Enough Style**\n- **Cause**: Noise strength too low\n- **Solution**: Increase to 0.5-0.6\n\n**3. Over-Styled (Unrecognizable)**\n- **Cause**: Noise strength too high\n- **Solution**: Decrease to 0.3-0.4\n\n**4. Ignores Prompt**\n- **Cause**: Guidance scale too low\n- **Solution**: Increase to 8.0-10.0\n\n**5. Artifacts/Distortions**\n- **Cause**: Guidance scale too high\n- **Solution**: Decrease to 6.0-7.0\n\n---\n\n## CPU vs GPU Performance\n\n### **Speed Comparison (per 512√ó512 image, 50 steps):**\n\n| Device | Control Extraction | Style Transfer | Total Time |\n|--------|-------------------|----------------|------------|\n| **CPU (8 cores)** | 2-5 sec | 8-15 min | **8-15 min** |\n| **GPU (GTX 1080)** | 0.5 sec | 8-12 sec | **10-15 sec** |\n| **GPU (RTX 3090)** | 0.3 sec | 4-6 sec | **5-8 sec** |\n| **GPU (A100)** | 0.2 sec | 2-3 sec | **2-4 sec** |\n\n### **Batch Processing Time:**\n\n**100 images:**\n- CPU: 13-25 hours\n- GPU (GTX 1080): 15-25 minutes\n- GPU (RTX 3090): 8-15 minutes\n\n**Recommendation:** Use GPU for production batches, CPU only for testing/small batches\n\n---\n\n## Memory Requirements\n\n### **Model Loading:**\n\n| Component | Size | Purpose |\n|-----------|------|---------|\n| Stable Diffusion v1.5 | 4 GB | Base model |\n| ControlNet OpenPose | 1.5 GB | Pose control |\n| ControlNet Canny | 1.5 GB | Edge control |\n| Working Memory | 2-4 GB | Inference |\n| **Total (OpenPose)** | **7-9 GB** | |\n| **Total (Canny)** | **7-9 GB** | |\n\n### **Hardware Recommendations:**\n\n| Device | RAM | VRAM | Batch Size | Steps |\n|--------|-----|------|------------|-------|\n| **CPU** | 16 GB | N/A | 1 | 30-50 |\n| **GPU (8GB)** | 8 GB | 8 GB | 1 | 50 |\n| **GPU (12GB)** | 8 GB | 12 GB | 1-2 | 50-75 |\n| **GPU (24GB)** | 16 GB | 24 GB | 2-4 | 75-100 |\n\n---\n\n## Common Issues & Solutions\n\n### **Problem 1: Hugging Face Authentication Error**\n\n**Symptoms:**\n```\n‚ùå Authentication Error: This model requires a Hugging Face token\n```\n\n**Solutions:**\n\n**Method 1: CLI Login**\n```bash\npip install huggingface_hub\nhuggingface-cli login\n# Enter token when prompted\n```\n\n**Method 2: Environment Variable**\n```bash\nexport HF_TOKEN=\"hf_your_token_here\"\n```\n\n**Method 3: Code Parameter**\n```python\nartist = AvatarArtist2D(hf_token=\"hf_your_token_here\")\n```\n\n**Method 4: Use Token-Free Model**\n```python\nMODEL_ID = \"runwayml/stable-diffusion-v1-5\"  # No token needed\n```\n\n---\n\n### **Problem 2: Out of Memory (OOM)**\n\n**Symptoms:**\n```\nRuntimeError: CUDA out of memory\n```\n\n**Solutions:**\n\n```python\n# 1. Enable memory optimizations\npipe.enable_attention_slicing(slice_size=1)\npipe.enable_vae_slicing()\n\n# 2. Reduce inference steps\nNUM_STEPS = 30  # vs 50\n\n# 3. Use Canny instead of OpenPose\nUSE_CANNY = True  # Faster, less memory\n\n# 4. Process one image at a time\n# Don't batch multiple images simultaneously\n\n# 5. Switch to CPU (slow but works)\ndevice = \"cpu\"\n```\n\n---\n\n### **Problem 3: ControlNet Not Loading**\n\n**Symptoms:**\n```\nError: Failed to load lllyasviel/sd-controlnet-openpose\n```\n\n**Solutions:**\n\n**Automatic Fallback:**\n```python\n# Code automatically tries Canny if OpenPose fails\ntry:\n    controlnet = load(\"openpose\")\nexcept:\n    controlnet = load(\"canny\")  # Fallback\n    use_canny = True\n```\n\n**Manual Selection:**\n```python\n# Force Canny from start\nartist = AvatarArtist2D(use_canny=True)\n```\n\n---\n\n### **Problem 4: Style Not Applied**\n\n**Symptoms:**\n- Output looks like original photo\n- No artistic style visible\n\n**Solutions:**\n\n```python\n# Increase noise strength\nnoise_strength = 0.6  # vs 0.3-0.4\n\n# Increase guidance scale\nguidance_scale = 9.0  # vs 7.5\n\n# Reduce ControlNet strength (allow more freedom)\ncontrolnet_strength = 0.7  # vs 1.0\n\n# Improve prompt\nprompt = \"highly detailed Pixar 3D animated character, \" \\\n         \"professional render, studio lighting, \" \\\n         \"smooth surfaces, expressive features\"\n```\n\n---\n\n### **Problem 5: Face Position Wrong**\n\n**Symptoms:**\n- Face moved to different position\n- Proportions changed\n- Pose doesn't match\n\n**Solutions:**\n\n```python\n# Increase ControlNet strength\ncontrolnet_strength = 1.2  # vs 0.8-1.0\n\n# Use OpenPose instead of Canny\nuse_canny = False  # More precise control\n\n# Check control image visually\ncontrol_image.save(\"debug_control.png\")\n# Should show skeleton/edges matching input\n```\n\n---\n\n### **Problem 6: Slow CPU Performance**\n\n**Expected:** CPU is 50-100√ó slower than GPU\n\n**Optimizations:**\n\n```python\n# 1. Use Canny (5√ó faster than OpenPose)\nuse_canny = True\n\n# 2. Reduce steps (proportional speedup)\nnum_steps = 20  # vs 50 (2.5√ó faster)\n\n# 3. Enable all optimizations\npipe.enable_attention_slicing(slice_size=1)\npipe.enable_vae_slicing()\n\n# 4. Process smaller batches overnight\n# 5. Consider cloud GPU (Google Colab, AWS, etc.)\n```\n\n---\n\n## Integration with Next Steps\n\n### **Output Requirements for Step 2:**\n\n**Quantity:**\n- Minimum: 100 images\n- Recommended: 500-1000 images\n- Optimal: 2000+ images\n\n**Quality:**\n- Clear artistic style\n- Consistent across batch\n- Proper face structure\n- No major artifacts\n\n**Diversity:**\n- Different people\n- Various expressions\n- Multiple angles\n- Age/gender variety\n\n### **Validation Before Step 2:**\n\n```python\n# Check output quality\nfor img_path in output_images:\n    img = Image.open(img_path)\n    \n    # Size check\n    assert img.size == (512, 512), \"Wrong size\"\n    \n    # Visual inspection\n    # - Clear style?\n    # - Face recognizable?\n    # - No artifacts?\n```\n\n---\n\n## Summary\n\nThis pipeline:\n\n‚úÖ **Converts** real photos ‚Üí artistic avatars  \n‚úÖ **Preserves** facial structure and pose  \n‚úÖ **Applies** consistent artistic style  \n‚úÖ **Supports** multiple styles (Pixar, anime, LEGO, etc.)  \n‚úÖ **Works** on CPU and GPU (GPU strongly recommended)  \n‚úÖ **Processes** single images or batches  \n‚úÖ **Handles** errors gracefully with fallbacks\n\n**Key Parameters:**\n- **Noise Strength**: 0.4-0.6 (style intensity)\n- **ControlNet Strength**: 0.8-1.0 (structure preservation)\n- **Guidance Scale**: 7.5 (prompt adherence)\n- **Steps**: 30-50 (quality vs speed)\n\n**Performance:**\n- **GPU**: 5-10 seconds per image\n- **CPU**: 8-15 minutes per image\n\n**Output:** High-quality stylized avatars ready for 3D avatar training in Steps 2-5.\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision\n!pip install diffusers transformers accelerate\n!pip install controlnet-aux opencv-python pillow\n!pip install mediapipe==0.10.13","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T11:24:47.417495Z","iopub.execute_input":"2025-12-24T11:24:47.417924Z","iopub.status.idle":"2025-12-24T11:24:47.533492Z","shell.execute_reply.started":"2025-12-24T11:24:47.417891Z","shell.execute_reply":"2025-12-24T11:24:47.532024Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token0 = user_secrets.get_secret(\"secret_hf_token\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\nimport random\n\npaths=[]\nfor dirname, _, filenames in os.walk('/kaggle/input/pins-face-recognition/105_classes_pins_dataset'):\n    for filename in filenames:\n        paths+=[(os.path.join(dirname, filename))]\nprint(paths[0:6])\nrandom.shuffle(paths)\n\nos.makedirs(\"input_images\", exist_ok=True)\nfor path in paths[0:6]:\n    shutil.copy(path, \"input_images\")\n\nfor dirname, _, filenames in os.walk('./input_images'):\n    for filename in filenames:\n        print(filename)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nAvatarArtist: 2D Domain Transfer Script (CPU Optimized)\nConverts real-life images into specific styles using \nStable Diffusion + ControlNet + SDEdit.\n\"\"\"\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import Optional, List, Tuple\nimport cv2\nfrom diffusers import (\n    StableDiffusionControlNetPipeline,\n    ControlNetModel,\n    DDIMScheduler,\n    UniPCMultistepScheduler\n)\nfrom diffusers.utils import load_image\nfrom controlnet_aux import OpenposeDetector, CannyDetector\n\n# MediaPipe is optional\ntry:\n    import mediapipe as mp\n    MEDIAPIPE_AVAILABLE = True\nexcept ImportError:\n    MEDIAPIPE_AVAILABLE = False\n    print(\"Warning: MediaPipe not available. Using ControlNet only for pose detection.\")\n\nclass AvatarArtist2D:\n    \"\"\"Main class for 2D domain transfer (CPU Optimized).\"\"\"\n    \n    def __init__(\n        self,\n        model_id: str = \"runwayml/stable-diffusion-v1-5\",\n        controlnet_model: str = \"lllyasviel/sd-controlnet-openpose\",\n        device: str = \"cpu\",\n        dtype: torch.dtype = torch.float32,  # CPU requires float32\n        use_canny: bool = False,\n        hf_token: Optional[str] = None\n    ):\n        \"\"\"\n        Args:\n            model_id: Path or ID for the Stable Diffusion model.\n            controlnet_model: Path or ID for the ControlNet model.\n            device: Computing device to use.\n            dtype: Data type (float32 for CPU).\n            use_canny: Use Canny edge detection (simpler and lightweight).\n            hf_token: Hugging Face token (optional).\n        \"\"\"\n        self.device = device\n        self.dtype = dtype\n        self.use_canny = use_canny\n        self.hf_token = hf_token or os.environ.get(\"HF_TOKEN\")\n        \n        print(f\"Using Model: {model_id}\")\n        print(f\"Device: {device.upper()}\")\n        print(f\"Data Type: {dtype}\")\n        print(\"Loading models...\")\n        \n        # Select ControlNet based on the base model\n        if \"stable-diffusion-v1-5\" in model_id or \"v1-5\" in model_id or \"v1-4\" in model_id:\n            if use_canny:\n                controlnet_model = \"lllyasviel/sd-controlnet-canny\"\n            else:\n                controlnet_model = \"lllyasviel/sd-controlnet-openpose\"\n        elif \"stable-diffusion-2\" in model_id:\n            if use_canny:\n                controlnet_model = \"thibaud/controlnet-sd21-canny-diffusers\"\n            else:\n                controlnet_model = \"thibaud/controlnet-sd21-openpose-diffusers\"\n        \n        print(f\"ControlNet: {controlnet_model}\")\n        \n        # Load ControlNet\n        try:\n            self.controlnet = ControlNetModel.from_pretrained(\n                controlnet_model,\n                torch_dtype=dtype,\n                token=self.hf_token\n            )\n            print(f\"‚úì ControlNet loaded successfully\")\n        except Exception as e:\n            print(f\"‚ö† Error: Failed to load {controlnet_model}\")\n            print(f\"  Details: {e}\")\n            print(\"Attempting fallback to Canny model...\")\n            try:\n                fallback_model = \"lllyasviel/sd-controlnet-canny\"\n                self.controlnet = ControlNetModel.from_pretrained(\n                    fallback_model,\n                    torch_dtype=dtype,\n                    token=self.hf_token\n                )\n                self.use_canny = True\n                print(f\"‚úì Using Canny ControlNet as fallback.\")\n            except Exception as e2:\n                raise Exception(f\"Failed to load any ControlNet model: {e2}\")\n        \n        # Stable Diffusion pipeline setup\n        try:\n            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n                model_id,\n                controlnet=self.controlnet,\n                torch_dtype=dtype,\n                safety_checker=None,\n                token=self.hf_token\n            )\n            print(f\"‚úì Stable Diffusion loaded successfully\")\n        except Exception as e:\n            error_msg = str(e)\n            if \"gated\" in error_msg.lower() or \"token\" in error_msg.lower():\n                raise Exception(\n                    f\"\\n{'='*60}\\n\"\n                    f\"üîê Authentication Error: This model requires a Hugging Face token.\\n\"\n                    f\"\\nInstructions:\"\n                    f\"\\n1. Get a token at: https://huggingface.co/settings/tokens\"\n                    f\"\\n2. Set it using one of these methods:\"\n                    f\"\\n   a) export HF_TOKEN='your_token'\"\n                    f\"\\n   b) huggingface-cli login\"\n                    f\"\\n   c) artist = AvatarArtist2D(hf_token='your_token')\"\n                    f\"\\n\\nAlternatively, use a model that doesn't require a token:\"\n                    f\"\\n   model_id='runwayml/stable-diffusion-v1-5'\"\n                    f\"\\n{'='*60}\\n\"\n                )\n            raise\n        \n        # Scheduler configuration (SDEdit compatible)\n        self.pipe.scheduler = DDIMScheduler.from_config(\n            self.pipe.scheduler.config\n        )\n        \n        self.pipe = self.pipe.to(device)\n        \n        # CPU Optimization: Enable attention slicing to reduce memory usage\n        self.pipe.enable_attention_slicing(slice_size=1)\n        \n        # CPU Optimization: Enable VAE slicing\n        if hasattr(self.pipe, 'enable_vae_slicing'):\n            self.pipe.enable_vae_slicing()\n            print(\"‚úì VAE slicing enabled for memory efficiency\")\n        \n        print(f\"‚úì Memory optimization enabled for CPU\")\n        \n        # Control image processor\n        if self.use_canny:\n            print(\"Initializing Canny detector...\")\n            self.processor = CannyDetector()\n        else:\n            print(\"Loading Openpose processor...\")\n            try:\n                self.processor = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n            except Exception as e:\n                print(f\"Warning: OpenPose load failed: {e}\")\n                print(\"Falling back to Canny...\")\n                self.processor = CannyDetector()\n                self.use_canny = True\n        \n        # MediaPipe Face Detection (Optional: for more detailed control)\n        self.face_mesh = None\n        if MEDIAPIPE_AVAILABLE:\n            try:\n                mp_face_mesh = mp.solutions.face_mesh\n                self.face_mesh = mp_face_mesh.FaceMesh(\n                    static_image_mode=True,\n                    max_num_faces=1,\n                    min_detection_confidence=0.5\n                )\n                print(\"MediaPipe face detection enabled.\")\n            except Exception as e:\n                print(f\"MediaPipe initialization failed: {e}\")\n                self.face_mesh = None\n        \n        print(\"Initialization complete!\")\n    \n    def extract_pose_landmarks(self, image: Image.Image) -> Image.Image:\n        \"\"\"Extract control image from input (OpenPose or Canny).\"\"\"\n        control_image = self.processor(image)\n        return control_image\n    \n    def extract_face_landmarks(self, image: Image.Image) -> Optional[np.ndarray]:\n        \"\"\"Extract face landmarks using MediaPipe.\"\"\"\n        if self.face_mesh is None:\n            return None\n            \n        try:\n            image_np = np.array(image)\n            results = self.face_mesh.process(cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n            \n            if results.multi_face_landmarks:\n                landmarks = results.multi_face_landmarks[0]\n                h, w = image_np.shape[:2]\n                points = np.array([\n                    [lm.x * w, lm.y * h] \n                    for lm in landmarks.landmark\n                ])\n                return points\n        except Exception as e:\n            print(f\"Warning: Face landmark extraction failed: {e}\")\n        return None\n    \n    def apply_sdedit(\n        self,\n        image: Image.Image,\n        prompt: str,\n        control_image: Image.Image,\n        noise_strength: float = 0.5,\n        controlnet_conditioning_scale: float = 1.0,\n        guidance_scale: float = 7.5,\n        num_inference_steps: int = 50,\n        seed: Optional[int] = None\n    ) -> Image.Image:\n        \"\"\"Perform domain transfer applying SDEdit logic.\"\"\"\n        if seed is not None:\n            generator = torch.Generator(device=self.device).manual_seed(seed)\n        else:\n            generator = None\n        \n        output = self.pipe(\n            prompt=prompt,\n            image=control_image,\n            num_inference_steps=num_inference_steps,\n            guidance_scale=guidance_scale,\n            controlnet_conditioning_scale=controlnet_conditioning_scale,\n            generator=generator,\n        )\n        return output.images[0]\n    \n    def process_single_image(\n        self,\n        image_path: str,\n        output_path: str,\n        style_prompt: str,\n        noise_strength: float = 0.5,\n        controlnet_strength: float = 1.0,\n        guidance_scale: float = 7.5,\n        num_steps: int = 50,\n        seed: Optional[int] = None\n    ) -> bool:\n        \"\"\"Process a single image.\"\"\"\n        try:\n            image = load_image(image_path)\n            image = image.resize((512, 512))\n            \n            print(f\"  Extracting control image...\")\n            control_image = self.extract_pose_landmarks(image)\n            \n            print(f\"  Transforming style (this may take a while on CPU)...\")\n            output_image = self.apply_sdedit(\n                image=image,\n                prompt=style_prompt,\n                control_image=control_image,\n                noise_strength=noise_strength,\n                controlnet_conditioning_scale=controlnet_strength,\n                guidance_scale=guidance_scale,\n                num_inference_steps=num_steps,\n                seed=seed\n            )\n            \n            output_image.save(output_path)\n            print(f\"  Saved to: {output_path}\")\n            return True\n        except Exception as e:\n            print(f\"  Error: {str(e)}\")\n            return False\n    \n    def process_batch(\n        self,\n        input_dir: str,\n        output_dir: str,\n        style_prompt: str,\n        noise_strength: float = 0.5,\n        controlnet_strength: float = 1.0,\n        guidance_scale: float = 7.5,\n        num_steps: int = 50,\n        extensions: List[str] = [\".jpg\", \".jpeg\", \".png\"],\n        seed: Optional[int] = None\n    ):\n        \"\"\"Process all images in a folder.\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n        input_path = Path(input_dir)\n        image_files = []\n        for ext in extensions:\n            image_files.extend(list(input_path.glob(f\"*{ext}\")))\n            image_files.extend(list(input_path.glob(f\"*{ext.upper()}\")))\n        \n        print(f\"\\nProcessing {len(image_files)} images\")\n        print(f\"Style: {style_prompt}\")\n        print(f\"Noise Strength: {noise_strength}\")\n        print(f\"ControlNet Strength: {controlnet_strength}\")\n        print(f\"‚ö† Note: CPU processing is slower than GPU. Each image may take several minutes.\\n\")\n        \n        success_count = 0\n        for i, img_path in enumerate(image_files, 1):\n            print(f\"[{i}/{len(image_files)}] Processing: {img_path.name}\")\n            output_path = os.path.join(output_dir, f\"styled_{img_path.name}\")\n            \n            if self.process_single_image(\n                str(img_path), output_path, style_prompt,\n                noise_strength, controlnet_strength,\n                guidance_scale, num_steps, seed\n            ):\n                success_count += 1\n        \n        print(f\"\\nFinished: Transformed {success_count}/{len(image_files)} images.\")\n\ndef main():\n    \"\"\"Main execution entry point.\"\"\"\n    INPUT_DIR = \"./input_images\"\n    OUTPUT_DIR = \"./output_styled\"\n    \n    # Model Selection (Fixed: removed trailing comma)\n    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"  # Token-free model\n    \n    # Alternative models:\n    # MODEL_ID = \"CompVis/stable-diffusion-v1-4\"\n    # MODEL_ID = \"stabilityai/stable-diffusion-2-1\"  # May require token\n    \n    # Hugging Face Token (if required)\n    HF_TOKEN = hf_token0  # Use your token if needed\n    \n    STYLE_PROMPTS = {\n        \"pixar\": \"a 3D render of a face in Pixar animation style, high quality, detailed, professional lighting\",\n        \"anime\": \"anime style portrait, cel shaded, vibrant colors, expressive eyes, detailed\",\n        \"lego\": \"LEGO minifigure face, plastic texture, simplified features, toy style\",\n        \"oil_painting\": \"oil painting portrait, classical style, rich colors, brushstrokes visible\",\n        \"cartoon\": \"cartoon style portrait, bold lines, vibrant colors, simplified features\"\n    }\n    \n    STYLE = \"pixar\"\n    NOISE_STRENGTH = 0.4\n    CONTROLNET_STRENGTH = 0.8\n    GUIDANCE_SCALE = 7.5\n    NUM_STEPS = 30  # Reduced for faster CPU processing (was 50)\n    SEED = 42\n    USE_CANNY = True  # Canny is faster than OpenPose on CPU\n    \n    try:\n        artist = AvatarArtist2D(\n            model_id=MODEL_ID,\n            device=\"cpu\",  # Force CPU\n            dtype=torch.float32,  # CPU requires float32\n            use_canny=USE_CANNY,\n            hf_token=HF_TOKEN\n        )\n    except Exception as e:\n        print(f\"\\n‚ùå Initialization Error: {e}\")\n        print(\"\\nüí° Troubleshooting:\")\n        print(\"  1. Login to Hugging Face: huggingface-cli login\")\n        print(\"  2. Or set environment variable: export HF_TOKEN='your_token'\")\n        print(\"  3. Or use a token-free model: MODEL_ID='runwayml/stable-diffusion-v1-5'\")\n        return\n    \n    artist.process_batch(\n        input_dir=INPUT_DIR,\n        output_dir=OUTPUT_DIR,\n        style_prompt=STYLE_PROMPTS[STYLE],\n        noise_strength=NOISE_STRENGTH,\n        controlnet_strength=CONTROLNET_STRENGTH,\n        guidance_scale=GUIDANCE_SCALE,\n        num_steps=NUM_STEPS,\n        seed=SEED\n    )\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display results\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef show_image(image_dir):\n    image_paths = [\n        os.path.join(image_dir, f)\n        for f in sorted(os.listdir(image_dir))\n        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n    ][:6]  \n    \n    if not image_paths:\n        print(f\"No images found in {image_dir}\")\n        return\n    \n    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n    axes = axes.flatten()\n    for ax, img_path in zip(axes, image_paths):\n        img = Image.open(img_path)\n        ax.imshow(img)\n        ax.axis(\"off\")\n        ax.set_title(os.path.basename(img_path), fontsize=9)\n    for ax in axes[len(image_paths):]:\n        ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nprint(\"\\n=== Input Images ===\")\nshow_image('input_images')\n\nprint(\"\\n=== Styled Output Images ===\")\nif os.path.exists('./output_styled'):\n    show_image('./output_styled')\nelse:\n    print(\"Output directory not found. Run the main() function first.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}