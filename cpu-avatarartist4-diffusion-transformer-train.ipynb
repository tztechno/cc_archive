{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":288323840,"sourceType":"kernelVersion"},{"sourceId":288328029,"sourceType":"kernelVersion"},{"sourceId":288334056,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n# **CPU AvatarArtist4: Diffusion Transformer Training**","metadata":{}},{"cell_type":"markdown","source":"https://kumapowerliu.github.io/AvatarArtist/","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **Step 4 Pipeline Explanation: DiT (Diffusion Transformer) Training**\n\n## Overview\nThis pipeline trains a **Diffusion Transformer (DiT)** model that learns to generate 3D triplanes from 2D avatar images. Unlike Step 2 (which trained a GAN), this uses **diffusion models** - a newer, more stable approach that progressively denoises random noise into structured 3D representations.\n\n---\n\n## Key Concept: Diffusion Models\n\n### **What is Diffusion?**\n\n```\nTraining:\n  Clean Triplane â†’ Add Noise (1000 steps) â†’ Random Noise\n  Model learns to reverse this process\n\nGeneration:\n  Random Noise â†’ Remove Noise (1000 steps) â†’ Clean Triplane\n  Conditioned on input image to control the output\n```\n\n**Analogy**: Like watching a video of ink dissolving in water in reverse - the model learns to un-dissolve the ink back into a clear pattern.\n\n---\n\n## Pipeline Architecture\n\n### **1. Data Loading (TriplaneDataset)**\n\n```python\nTriplaneDataset(\n    data_dir=\"./dit_training_data\",  # From Step 3\n    load_images=True,\n    image_size=256,\n    triplane_size=256\n)\n```\n\n**Loads:**\n- **Images**: 256Ã—256 RGB images (condition)\n- **Triplanes**: Static + Dynamic features (target to predict)\n- **Metadata**: Links images to triplanes\n- **3DMM params**: Shape, expression, pose (optional)\n\n**Data Structure:**\n```python\nbatch = {\n    'image': [B, 3, 256, 256],      # Condition\n    'triplane': [B, 96, 256, 256],  # Target (67 static + 29 dynamic)\n    'static': [B, 67, 256, 256],\n    'dynamic': [B, 29, 256, 256],\n    'z': [B, 512],\n    'shape': [B, 80],\n    'exp': [B, 64],\n    'pose': [B, 6]\n}\n```\n\n---\n\n### **2. Model Architecture (DiffusionTransformer)**\n\n#### **A) Input Processing**\n\n**PatchEmbed**: Converts images into patches\n```python\nImage [B, 3, 256, 256] \n  â†’ Patches [B, 256, 768]  # 16Ã—16 patches, 768-dim embeddings\n```\n\n**Two Parallel Embedders:**\n1. **x_embedder**: Embeds noisy triplane (what we're denoising)\n2. **c_embedder**: Embeds condition image (guides denoising)\n\n#### **B) Timestep Embedding**\n\n```python\nTimestepEmbedding(dim=768)\n```\n- Encodes diffusion timestep (0-1000) into embedding\n- Tells model \"how noisy is the input?\"\n- Uses sinusoidal encoding (like Transformers)\n\n#### **C) Transformer Blocks (DiTBlock)**\n\n```python\nDiTBlock Ã— depth (6 or 12 layers)\n  â”œâ”€ Adaptive Layer Norm (AdaLN)\n  â”œâ”€ Multi-Head Self-Attention\n  â””â”€ Feed-Forward Network (MLP)\n```\n\n**Key Innovation: AdaLN (Adaptive Layer Norm)**\n- Conditions each layer on the timestep\n- Learns different transformations for different noise levels\n- 6 parameters per block: shift/scale/gate for attention & MLP\n\n**Formula:**\n```\nh = LayerNorm(x)\nh = h * (1 + scale) + shift\noutput = x + gate * Attention(h)\n```\n\n#### **D) Output Layer (FinalLayer)**\n\n```python\nFinalLayer\n  â”œâ”€ Adaptive Layer Norm\n  â”œâ”€ Linear projection\n  â””â”€ Unpatchify: Patches â†’ Image\n```\n\n**Output**: Predicted noise [B, 96, 256, 256]\n\n---\n\n### **3. Diffusion Process (GaussianDiffusion)**\n\n#### **Forward Process (Training)**\n\n```python\nq_sample(x_start, t, noise):\n  x_t = sqrt(Î±_t) * x_0 + sqrt(1 - Î±_t) * Îµ\n```\n\n**Steps:**\n1. Take clean triplane `x_0`\n2. Sample random timestep `t` âˆˆ [0, 1000]\n3. Add Gaussian noise according to schedule\n4. Result: Noisy triplane `x_t`\n\n**Noise Schedule:**\n```python\nÎ²: 0.0001 â†’ 0.02  # Linear schedule\nÎ± = 1 - Î²\ná¾± = cumprod(Î±)     # Cumulative product\n```\n\n#### **Reverse Process (Inference)**\n\n```python\np_sample(x_t, t, condition):\n  Îµ_predicted = model(x_t, t, condition)\n  x_{t-1} = (x_t - âˆš(1-á¾±) * Îµ) / âˆšá¾±  # Remove noise\n```\n\n**Generation Loop:**\n```\nfor t in [1000, 999, ..., 1, 0]:\n    x = remove_noise(x, t)  # Progressive denoising\nreturn x  # Clean triplane\n```\n\n---\n\n## Training Process\n\n### **4. Loss Function**\n\n**Simple MSE Loss:**\n```python\nloss = MSE(predicted_noise, actual_noise)\n```\n\n**Why this works:**\n- Model learns to predict noise at any timestep\n- Once it can predict noise, it can reverse the diffusion\n- Simpler than GAN losses (no discriminator needed)\n\n**Training Step:**\n```\n1. Load batch: (image, triplane)\n2. Sample random timestep t\n3. Add noise to triplane: x_t = q_sample(triplane, t)\n4. Predict noise: Îµ_pred = model(x_t, t, image)\n5. Compute loss: L = MSE(Îµ_pred, Îµ_actual)\n6. Backpropagate and update weights\n```\n\n---\n\n## Parameter Settings\n\n### **Model Configurations**\n\n**Small Model (Recommended for storage constraints):**\n```python\nhidden_size = 384\ndepth = 6\nnum_heads = 6\nParameters: ~40M\nCheckpoint size: ~160MB\n```\n\n**Standard Model (Better quality):**\n```python\nhidden_size = 768\ndepth = 12\nnum_heads = 12\nParameters: ~172M\nCheckpoint size: ~700MB\n```\n\n### **Training Hyperparameters**\n\n| Parameter | Default | Purpose | Tuning Guide |\n|-----------|---------|---------|--------------|\n| `batch_size` | 4 | Samples per iteration | GPU 24GB: 4-8, 12GB: 2 |\n| `num_epochs` | 100 | Training epochs | More data = fewer epochs |\n| `lr` | 1e-4 | Learning rate | Stable, rarely needs tuning |\n| `weight_decay` | 0.0 | L2 regularization | Increase if overfitting |\n| `timesteps` | 1000 | Diffusion steps | More = better quality, slower |\n| `save_every` | 10 | Checkpoint frequency | Adjust for storage |\n| `keep_last_n` | 2 | Recent checkpoints to keep | Saves disk space |\n\n### **Diffusion Hyperparameters**\n\n| Parameter | Value | Meaning |\n|-----------|-------|---------|\n| `beta_start` | 0.0001 | Noise at t=0 (very small) |\n| `beta_end` | 0.02 | Noise at t=1000 (large) |\n| `timesteps` | 1000 | Number of diffusion steps |\n\n---\n\n## Training Workflow\n\n### **Initialization Phase**\n\n```\n1. Load dataset (90% train, 10% validation)\n2. Initialize model (Small or Standard)\n3. Initialize diffusion schedule\n4. Create optimizer (AdamW)\n5. Create learning rate scheduler (Cosine)\n```\n\n### **Training Loop**\n\n```python\nfor epoch in range(num_epochs):\n    # Training\n    for batch in train_loader:\n        loss = train_step(batch)\n    \n    # Validation\n    val_loss = validate(val_loader)\n    \n    # Learning rate decay\n    scheduler.step()\n    \n    # Save checkpoints\n    if epoch % save_every == 0:\n        save_checkpoint()\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        save_best_model()\n```\n\n### **Memory Optimization Features**\n\n1. **Checkpoint Cleanup**: Keeps only last 2 checkpoints\n2. **Best Model Tracking**: Saves model with lowest validation loss\n3. **Gradient Clipping**: Prevents exploding gradients (max norm = 1.0)\n4. **Mixed Precision**: Can be enabled for faster training (not shown in code)\n\n---\n\n## Expected Results\n\n### **Training Metrics**\n\n#### **Healthy Training Signs:**\n\n```\nEpoch 1-10 (Early):\n  Train Loss: 0.15 - 0.08 (decreasing rapidly)\n  Val Loss: 0.12 - 0.09\n  LR: 1e-4\n\nEpoch 10-50 (Mid):\n  Train Loss: 0.08 - 0.04 (steady decrease)\n  Val Loss: 0.09 - 0.05\n  LR: 5e-5 (cosine decay)\n\nEpoch 50-100 (Late):\n  Train Loss: 0.04 - 0.02 (slow decrease)\n  Val Loss: 0.05 - 0.03\n  LR: 1e-6 (near minimum)\n```\n\n#### **Convergence Indicators:**\n\nâœ… **Good Signs:**\n- Train loss steadily decreases\n- Val loss decreases similarly (small gap)\n- Loss stabilizes around 0.02-0.05\n- No sudden spikes or NaN\n\nâŒ **Problem Signs:**\n- Val loss >> Train loss (overfitting)\n- Loss not decreasing after 20 epochs (learning rate too low)\n- NaN or Inf values (numerical instability)\n- Loss oscillating wildly (learning rate too high)\n\n### **Loss Interpretation**\n\n| Loss Range | Meaning | Action |\n|------------|---------|--------|\n| 0.01 - 0.03 | Excellent | Model converged well |\n| 0.03 - 0.06 | Good | Acceptable quality |\n| 0.06 - 0.10 | Moderate | May need more training |\n| > 0.10 | Poor | Check data/model/hyperparameters |\n\n---\n\n## Output Files\n\n### **Checkpoint Structure**\n\n```\ndit_checkpoints/\nâ”œâ”€â”€ checkpoint_0010.pt    # Epoch 10 (will be deleted)\nâ”œâ”€â”€ checkpoint_0020.pt    # Epoch 20 (will be deleted)\nâ”œâ”€â”€ ...\nâ”œâ”€â”€ checkpoint_0090.pt    # Epoch 90 (kept - last 2)\nâ”œâ”€â”€ checkpoint_0100.pt    # Epoch 100 (kept - last 2)\nâ”œâ”€â”€ best_model.pt         # Best validation loss\nâ””â”€â”€ final_model.pt        # Final epoch\n```\n\n**Checkpoint Contents:**\n```python\n{\n    'epoch': 100,\n    'model': model.state_dict(),      # Model weights\n    'optimizer': optimizer.state_dict(),  # Optimizer state\n    'scheduler': scheduler.state_dict()   # LR scheduler state\n}\n```\n\n**File Sizes:**\n- Small model: ~160 MB per checkpoint\n- Standard model: ~700 MB per checkpoint\n\n---\n\n## What Results Indicate\n\n### **1. Training Curves**\n\n**Ideal Curve Shape:**\n```\nLoss\n  |\n  |  Train ----\n  |            ----\n  |                ----___\n  |  Val ----           ----___\n  |         ----                ----___\n  |             ----                   ----___\n  +-----------------------------------------> Epoch\n```\n\n**Interpretation:**\n- **Parallel curves** = Good generalization\n- **Diverging curves** = Overfitting (val loss increases)\n- **Flat curve** = Underfitting (model too simple or LR too low)\n\n### **2. Model Quality Assessment**\n\n#### **Direct Indicators:**\n\n| Metric | Good | Moderate | Poor |\n|--------|------|----------|------|\n| Final Train Loss | < 0.03 | 0.03-0.06 | > 0.06 |\n| Final Val Loss | < 0.05 | 0.05-0.08 | > 0.08 |\n| Train-Val Gap | < 0.02 | 0.02-0.04 | > 0.04 |\n| Convergence Speed | < 50 epochs | 50-80 epochs | > 80 epochs |\n\n#### **Generation Quality (After Training):**\n\nTo evaluate model quality, generate samples:\n\n```python\n# Load best model\nmodel = load_model('dit_checkpoints/best_model.pt')\n\n# Sample from diffusion\ncondition_image = load_image('test.png')\ntriplane = diffusion.sample(model, shape, condition_image)\n\n# Check quality:\n- Triplane values in reasonable range [-5, 5]\n- No NaN or Inf values\n- Smooth spatial features (no checkerboard patterns)\n- Good correlation with condition image\n```\n\n---\n\n## Common Issues & Solutions\n\n### **Problem 1: High Validation Loss**\n\n**Symptoms:**\n```\nTrain Loss: 0.02\nVal Loss: 0.15  # Large gap\n```\n\n**Causes & Solutions:**\n1. **Overfitting**\n   - Add weight decay: `weight_decay=0.01`\n   - Reduce model size: Use Small model\n   - Get more training data (Step 3)\n   - Add dropout (modify DiTBlock)\n\n2. **Train/Val distribution mismatch**\n   - Check data split is random\n   - Ensure validation set is representative\n\n### **Problem 2: Slow Convergence**\n\n**Symptoms:**\n```\nEpoch 50: Loss still > 0.10\n```\n\n**Solutions:**\n1. Increase learning rate: `lr=2e-4`\n2. Warm-up learning rate schedule\n3. Increase model capacity: Use Standard model\n4. Check data quality (Step 3)\n5. Increase batch size (if memory allows)\n\n### **Problem 3: Loss Spikes**\n\n**Symptoms:**\n```\nEpoch 30: Loss=0.05\nEpoch 31: Loss=0.15  # Sudden spike\nEpoch 32: Loss=0.08\n```\n\n**Solutions:**\n1. Reduce learning rate: `lr=5e-5`\n2. Enable gradient clipping (already implemented)\n3. Check for corrupted data samples\n4. Reduce batch size\n\n### **Problem 4: Out of Memory**\n\n**Symptoms:**\n```\nRuntimeError: CUDA out of memory\n```\n\n**Solutions:**\n1. Reduce batch size: `batch_size=2`\n2. Use Small model\n3. Reduce image/triplane size: `256 â†’ 128`\n4. Enable gradient checkpointing (advanced)\n5. Use CPU (very slow)\n\n### **Problem 5: Disk Space Shortage**\n\n**Symptoms:**\n```\nOSError: [Errno 28] No space left on device\n```\n\n**Solutions:**\n1. Set `keep_last_n=1` (keep only 1 checkpoint)\n2. Use Small model (~160MB vs 700MB)\n3. Increase `save_every` to 20 or 50\n4. Delete old checkpoints manually\n5. Save only best model, skip periodic saves\n\n---\n\n## After Training: Inference Usage\n\n### **Load Trained Model**\n\n```python\n# Initialize model\nmodel = DiffusionTransformer(\n    img_size=256,\n    patch_size=16,\n    in_channels=96,\n    condition_channels=3,\n    hidden_size=384,  # Match training config\n    depth=6,\n    num_heads=6\n)\n\n# Load weights\ncheckpoint = torch.load('dit_checkpoints/best_model.pt')\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\n```\n\n### **Generate Triplane from Image**\n\n```python\n# Load condition image\nimage = load_and_preprocess_image('avatar.png')  # [1, 3, 256, 256]\n\n# Initialize diffusion\ndiffusion = GaussianDiffusion(timesteps=1000, device='cuda')\n\n# Sample triplane\ntriplane = diffusion.sample(\n    model=model,\n    shape=(1, 96, 256, 256),  # Batch, Channels, H, W\n    condition=image\n)\n\n# triplane is now a 3D representation of the avatar\n# Can be rendered from different angles using volume rendering\n```\n\n### **Controllable Generation**\n\n```python\n# Modify expression by adjusting dynamic features\ntriplane_static = triplane[:, :67]   # Identity (unchanged)\ntriplane_dynamic = triplane[:, 67:]  # Expression (modify this)\n\n# Example: Amplify expression\ntriplane_dynamic_amplified = triplane_dynamic * 1.5\n\n# Reconstruct\ntriplane_new = torch.cat([triplane_static, triplane_dynamic_amplified], dim=1)\n\n# Render to get new image with stronger expression\n```\n\n---\n\n## Comparison: Step 2 (GAN) vs Step 4 (DiT)\n\n| Aspect | Step 2 (Next3D GAN) | Step 4 (DiT) |\n|--------|---------------------|--------------|\n| **Model Type** | GAN (Generator + Discriminator) | Diffusion (Denoising Model) |\n| **Training Stability** | Moderate (requires ADA, R1, careful tuning) | High (simpler loss, no discriminator) |\n| **Generation Quality** | Good (fast generation) | Better (slower generation) |\n| **Controllability** | Limited (via 3DMM params) | High (via condition image + triplane decomposition) |\n| **Training Speed** | Faster per epoch | Slower per epoch |\n| **Inference Speed** | Fast (1 forward pass) | Slow (1000 denoising steps) |\n| **Use Case** | Generate varied avatars quickly | High-quality, controlled generation |\n\n---\n\n## Performance Expectations\n\n### **Training Time**\n\n| Hardware | Batch Size | Time/Epoch | 100 Epochs | Small vs Standard |\n|----------|-----------|------------|------------|-------------------|\n| CPU (8 cores) | 1 | 60+ min | 100+ hours | N/A (too slow) |\n| GPU (GTX 1080) | 2 | 5-8 min | 8-13 hours | Small recommended |\n| GPU (RTX 3090) | 4-8 | 2-4 min | 3-7 hours | Either works |\n| GPU (A100) | 8-16 | 1-2 min | 2-3 hours | Standard recommended |\n\n**With 1000 training samples:**\n- Small model, GPU (RTX 3090), batch=4: ~250 iterations/epoch â†’ ~3 min/epoch â†’ ~5 hours total\n\n### **Generation Time (Inference)**\n\n| Diffusion Steps | Quality | Time (RTX 3090) | Use Case |\n|-----------------|---------|-----------------|----------|\n| 50 | Preview | 1-2 sec | Fast preview |\n| 250 | Good | 5-8 sec | Balanced |\n| 1000 | Best | 15-20 sec | Final output |\n\n**Trade-off**: More steps = better quality but slower\n\n---\n\n## Summary\n\nThis pipeline:\n\nâœ… **Trains** a Diffusion Transformer to generate 3D triplanes  \nâœ… **Conditions** on 2D images for controlled generation  \nâœ… **Uses** stable diffusion training (no GAN instability)  \nâœ… **Enables** high-quality, controllable 3D-aware generation  \nâœ… **Optimizes** disk space with checkpoint management  \nâœ… **Provides** small and standard model options\n\n**Key Innovation**: Combines Transformers (powerful learning) + Diffusion (stable training) + 3D triplane decomposition (controllability)\n\n**Result Interpretation:**\n- **Low loss (< 0.05)** = Model learned to denoise triplanes well\n- **Small train-val gap** = Good generalization\n- **Stable training** = No sudden spikes or divergence\n- **Quality samples** = Triplanes can render high-quality 3D avatars\n\n**Next Steps**: Use trained model to generate novel 3D avatars from 2D reference images, with control over expressions and poses through triplane manipulation.\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n!pip install diffusers transformers accelerate\n!pip install controlnet-aux opencv-python pillow\n!pip install mediapipe==0.10.13","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-12-23T02:53:16.646178Z","iopub.execute_input":"2025-12-23T02:53:16.646528Z","iopub.status.idle":"2025-12-23T02:55:20.4819Z","shell.execute_reply.started":"2025-12-23T02:53:16.6465Z","shell.execute_reply":"2025-12-23T02:55:20.480172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dit_training_data = '/kaggle/input/cpu-avatarartist3-triplane-decomposition/dit_training_data'\ndit_checkpoints = '/kaggle/input/cpu-avatarartist2-next3d-4d-gan-fine-tuning/next3d_checkpoints'\nfinal_model_path = '/kaggle/input/cpu-avatarartist2-next3d-4d-gan-fine-tuning/next3d_checkpoints/final_model.pt'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nDiffusion Transformer (DiT) Training Script\nTraining a Diffusion model to predict 3D triplanes from 2D images.\n\nReference: DiT (Scalable Diffusion Models with Transformers)\nhttps://arxiv.org/abs/2212.09748\n\"\"\"\n\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nfrom tqdm import tqdm\nimport json\nimport torchvision.transforms as transforms\nfrom einops import rearrange, repeat\nimport shutil\n\n\n# ==================== Dataloader ====================\n\nclass TriplaneDataset(Dataset):\n    \"\"\"Dataset for Image and Triplane pairs\"\"\"\n    \n    def __init__(\n        self,\n        data_dir: str,\n        load_images: bool = True,\n        image_size: int = 256,\n        triplane_size: int = 256\n    ):\n        \"\"\"\n        Args:\n            data_dir: Dataset directory (dit_training_data)\n            load_images: Whether to load images\n            image_size: Target image size\n            triplane_size: Target triplane size\n        \"\"\"\n        self.data_dir = Path(data_dir)\n        self.load_images = load_images\n        self.image_size = image_size\n        self.triplane_size = triplane_size\n        \n        # Load metadata\n        with open(self.data_dir / 'metadata.json', 'r') as f:\n            self.metadata = json.load(f)\n        \n        # Load dataset info\n        with open(self.data_dir / 'dataset_info.json', 'r') as f:\n            self.info = json.load(f)\n        \n        # Image transformations\n        self.transform = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n        ])\n        \n        print(f\"Dataset loaded: {len(self.metadata)} samples\")\n        print(f\"Static channels: {self.info['static_channels']}\")\n        print(f\"Dynamic channels: {self.info['dynamic_channels']}\")\n        \n        # Path test: Check if the first sample can be loaded\n        if len(self.metadata) > 0:\n            test_meta = self.metadata[0]\n            test_triplane_path = test_meta['triplane_path']\n            \n            # Path conversion test\n            if not os.path.isabs(test_triplane_path):\n                if test_triplane_path.startswith('./'):\n                    test_triplane_path = test_triplane_path[2:]\n                if test_triplane_path.startswith('dit_training_data/'):\n                    test_triplane_path = test_triplane_path.replace('dit_training_data/', '', 1)\n                test_triplane_path = os.path.join(self.data_dir, test_triplane_path)\n            \n            if os.path.exists(test_triplane_path):\n                print(f\"âœ“ Path test passed: First sample accessible\")\n            else:\n                print(f\"âš  Path test failed:\")\n                print(f\"  Original: {test_meta['triplane_path']}\")\n                print(f\"  Converted: {test_triplane_path}\")\n                print(f\"  Exists: {os.path.exists(test_triplane_path)}\")\n                \n                # Display directory contents for debugging\n                triplane_dir = os.path.join(self.data_dir, 'triplanes')\n                if os.path.exists(triplane_dir):\n                    files = sorted(os.listdir(triplane_dir))[:5]\n                    print(f\"  First 5 files in triplanes/: {files}\")\n    \n    def __len__(self):\n        return len(self.metadata)\n    \n    def __getitem__(self, idx):\n        meta = self.metadata[idx]\n        \n        # Path correction: Convert relative path to absolute\n        triplane_path = meta['triplane_path']\n        \n        if not os.path.isabs(triplane_path):\n            # Convert './dit_training_data/triplanes/...' -> 'triplanes/...'\n            if triplane_path.startswith('./'):\n                triplane_path = triplane_path[2:]\n            \n            # Remove 'dit_training_data/' prefix if present\n            if triplane_path.startswith('dit_training_data/'):\n                triplane_path = triplane_path.replace('dit_training_data/', '', 1)\n            \n            # Combine with data_dir\n            triplane_path = os.path.join(self.data_dir, triplane_path)\n        \n        # Load triplane\n        if triplane_path.endswith('.npz'):\n            data = np.load(triplane_path)\n            triplane_data = {\n                k: torch.from_numpy(data[k]).float() for k in data.keys()\n            }\n        else:  # .pth\n            triplane_data = torch.load(triplane_path)\n        \n        # Resize triplanes if necessary\n        static = triplane_data['static']\n        dynamic = triplane_data['dynamic']\n        \n        if static.shape[-1] != self.triplane_size:\n            static = F.interpolate(\n                static.unsqueeze(0),\n                size=(self.triplane_size, self.triplane_size),\n                mode='bilinear',\n                align_corners=False\n            ).squeeze(0)\n            \n            dynamic = F.interpolate(\n                dynamic.unsqueeze(0),\n                size=(self.triplane_size, self.triplane_size),\n                mode='bilinear',\n                align_corners=False\n            ).squeeze(0)\n        \n        # Concatenate triplanes\n        triplane = torch.cat([static, dynamic], dim=0)\n        \n        result = {\n            'triplane': triplane,  # [C, H, W]\n            'static': static,\n            'dynamic': dynamic,\n            'z': triplane_data['z'],\n            'shape': triplane_data['shape'],\n            'exp': triplane_data['exp'],\n            'pose': triplane_data['pose'],\n        }\n        \n        # Load image\n        if self.load_images and meta.get('image_path'):\n            image_path = meta['image_path']\n            \n            # Correct image path similarly\n            if not os.path.isabs(image_path):\n                if image_path.startswith('./'):\n                    image_path = image_path[2:]\n                if image_path.startswith('dit_training_data/'):\n                    image_path = image_path.replace('dit_training_data/', '', 1)\n                image_path = os.path.join(self.data_dir, image_path)\n            \n            if os.path.exists(image_path):\n                image = Image.open(image_path).convert('RGB')\n                image = self.transform(image)\n                result['image'] = image\n            else:\n                print(f\"âš  Image not found: {image_path}\")\n                # Dummy image as fallback\n                result['image'] = torch.zeros(3, self.image_size, self.image_size)\n        \n        return result\n\n\n# ==================== DiT Model Components ====================\n\nclass TimestepEmbedding(nn.Module):\n    \"\"\"Sinusoidal Embedding for Timesteps\"\"\"\n    \n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.SiLU(),\n            nn.Linear(dim * 4, dim)\n        )\n    \n    def forward(self, timesteps):\n        \"\"\"\n        Args:\n            timesteps: [B] timesteps\n        Returns:\n            [B, dim] embedding vector\n        \"\"\"\n        half_dim = self.dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n        emb = timesteps[:, None] * emb[None, :]\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n        \n        return self.mlp(emb)\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"Splits images into patches and embeds them\"\"\"\n    \n    def __init__(self, img_size=256, patch_size=16, in_channels=3, embed_dim=768):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        self.proj = nn.Conv2d(\n            in_channels, embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: [B, C, H, W]\n        Returns:\n            [B, N, D] where N=num_patches, D=embed_dim\n        \"\"\"\n        x = self.proj(x)  # [B, D, H', W']\n        x = rearrange(x, 'b d h w -> b (h w) d')\n        return x\n\n\nclass DiTBlock(nn.Module):\n    \"\"\"\n    DiT Transformer Block with Adaptive Layer Norm (AdaLN)\n    Uses conditional Layer Norm.\n    \"\"\"\n    \n    def __init__(self, dim: int, num_heads: int, mlp_ratio: float = 4.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False)\n        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False)\n        \n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Linear(mlp_hidden_dim, dim)\n        )\n        \n        # Adaptive layer norm parameters\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(dim, 6 * dim)\n        )\n    \n    def forward(self, x, c):\n        \"\"\"\n        Args:\n            x: [B, N, D] Tokens\n            c: [B, D] Condition vector (e.g., time embedding)\n        Returns:\n            [B, N, D]\n        \"\"\"\n        # Extract AdaLN parameters\n        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = \\\n            self.adaLN_modulation(c).chunk(6, dim=-1)\n        \n        # Attention with adaptive LN\n        h = self.norm1(x)\n        h = h * (1 + scale_msa.unsqueeze(1)) + shift_msa.unsqueeze(1)\n        h, _ = self.attn(h, h, h)\n        x = x + gate_msa.unsqueeze(1) * h\n        \n        # MLP with adaptive LN\n        h = self.norm2(x)\n        h = h * (1 + scale_mlp.unsqueeze(1)) + shift_mlp.unsqueeze(1)\n        h = self.mlp(h)\n        x = x + gate_mlp.unsqueeze(1) * h\n        \n        return x\n\n\nclass FinalLayer(nn.Module):\n    \"\"\"Final Output Layer\"\"\"\n    \n    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n        super().__init__()\n        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False)\n        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n        self.adaLN_modulation = nn.Sequential(\n            nn.SiLU(),\n            nn.Linear(hidden_size, 2 * hidden_size)\n        )\n    \n    def forward(self, x, c):\n        shift, scale = self.adaLN_modulation(c).chunk(2, dim=-1)\n        x = self.norm_final(x) * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n        x = self.linear(x)\n        return x\n\n\nclass DiffusionTransformer(nn.Module):\n    \"\"\"\n    Diffusion Transformer (DiT)\n    Predicts 3D triplane conditioned on a 2D image.\n    \"\"\"\n    \n    def __init__(\n        self,\n        img_size: int = 256,\n        patch_size: int = 16,\n        in_channels: int = 96,  # Number of channels in triplane\n        condition_channels: int = 3,  # Number of channels in condition image\n        hidden_size: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0\n    ):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_channels = in_channels\n        self.out_channels = in_channels\n        self.num_patches = (img_size // patch_size) ** 2\n        \n        # Input embedding (Noisy triplane)\n        self.x_embedder = PatchEmbed(\n            img_size, patch_size, in_channels, hidden_size\n        )\n        \n        # Condition embedding (Image)\n        self.c_embedder = PatchEmbed(\n            img_size, patch_size, condition_channels, hidden_size\n        )\n        \n        # Time embedding\n        self.t_embedder = TimestepEmbedding(hidden_size)\n        \n        # Positional embedding\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, self.num_patches, hidden_size)\n        )\n        \n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            DiTBlock(hidden_size, num_heads, mlp_ratio)\n            for _ in range(depth)\n        ])\n        \n        # Final layer\n        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n        \n        self.initialize_weights()\n    \n    def initialize_weights(self):\n        # Weight initialization\n        def _basic_init(module):\n            if isinstance(module, nn.Linear):\n                torch.nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n        self.apply(_basic_init)\n        \n        # Positional embedding\n        nn.init.normal_(self.pos_embed, std=0.02)\n        \n        # Zero-initialize output layers\n        nn.init.constant_(self.final_layer.linear.weight, 0)\n        nn.init.constant_(self.final_layer.linear.bias, 0)\n    \n    def unpatchify(self, x):\n        \"\"\"\n        Args:\n            x: [B, N, patch_size^2 * C]\n        Returns:\n            [B, C, H, W]\n        \"\"\"\n        p = self.patch_size\n        h = w = int(x.shape[1] ** 0.5)\n        x = rearrange(x, 'b (h w) (p1 p2 c) -> b c (h p1) (w p2)',\n                     h=h, p1=p, p2=p)\n        return x\n    \n    def forward(self, x, t, c):\n        \"\"\"\n        Args:\n            x: [B, C, H, W] Noisy triplane\n            t: [B] Timestep\n            c: [B, 3, H, W] Condition image\n        Returns:\n            [B, C, H, W] Predicted noise\n        \"\"\"\n        # Embeddings\n        x = self.x_embedder(x) + self.pos_embed  # [B, N, D]\n        c_embed = self.c_embedder(c)             # [B, N, D]\n        t_embed = self.t_embedder(t)             # [B, D]\n        \n        # Combine conditions\n        x = x + c_embed  # Add image condition\n        c = t_embed      # Time condition via AdaLN\n        \n        # Transformer blocks\n        for block in self.blocks:\n            x = block(x, c)\n        \n        # Final layer\n        x = self.final_layer(x, c)\n        x = self.unpatchify(x)\n        \n        return x\n\n\n# ==================== Diffusion Process ====================\n\nclass GaussianDiffusion:\n    \"\"\"DDPM Gaussian Diffusion Process\"\"\"\n    \n    def __init__(\n        self,\n        timesteps: int = 1000,\n        beta_start: float = 0.0001,\n        beta_end: float = 0.02,\n        device: str = 'cuda'\n    ):\n        self.timesteps = timesteps\n        self.device = device\n        \n        # Beta schedule (linear)\n        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        \n        # Calculations for diffusion q(x_t | x_{t-1})\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        \n        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n        self.posterior_variance = (\n            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n    \n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"\n        Forward diffusion: x_0 -> x_t\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        \n        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t]\n        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t]\n        \n        # Broadcast to match x_start shape\n        sqrt_alphas_cumprod_t = sqrt_alphas_cumprod_t[:, None, None, None]\n        sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod_t[:, None, None, None]\n        \n        return (\n            sqrt_alphas_cumprod_t * x_start +\n            sqrt_one_minus_alphas_cumprod_t * noise\n        )\n    \n    def p_losses(self, model, x_start, t, condition, noise=None):\n        \"\"\"\n        Training loss calculation\n        \"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        \n        # Forward diffusion\n        x_noisy = self.q_sample(x_start, t, noise)\n        \n        # Predict noise\n        predicted_noise = model(x_noisy, t, condition)\n        \n        # MSE loss\n        loss = F.mse_loss(predicted_noise, noise)\n        \n        return loss\n    \n    @torch.no_grad()\n    def p_sample(self, model, x, t, condition):\n        \"\"\"\n        Reverse diffusion: x_t -> x_{t-1}\n        \"\"\"\n        predicted_noise = model(x, t, condition)\n        \n        # Coefficients\n        alpha = self.alphas[t][:, None, None, None]\n        alpha_cumprod = self.alphas_cumprod[t][:, None, None, None]\n        beta = self.betas[t][:, None, None, None]\n        \n        # Predict x_0\n        pred_x0 = (x - torch.sqrt(1 - alpha_cumprod) * predicted_noise) / torch.sqrt(alpha_cumprod)\n        \n        # Calculate x_{t-1}\n        if t[0] > 0:\n            noise = torch.randn_like(x)\n            sigma = torch.sqrt(self.posterior_variance[t])[:, None, None, None]\n        else:\n            noise = 0\n            sigma = 0\n        \n        mean = (1 / torch.sqrt(alpha)) * (\n            x - (beta / torch.sqrt(1 - alpha_cumprod)) * predicted_noise\n        )\n        \n        return mean + sigma * noise\n    \n    @torch.no_grad()\n    def sample(self, model, shape, condition):\n        \"\"\"\n        Sampling: Generate clean data from random noise\n        \"\"\"\n        batch_size = shape[0]\n        device = condition.device\n        \n        # Start from random noise\n        x = torch.randn(shape, device=device)\n        \n        # Reverse diffusion steps\n        for i in tqdm(reversed(range(self.timesteps)), desc='Sampling', total=self.timesteps):\n            t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n            x = self.p_sample(model, x, t, condition)\n        \n        return x\n\n\n# ==================== Training Loop ====================\n\nclass DiTTrainer:\n    \"\"\"Diffusion Transformer Trainer\"\"\"\n    \n    def __init__(\n        self,\n        model: nn.Module,\n        diffusion: GaussianDiffusion,\n        device: str = 'cuda',\n        lr: float = 1e-4,\n        weight_decay: float = 0.0\n    ):\n        self.model = model.to(device)\n        self.diffusion = diffusion\n        self.device = device\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(),\n            lr=lr,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.999)\n        )\n        \n        # Learning rate scheduler\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, T_max=1000, eta_min=1e-6\n        )\n        \n        print(f\"Trainer initialized\")\n        print(f\"  Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n        print(f\"  Device: {device}\")\n    \n    def train_step(self, batch: Dict[str, torch.Tensor]):\n        \"\"\"Single training step\"\"\"\n        self.model.train()\n        self.optimizer.zero_grad()\n        \n        # Data\n        triplane = batch['triplane'].to(self.device)\n        image = batch['image'].to(self.device)\n        \n        batch_size = triplane.shape[0]\n        \n        # Random timesteps\n        t = torch.randint(\n            0, self.diffusion.timesteps,\n            (batch_size,),\n            device=self.device\n        ).long()\n        \n        # Loss calculation\n        loss = self.diffusion.p_losses(self.model, triplane, t, image)\n        \n        # Backpropagation\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n        \n        self.optimizer.step()\n        \n        return {'loss': loss.item()}\n    \n    @torch.no_grad()\n    def validate(self, dataloader):\n        \"\"\"Validation step\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        for batch in dataloader:\n            triplane = batch['triplane'].to(self.device)\n            image = batch['image'].to(self.device)\n            \n            batch_size = triplane.shape[0]\n            t = torch.randint(\n                0, self.diffusion.timesteps,\n                (batch_size,),\n                device=self.device\n            ).long()\n            \n            loss = self.diffusion.p_losses(self.model, triplane, t, image)\n            total_loss += loss.item()\n        \n        return total_loss / len(dataloader)\n    \n    def save_checkpoint(self, path: str, epoch: int, keep_last_n: int = 2):\n        \"\"\"\n        Saves checkpoint (Storage-optimized version)\n        \n        Args:\n            path: Save path\n            epoch: Current epoch\n            keep_last_n: Number of recent checkpoints to keep (older ones are deleted)\n        \"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model': self.model.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n            'scheduler': self.scheduler.state_dict(),\n        }\n        \n        torch.save(checkpoint, path)\n        print(f\"Checkpoint saved: {path}\")\n        \n        # Display file size\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        print(f\"  Size: {size_mb:.1f} MB\")\n        \n        # Cleanup old checkpoints (excluding best_model.pt and final_model.pt)\n        if keep_last_n > 0:\n            checkpoint_dir = os.path.dirname(path)\n            checkpoint_files = sorted([\n                f for f in os.listdir(checkpoint_dir)\n                if f.startswith('checkpoint_') and f.endswith('.pt')\n            ])\n            \n            # Delete from oldest\n            if len(checkpoint_files) > keep_last_n:\n                for old_file in checkpoint_files[:-keep_last_n]:\n                    old_path = os.path.join(checkpoint_dir, old_file)\n                    try:\n                        os.remove(old_path)\n                        print(f\"  Removed old checkpoint: {old_file}\")\n                    except:\n                        pass\n    \n    def load_checkpoint(self, path: str):\n        \"\"\"Loads checkpoint\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        self.scheduler.load_state_dict(checkpoint['scheduler'])\n        print(f\"Checkpoint loaded: {path}\")\n        return checkpoint['epoch']\n\n\n# ==================== Main Training Function ====================\n\ndef train_dit(\n    data_dir: str = \"./dit_training_data\",\n    output_dir: str = \"./dit_checkpoints\",\n    batch_size: int = 4,\n    num_epochs: int = 100,\n    lr: float = 1e-4,\n    save_every: int = 10,\n    device: str = 'cuda',\n    use_small_model: bool = True  # For disk space optimization\n):\n    \"\"\"\n    Main training function for DiT\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Dataset\n    dataset = TriplaneDataset(\n        data_dir=data_dir,\n        load_images=True,\n        image_size=256,\n        triplane_size=256\n    )\n    \n    # Train/Val Split\n    train_size = int(0.9 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        dataset, [train_size, val_size]\n    )\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=0,  # Set to 0 to avoid path issues on Windows/Kaggle\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True\n    )\n    \n    # Model Initialization\n    if use_small_model:\n        # Small Model (~40M params, ~160MB/checkpoint)\n        model = DiffusionTransformer(\n            img_size=256,\n            patch_size=16,\n            in_channels=dataset.info['static_channels'] + dataset.info['dynamic_channels'],\n            condition_channels=3,\n            hidden_size=384,\n            depth=6,\n            num_heads=6,\n            mlp_ratio=4.0\n        )\n        print(\"Model: SMALL (40M params, ~160MB)\")\n    else:\n        # Standard Model (~172M params, ~700MB/checkpoint)\n        model = DiffusionTransformer(\n            img_size=256,\n            patch_size=16,\n            in_channels=dataset.info['static_channels'] + dataset.info['dynamic_channels'],\n            condition_channels=3,\n            hidden_size=768,\n            depth=12,\n            num_heads=12,\n            mlp_ratio=4.0\n        )\n        print(\"Model: STANDARD (172M params, ~700MB)\")\n    \n    # Diffusion\n    diffusion = GaussianDiffusion(\n        timesteps=1000,\n        beta_start=0.0001,\n        beta_end=0.02,\n        device=device\n    )\n    \n    # Trainer\n    trainer = DiTTrainer(\n        model=model,\n        diffusion=diffusion,\n        device=device,\n        lr=lr\n    )\n    \n    # Training Loop\n    print(f\"\\n{'='*60}\")\n    print(\"Starting DiT Training\")\n    print(f\"{'='*60}\\n\")\n    print(f\"Training samples: {len(train_dataset)}\")\n    print(f\"Validation samples: {len(val_dataset)}\")\n    print(f\"Batch size: {batch_size}\")\n    print(f\"Epochs: {num_epochs}\\n\")\n    \n    best_val_loss = float('inf')\n    \n    for epoch in range(num_epochs):\n        # Training\n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        epoch_loss = 0\n        \n        for batch in pbar:\n            metrics = trainer.train_step(batch)\n            epoch_loss += metrics['loss']\n            pbar.set_postfix({'loss': f\"{metrics['loss']:.4f}\"})\n        \n        avg_train_loss = epoch_loss / len(train_loader)\n        \n        # Validation\n        val_loss = trainer.validate(val_loader)\n        \n        # Scheduler Step\n        trainer.scheduler.step()\n        \n        print(f\"\\nEpoch {epoch+1} Complete:\")\n        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n        print(f\"  Val Loss: {val_loss:.4f}\")\n        print(f\"  LR: {trainer.optimizer.param_groups[0]['lr']:.2e}\")\n        \n        # Periodic Save (Keep only last 2)\n        if (epoch + 1) % save_every == 0:\n            save_path = os.path.join(output_dir, f\"checkpoint_{epoch+1:04d}.pt\")\n            trainer.save_checkpoint(save_path, epoch+1, keep_last_n=2)\n        \n        # Save Best Model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_path = os.path.join(output_dir, \"best_model.pt\")\n            \n            if os.path.exists(best_path):\n                os.remove(best_path)\n            \n            trainer.save_checkpoint(best_path, epoch+1, keep_last_n=0)\n            print(f\"  â˜… Best model saved! (Val Loss: {val_loss:.4f})\")\n    \n    # Cleanup intermediate checkpoints and save final model\n    for f in os.listdir(output_dir):\n        if f.startswith('checkpoint_') and f.endswith('.pt'):\n            try:\n                os.remove(os.path.join(output_dir, f))\n                print(f\"Removed temporary checkpoint: {f}\")\n            except:\n                pass\n    \n    final_path = os.path.join(output_dir, \"final_model.pt\")\n    trainer.save_checkpoint(final_path, num_epochs, keep_last_n=0)\n    \n    print(f\"\\n{'='*60}\")\n    print(\"Training Complete!\")\n    print(f\"{'='*60}\")\n    print(f\"Best model: {best_path}\")\n    print(f\"Final model: {final_path}\")\n    print(f\"{'='*60}\\n\")\n\n\nif __name__ == \"__main__\":\n    # ========== Kaggle Environment Path Setup ==========\n    \n    # Input data (Read-only)\n    DATA_DIR = \"/kaggle/input/cpu-avatarartist3-triplane-decomposition/dit_training_data\"\n    \n    # Output directory (Writable)\n    OUTPUT_DIR = \"/kaggle/working/dit_checkpoints\"\n    \n    # Path Verification\n    print(\"=\" * 60)\n    print(\"Path Verification\")\n    print(\"=\" * 60)\n    print(f\"Data Directory: {DATA_DIR}\")\n    print(f\"  Exists: {os.path.exists(DATA_DIR)}\")\n    \n    if os.path.exists(DATA_DIR):\n        print(f\"  File Count:\")\n        if os.path.exists(os.path.join(DATA_DIR, 'images')):\n            num_images = len(os.listdir(os.path.join(DATA_DIR, 'images')))\n            print(f\"    images/: {num_images}\")\n        if os.path.exists(os.path.join(DATA_DIR, 'triplanes')):\n            num_triplanes = len(os.listdir(os.path.join(DATA_DIR, 'triplanes')))\n            print(f\"    triplanes/: {num_triplanes}\")\n    \n    print(f\"\\nOutput Directory: {OUTPUT_DIR}\")\n    print(f\"  Writable: {os.access('/kaggle/working', os.W_OK)}\")\n    print(\"=\" * 60 + \"\\n\")\n    \n    # Check if data directory exists\n    if not os.path.exists(DATA_DIR):\n        print(f\"âŒ Data directory not found: {DATA_DIR}\")\n        print(\"\\nSearching for available datasets...\")\n        \n        kaggle_input = \"/kaggle/input\"\n        if os.path.exists(kaggle_input):\n            for dataset in os.listdir(kaggle_input):\n                dataset_path = os.path.join(kaggle_input, dataset)\n                print(f\"\\nðŸ“ Dataset: {dataset}\")\n                \n                # Search for dit_training_data within the input datasets\n                for root, dirs, files in os.walk(dataset_path):\n                    if 'dit_training_data' in root or 'metadata.json' in files:\n                        print(f\"  Found candidate: {root}\")\n        \n        print(\"\\nPlease update DATA_DIR with one of the paths above.\")\n        import sys\n        sys.exit(1)\n    \n    # ========== Training Configurations ==========\n    \n    BATCH_SIZE = 4      # VRAM 24GB: 4-8, 12GB: 2\n    NUM_EPOCHS = 80     # Shortened for testing (Prod: 100+)\n    LEARNING_RATE = 1e-4\n    SAVE_EVERY = 10\n    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    print(f\"Device: {DEVICE}\")\n    if DEVICE == 'cuda':\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\\n\")\n    \n    # Execution\n    try:\n        train_dit(\n            data_dir=DATA_DIR,\n            output_dir=OUTPUT_DIR,\n            batch_size=BATCH_SIZE,\n            num_epochs=NUM_EPOCHS,\n            lr=LEARNING_RATE,\n            save_every=SAVE_EVERY,\n            device=DEVICE\n        )\n    except Exception as e:\n        print(f\"\\nâŒ Training Error: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}