{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":992580,"sourceType":"datasetVersion","datasetId":543939},{"sourceId":288228911,"sourceType":"kernelVersion"},{"sourceId":288328029,"sourceType":"kernelVersion"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CPU AvatarArtist3: Triplane Decomposition**","metadata":{}},{"cell_type":"markdown","source":"https://kumapowerliu.github.io/AvatarArtist/","metadata":{}},{"cell_type":"markdown","source":"---\n\n## **Step 3 Pipeline Explanation: Data Synthesis for DiT Training**\n\n## Overview\nThis pipeline uses your **trained Next3D model from Step 2** to synthesize a large dataset of (image, triplane) pairs. This synthetic data will be used to train a **DiT (Diffusion Transformer)** model in Step 4 for controllable 3D-aware generation.\n\n---\n\n## Pipeline Architecture\n\n### **1. Device Management (CPU/GPU Compatible)**\n\n```python\nclass DeviceManager:\n    - Auto-detects GPU/CPU\n    - Optimizes batch size based on hardware\n    - Manages memory efficiently\n```\n\n**Key Features:**\n- **GPU Mode**: Faster synthesis (8-16 samples/batch)\n- **CPU Mode**: Slower but accessible (2-4 samples/batch)\n- Automatic memory clearing to prevent OOM errors\n\n**Device Selection:**\n```python\nFORCE_CPU = False  # Set to True to force CPU\n# Auto-detects: CUDA if available, else CPU\n```\n\n---\n\n### **2. Model Components**\n\n#### **A) Generator Loading**\n```python\nload_generator(generator_path)\n```\n- Loads your trained Next3D model from Step 2\n- Checkpoint: `./next3d_checkpoints/final_model.pt`\n- Sets model to **eval mode** (no training, only inference)\n- Disables gradients for efficiency\n\n#### **B) Triplane Decomposer**\n```python\nParametricTriplaneDecomposer(\n    triplane_channels=96,\n    static_ratio=0.7  # 70% static, 30% dynamic\n)\n```\n\n**Purpose**: Splits triplane features into two components:\n\n| Component | Channels | Represents | Examples |\n|-----------|----------|------------|----------|\n| **Static** | 67 (70%) | Identity features (unchanging) | Face shape, bone structure, skin texture |\n| **Dynamic** | 29 (30%) | Expression/Pose features | Smile, frown, head rotation, eye gaze |\n\n**Why decompose?**\n- Enables **disentangled control** in DiT model\n- Static features: Control \"who\" the person is\n- Dynamic features: Control \"what expression/pose\" they have\n\n---\n\n### **3. Data Synthesis Process**\n\n#### **Workflow Per Sample:**\n\n```\n1. Sample Random Inputs:\n   ├─ z: Random latent code [512-dim]\n   ├─ shape: 3DMM identity parameters [80-dim]\n   ├─ exp: Expression parameters [64-dim]\n   └─ pose: Head pose parameters [6-dim]\n\n2. Generate with Next3D:\n   ├─ Pass (z, shape, exp) → Generator\n   ├─ Output: Image (512×512) + Triplane (96×64×64)\n   └─ Image represents the avatar face\n\n3. Decompose Triplane:\n   ├─ Split triplane → Static + Dynamic\n   ├─ Static: Identity-related features\n   └─ Dynamic: Expression/pose-related features\n\n4. Save Outputs:\n   ├─ Image: image_000001.png\n   ├─ Triplane data: triplane_000001.npz\n   │   ├─ static: [67, 64, 64]\n   │   ├─ dynamic: [29, 64, 64]\n   │   ├─ z: [512]\n   │   ├─ shape: [80]\n   │   ├─ exp: [64]\n   │   └─ pose: [6]\n   └─ Metadata: Links image to triplane\n```\n\n---\n\n## Key Parameters\n\n### **Configuration Settings**\n\n```python\n# Required\nGENERATOR_PATH = \"./next3d_checkpoints/final_model.pt\"\nOUTPUT_DIR = \"./dit_training_data\"\n\n# Synthesis settings\nNUM_SAMPLES = 30              # Number of samples to generate\nBATCH_SIZE = None             # Auto-adjusted (GPU: 8, CPU: 2)\nTRIPLANE_RESOLUTION = 256     # Triplane spatial resolution\nUSE_3DMM = True               # Use 3DMM parameters\n\n# Output format\nSAVE_IMAGES = True            # Save PNG images\nSAVE_FORMAT = 'npz'           # 'npz' or 'pth' for triplanes\n```\n\n### **Parameter Guide**\n\n| Parameter | Recommended | Purpose | Notes |\n|-----------|-------------|---------|-------|\n| `NUM_SAMPLES` | 1000-10000 | DiT training samples | More = better DiT quality |\n| `BATCH_SIZE` | GPU: 8-16, CPU: 2 | Synthesis speed | Auto-adjusted if None |\n| `TRIPLANE_RESOLUTION` | 256 | Triplane detail | Must match Step 2 |\n| `USE_3DMM` | True | Enable control | False for simpler pipeline |\n| `SAVE_FORMAT` | 'npz' | Compression | 'npz' is smaller than 'pth' |\n\n---\n\n## Output Structure\n\n### **Directory Layout**\n\n```\ndit_training_data/\n├── images/                    # Generated avatar images\n│   ├── image_000000.png      # Sample 0\n│   ├── image_000001.png      # Sample 1\n│   └── ...\n│\n├── triplanes/                 # 3D representations\n│   ├── triplane_000000.npz   # Compressed triplane data\n│   ├── triplane_000001.npz\n│   └── ...\n│\n├── metadata.json              # Links images to triplanes\n└── dataset_info.json          # Dataset configuration\n```\n\n### **File Contents**\n\n#### **1. Images (PNG files)**\n- **Resolution**: 512×512 pixels\n- **Format**: RGB, 8-bit\n- **Content**: Generated avatar faces in your artistic style\n- **Purpose**: Visual verification + optional conditioning\n\n#### **2. Triplane Data (NPZ files)**\nEach `.npz` file contains:\n```python\n{\n    'static': [67, 64, 64],     # Identity features\n    'dynamic': [29, 64, 64],    # Expression/pose features\n    'z': [512],                  # Latent code\n    'shape': [80],               # 3DMM shape params\n    'exp': [64],                 # 3DMM expression params\n    'pose': [6]                  # 3DMM pose params\n}\n```\n\n#### **3. metadata.json**\n```json\n[\n  {\n    \"index\": 0,\n    \"triplane_path\": \"triplanes/triplane_000000.npz\",\n    \"image_path\": \"images/image_000000.png\",\n    \"static_shape\": [67, 64, 64],\n    \"dynamic_shape\": [29, 64, 64]\n  },\n  ...\n]\n```\n\n#### **4. dataset_info.json**\n```json\n{\n  \"num_samples\": 1000,\n  \"triplane_resolution\": 256,\n  \"static_channels\": 67,\n  \"dynamic_channels\": 29,\n  \"use_3dmm\": true,\n  \"format\": \"npz\",\n  \"device\": \"cuda\"\n}\n```\n\n---\n\n## What the Result Images Indicate\n\n### **Quality Indicators**\n\n#### **✅ Good Results:**\n\n1. **Diversity**\n   - Different face shapes and identities\n   - Various expressions (neutral, smiling, etc.)\n   - Multiple head poses (frontal, profile, tilted)\n   - Different ages/genders (if trained on diverse data)\n\n2. **Consistency with Style**\n   - Images match your avatar style from Step 1\n   - Artistic qualities preserved (line style, coloring, shading)\n   - Character design elements maintained\n\n3. **Quality Metrics**\n   - Clear facial features (eyes, nose, mouth)\n   - No artifacts or distortions\n   - Proper proportions\n   - Sharp details\n\n#### **❌ Problem Signs:**\n\n1. **Mode Collapse**\n   - All faces look nearly identical\n   - Limited variation in expression/pose\n   - **Solution**: Retrain Step 2 with more epochs or different settings\n\n2. **Quality Issues**\n   - Blurry images\n   - Missing facial features\n   - Distorted proportions\n   - **Solution**: Step 2 training didn't converge properly\n\n3. **Style Inconsistency**\n   - Images don't match target style\n   - Realistic instead of stylized\n   - **Solution**: Check Step 1 style transfer quality\n\n---\n\n## Sample Analysis\n\n### **Example Output Interpretation**\n\nImagine you generated 30 samples:\n\n```\nSample 0: Female face, neutral expression, frontal view\nSample 1: Male face, smiling, slight left turn\nSample 2: Female face, serious, tilted head\n...\n```\n\n**What to check:**\n\n| Aspect | What to Look For | Interpretation |\n|--------|------------------|----------------|\n| **Identity Variety** | 30 different-looking people | Static triplane is diverse ✓ |\n| **Expression Range** | Neutral, happy, sad, surprised | Dynamic triplane captures expressions ✓ |\n| **Pose Diversity** | Frontal, profile, tilted views | Model handles 3D rotations ✓ |\n| **Style Consistency** | All images look like your avatar style | Step 2 training successful ✓ |\n| **Image Quality** | Sharp, clear, no artifacts | Generator is stable ✓ |\n\n---\n\n## Performance Expectations\n\n### **Synthesis Speed**\n\n| Hardware | Batch Size | Time/Sample | 1000 Samples |\n|----------|-----------|-------------|--------------|\n| CPU (8 cores) | 2 | ~10 sec | ~1.5 hours |\n| GPU (GTX 1080) | 8 | ~0.5 sec | ~1 minute |\n| GPU (RTX 3090) | 16 | ~0.2 sec | ~15 seconds |\n| GPU (A100) | 32 | ~0.1 sec | ~5 seconds |\n\n### **Storage Requirements**\n\nFor 1000 samples:\n- **Images (PNG)**: ~1-2 GB\n- **Triplanes (NPZ)**: ~3-5 GB\n- **Total**: ~5-7 GB\n\nFor 10,000 samples:\n- **Total**: ~50-70 GB\n\n---\n\n## Usage After Synthesis\n\n### **Data Inspection**\n\n```python\n# Load a sample\nimport numpy as np\nfrom PIL import Image\n\n# Load image\nimg = Image.open('dit_training_data/images/image_000000.png')\nimg.show()\n\n# Load triplane\ndata = np.load('dit_training_data/triplanes/triplane_000000.npz')\nstatic = data['static']    # [67, 64, 64]\ndynamic = data['dynamic']  # [29, 64, 64]\nshape = data['shape']      # [80]\nexp = data['exp']          # [64]\n\nprint(f\"Static range: [{static.min():.3f}, {static.max():.3f}]\")\nprint(f\"Dynamic range: [{dynamic.min():.3f}, {dynamic.max():.3f}]\")\n```\n\n### **Validation Checks**\n\n```python\n# Check dataset completeness\nimport json\n\nwith open('dit_training_data/metadata.json') as f:\n    metadata = json.load(f)\n\nprint(f\"Total samples: {len(metadata)}\")\n\n# Verify all files exist\nfor item in metadata:\n    assert os.path.exists(item['triplane_path'])\n    if item['image_path']:\n        assert os.path.exists(item['image_path'])\n\nprint(\"✓ All files present\")\n```\n\n---\n\n## Common Issues & Solutions\n\n### **Problem 1: Out of Memory (OOM)**\n\n**Symptoms:**\n```\nRuntimeError: CUDA out of memory\n```\n\n**Solutions:**\n1. Reduce `BATCH_SIZE` manually (e.g., 4 → 2)\n2. Lower `NUM_SAMPLES` per run, synthesize in multiple runs\n3. Set `FORCE_CPU = True` (slower but works)\n4. Clear memory between batches (already implemented)\n\n---\n\n### **Problem 2: Poor Quality Images**\n\n**Symptoms:**\n- Blurry faces\n- Missing features\n- Style doesn't match\n\n**Solutions:**\n1. **Retrain Step 2**: Model didn't converge properly\n2. **Check Step 1**: Style transfer quality was poor\n3. **Verify checkpoint**: Ensure you're using the correct model file\n\n---\n\n### **Problem 3: Low Diversity**\n\n**Symptoms:**\n- All faces look similar\n- Same expression/pose repeated\n\n**Solutions:**\n1. Check `sample_3dmm_params()` ranges:\n   ```python\n   pose_range=0.3      # Increase to 0.5 for more pose variety\n   exp_strength=1.0    # Increase to 1.5 for stronger expressions\n   ```\n2. Step 2 may have mode collapse - retrain with stronger R1 regularization\n\n---\n\n### **Problem 4: Slow Synthesis on CPU**\n\n**Expected:** CPU is 50-100× slower than GPU\n\n**Solutions:**\n1. **Use GPU**: Cloud GPU (Google Colab, AWS, etc.)\n2. **Reduce samples**: Start with 50-100 for testing\n3. **Increase batch size**: CPU can handle 2-4 if you have 16GB+ RAM\n4. **Synthesize overnight**: Let it run for large datasets\n\n---\n\n## Preparation for Step 4 (DiT Training)\n\n### **What You Need:**\n\n✅ **Generated dataset** in `dit_training_data/`  \n✅ **Minimum 1000 samples** (more is better)  \n✅ **Quality validation**: Check image diversity and quality  \n✅ **Disk space**: Ensure 10-100 GB available for larger datasets\n\n### **Next Step Preview:**\n\nStep 4 will:\n1. Load your synthetic (image, triplane) pairs\n2. Train a DiT model to **predict triplanes from noise**\n3. Enable controllable generation: \"Generate an avatar with expression X and pose Y\"\n4. Support **progressive refinement** during inference\n\n---\n\n## Summary\n\nThis pipeline:\n\n✅ **Loads** your trained Next3D generator from Step 2  \n✅ **Generates** thousands of (image, triplane) pairs  \n✅ **Decomposes** triplanes into static (identity) + dynamic (expression) components  \n✅ **Saves** everything needed for DiT training  \n✅ **Works on** both CPU and GPU (with auto-optimization)\n\n**Result Images Indicate:**\n- **Diversity** → Model learned to generate varied identities/expressions\n- **Quality** → Step 2 training was successful\n- **Style match** → Your avatar style is preserved\n- **Variety in poses** → 3D consistency is maintained\n\n**Recommended:** Generate 1000-5000 samples for optimal DiT training quality in Step 4.\n\n---","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n!pip install diffusers transformers accelerate\n!pip install controlnet-aux opencv-python pillow\n!pip install mediapipe==0.10.13","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-12-23T02:53:16.646178Z","iopub.execute_input":"2025-12-23T02:53:16.646528Z","iopub.status.idle":"2025-12-23T02:55:20.4819Z","shell.execute_reply.started":"2025-12-23T02:53:16.6465Z","shell.execute_reply":"2025-12-23T02:55:20.480172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_model_path ='/kaggle/input/cpu-avatarartist2-next3d-4d-gan-fine-tuning/next3d_checkpoints/final_model.pt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\nfrom tqdm import tqdm\nimport json\nimport gc\nimport psutil\nimport warnings\n\n# ==================== CPU/GPU Compatibility Helper ====================\n\nclass DeviceManager:\n    \"\"\"Manages device selection and memory optimization\"\"\"\n    \n    def __init__(self, force_cpu: bool = False):\n        self.force_cpu = force_cpu\n        self.device = self._select_device()\n        self.is_cuda = self.device.type == 'cuda'\n        \n        self._log_device_info()\n    \n    def _select_device(self):\n        \"\"\"Select appropriate device\"\"\"\n        if self.force_cpu:\n            return torch.device('cpu')\n        \n        if torch.cuda.is_available():\n            return torch.device('cuda')\n        else:\n            return torch.device('cpu')\n    \n    def _log_device_info(self):\n        \"\"\"Log device information\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Device Configuration\")\n        print(f\"{'='*60}\")\n        print(f\"Selected Device: {self.device}\")\n        \n        if self.is_cuda:\n            print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n            print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n        else:\n            mem = psutil.virtual_memory()\n            print(f\"CPU RAM: {mem.total / 1e9:.2f} GB\")\n            print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n            print(f\"\\n⚠ Running on CPU - synthesis will be slower\")\n        \n        print(f\"{'='*60}\\n\")\n    \n    def get_optimal_batch_size(self, default_gpu: int = 8, default_cpu: int = 2):\n        \"\"\"Get optimal batch size based on device\"\"\"\n        if self.is_cuda:\n            return default_gpu\n        else:\n            # CPU: Use smaller batch size\n            return default_cpu\n    \n    def clear_memory(self):\n        \"\"\"Clear GPU/CPU memory\"\"\"\n        if self.is_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()\n\n\n# ==================== Model Definitions (CPU Compatible) ====================\n\nclass MappingNetwork(nn.Module):\n    \"\"\"StyleGAN2-style Mapping Network\"\"\"\n    \n    def __init__(self, z_dim: int = 512, w_dim: int = 512, num_layers: int = 8):\n        super().__init__()\n        layers = []\n        for i in range(num_layers):\n            in_dim = z_dim if i == 0 else w_dim\n            layers.extend([\n                nn.Linear(in_dim, w_dim),\n                nn.LeakyReLU(0.2)\n            ])\n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, z):\n        return self.net(z)\n\n\nclass NoiseInjection(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.weight = nn.Parameter(torch.zeros(1))\n    \n    def forward(self, x):\n        noise = torch.randn(x.shape[0], 1, x.shape[2], x.shape[3], \n                           device=x.device, dtype=x.dtype)\n        return x + self.weight * noise\n\n\nclass StyleBlock(nn.Module):\n    \"\"\"StyleGAN2-style Block\"\"\"\n    \n    def __init__(self, in_ch, out_ch, w_dim):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n        self.style1 = nn.Linear(w_dim, in_ch)\n        self.style2 = nn.Linear(w_dim, out_ch)\n        self.noise1 = NoiseInjection()\n        self.noise2 = NoiseInjection()\n        self.activation = nn.LeakyReLU(0.2)\n    \n    def forward(self, x, w):\n        s1 = self.style1(w).unsqueeze(-1).unsqueeze(-1)\n        x = self.conv1(x * s1)\n        x = self.noise1(x)\n        x = self.activation(x)\n        \n        s2 = self.style2(w).unsqueeze(-1).unsqueeze(-1)\n        x = self.conv2(x * s2)\n        x = self.noise2(x)\n        x = self.activation(x)\n        \n        return x\n\n\nclass TriplaneBackbone(nn.Module):\n    \"\"\"Triplane Generation Backbone\"\"\"\n    \n    def __init__(self, w_dim: int, channels: int, resolution: int):\n        super().__init__()\n        self.w_dim = w_dim\n        self.channels = channels\n        self.resolution = resolution\n        \n        self.const = nn.Parameter(torch.randn(1, channels, 4, 4))\n        \n        self.blocks = nn.ModuleList([\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n            StyleBlock(channels, channels, w_dim),\n        ])\n        \n        self.to_features = nn.Conv2d(channels, channels, 1)\n    \n    def forward(self, w):\n        B = w.shape[0]\n        x = self.const.repeat(B, 1, 1, 1)\n        \n        for block in self.blocks:\n            x = block(x, w)\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        \n        features = self.to_features(x)\n        return features\n\n\nclass SuperResolutionModule(nn.Module):\n    \"\"\"Super-resolution module: 64x64 -> 512x512\"\"\"\n    \n    def __init__(self, in_channels, output_resolution=512):\n        super().__init__()\n        self.conv_blocks = nn.Sequential(\n            nn.Conv2d(in_channels, 128, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(64, 32, 3, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n            \n            nn.Conv2d(32, 3, 3, padding=1),\n            nn.Tanh()\n        )\n    \n    def forward(self, x):\n        return self.conv_blocks(x)\n\n\nclass TriplaneGenerator(nn.Module):\n    \"\"\"Next3D Triplane Generator\"\"\"\n    \n    def __init__(\n        self,\n        z_dim: int = 512,\n        w_dim: int = 512,\n        triplane_channels: int = 96,\n        triplane_resolution: int = 256,\n        use_3dmm: bool = True,\n        shape_dim: int = 80,\n        exp_dim: int = 64\n    ):\n        super().__init__()\n        self.z_dim = z_dim\n        self.w_dim = w_dim\n        self.use_3dmm = use_3dmm\n        \n        self.mapping = MappingNetwork(z_dim, w_dim)\n        \n        if use_3dmm:\n            self.shape_encoder = nn.Linear(shape_dim, w_dim)\n            self.exp_encoder = nn.Linear(exp_dim, w_dim)\n            self.condition_fusion = nn.Linear(w_dim * 3, w_dim)\n        \n        self.triplane_generator = TriplaneBackbone(\n            w_dim=w_dim,\n            channels=triplane_channels,\n            resolution=triplane_resolution\n        )\n        \n        self.superres = SuperResolutionModule(\n            triplane_channels, \n            output_resolution=512\n        )\n    \n    def forward(\n        self,\n        z: torch.Tensor,\n        shape: Optional[torch.Tensor] = None,\n        exp: Optional[torch.Tensor] = None,\n        c: Optional[torch.Tensor] = None\n    ):\n        w = self.mapping(z)\n        \n        if self.use_3dmm and shape is not None and exp is not None:\n            shape_feat = self.shape_encoder(shape)\n            exp_feat = self.exp_encoder(exp)\n            w_conditioned = self.condition_fusion(\n                torch.cat([w, shape_feat, exp_feat], dim=1)\n            )\n        else:\n            w_conditioned = w\n        \n        triplane_features = self.triplane_generator(w_conditioned)\n        image = self.superres(triplane_features)\n        \n        return {\n            'image': image,\n            'triplane': triplane_features,\n            'w': w_conditioned\n        }\n\n\nclass ParametricTriplaneDecomposer(nn.Module):\n    \"\"\"Decomposes Triplanes into Static and Dynamic components\"\"\"\n    \n    def __init__(\n        self,\n        triplane_channels: int = 96,\n        triplane_resolution: int = 256,\n        static_ratio: float = 0.7\n    ):\n        super().__init__()\n        self.channels = triplane_channels\n        self.resolution = triplane_resolution\n        \n        self.static_channels = int(triplane_channels * static_ratio)\n        self.dynamic_channels = triplane_channels - self.static_channels\n        \n        print(f\"Triplane Decomposition:\")\n        print(f\"  Static: {self.static_channels}ch\")\n        print(f\"  Dynamic: {self.dynamic_channels}ch\")\n    \n    def decompose(\n        self,\n        triplane: torch.Tensor,\n        pose: torch.Tensor,\n        exp: torch.Tensor\n    ) -> Dict[str, torch.Tensor]:\n        static = triplane[:, :self.static_channels]\n        dynamic = triplane[:, self.static_channels:]\n        \n        return {\n            'static': static,\n            'dynamic': dynamic,\n            'pose': pose,\n            'exp': exp\n        }\n    \n    def reconstruct(self, decomposed: Dict[str, torch.Tensor]) -> torch.Tensor:\n        return torch.cat([\n            decomposed['static'],\n            decomposed['dynamic']\n        ], dim=1)\n\n\n# ==================== CPU-Optimized Data Synthesizer ====================\n\nclass Next3DDataSynthesizer:\n    \"\"\"Synthesizes training data pairs from a Next3D model (CPU Compatible)\"\"\"\n    \n    def __init__(\n        self,\n        generator_path: str,\n        device: Optional[str] = None,\n        triplane_resolution: int = 256,\n        use_3dmm: bool = True,\n        force_cpu: bool = False\n    ):\n        print(\"=\" * 60)\n        print(\"Next3D Data Synthesizer Initialization (CPU Compatible)\")\n        print(\"=\" * 60)\n        \n        # Device setup\n        self.device_manager = DeviceManager(force_cpu=force_cpu)\n        self.device = self.device_manager.device\n        self.resolution = triplane_resolution\n        self.use_3dmm = use_3dmm\n        \n        # Load model\n        print(f\"\\nLoading model: {generator_path}\")\n        self.load_generator(generator_path)\n        \n        # Decomposer\n        self.decomposer = ParametricTriplaneDecomposer(\n            triplane_channels=96,\n            triplane_resolution=triplane_resolution\n        )\n        \n        print(\"\\n✓ Initialization Complete!\")\n    \n    def load_generator(self, path: str):\n        \"\"\"Loads the Generator model\"\"\"\n        # Load to CPU first to avoid OOM on GPU\n        checkpoint = torch.load(path, map_location='cpu')\n        \n        self.generator = TriplaneGenerator(\n            z_dim=512,\n            w_dim=512,\n            triplane_channels=96,\n            triplane_resolution=self.resolution,\n            use_3dmm=self.use_3dmm\n        )\n        \n        # Load weights\n        if 'generator' in checkpoint:\n            state_dict = checkpoint['generator']\n        else:\n            state_dict = checkpoint\n        \n        try:\n            self.generator.load_state_dict(state_dict, strict=True)\n            print(\"✓ Generator loaded successfully (strict mode)\")\n        except RuntimeError as e:\n            print(f\"⚠ Strict loading failed, trying flexible loading...\")\n            self.generator.load_state_dict(state_dict, strict=False)\n            print(\"✓ Generator loaded successfully (flexible mode)\")\n        \n        # Move to device after loading\n        self.generator = self.generator.to(self.device)\n        self.generator.eval()\n        \n        total_params = sum(p.numel() for p in self.generator.parameters())\n        print(f\"✓ Total parameters: {total_params:,}\")\n        \n        # Set to eval mode and disable gradients\n        for param in self.generator.parameters():\n            param.requires_grad = False\n    \n    def sample_3dmm_params(\n        self,\n        batch_size: int = 1,\n        pose_range: float = 0.3,\n        exp_strength: float = 1.0\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Samples 3DMM parameters\"\"\"\n        shape = torch.randn(batch_size, 80, device=self.device) * 0.5\n        exp = torch.randn(batch_size, 64, device=self.device) * exp_strength\n        pose = torch.rand(batch_size, 6, device=self.device) * 2 * pose_range - pose_range\n        \n        return shape, exp, pose\n    \n    def generate_sample(\n        self,\n        z: Optional[torch.Tensor] = None,\n        shape: Optional[torch.Tensor] = None,\n        exp: Optional[torch.Tensor] = None,\n        pose: Optional[torch.Tensor] = None\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Generates a single sample\"\"\"\n        with torch.no_grad():\n            if z is None:\n                z = torch.randn(1, 512, device=self.device)\n            \n            if self.use_3dmm:\n                if shape is None or exp is None or pose is None:\n                    shape, exp, pose = self.sample_3dmm_params(z.shape[0])\n            else:\n                shape, exp, pose = None, None, None\n            \n            output = self.generator(z, shape, exp)\n            \n            image = output['image']\n            triplane = output['triplane']\n            \n            if self.use_3dmm:\n                decomposed = self.decomposer.decompose(triplane, pose, exp)\n            else:\n                decomposed = {\n                    'static': triplane,\n                    'dynamic': torch.zeros_like(triplane[:, :10]),\n                    'pose': torch.zeros(z.shape[0], 6, device=self.device),\n                    'exp': torch.zeros(z.shape[0], 64, device=self.device)\n                }\n            \n            return {\n                'image': image,\n                'triplane_static': decomposed['static'],\n                'triplane_dynamic': decomposed['dynamic'],\n                'z': z,\n                'shape': shape if shape is not None else torch.zeros(z.shape[0], 80, device=self.device),\n                'exp': exp if exp is not None else torch.zeros(z.shape[0], 64, device=self.device),\n                'pose': pose if pose is not None else torch.zeros(z.shape[0], 6, device=self.device)\n            }\n    \n    def synthesize_dataset(\n        self,\n        num_samples: int,\n        output_dir: str,\n        batch_size: Optional[int] = None,\n        save_images: bool = True,\n        save_format: str = 'npz'\n    ):\n        \"\"\"Synthesizes a large volume of data pairs\"\"\"\n        \n        # Auto-adjust batch size for CPU\n        if batch_size is None:\n            batch_size = self.device_manager.get_optimal_batch_size()\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Dataset Synthesis Configuration\")\n        print(f\"{'='*60}\")\n        print(f\"Samples: {num_samples}\")\n        print(f\"Batch Size: {batch_size}\")\n        print(f\"Device: {self.device}\")\n        print(f\"Output: {output_dir}\")\n        print(f\"Format: {save_format}\")\n        print(f\"{'='*60}\\n\")\n        \n        os.makedirs(output_dir, exist_ok=True)\n        \n        if save_images:\n            img_dir = os.path.join(output_dir, 'images')\n            os.makedirs(img_dir, exist_ok=True)\n        \n        triplanes_dir = os.path.join(output_dir, 'triplanes')\n        os.makedirs(triplanes_dir, exist_ok=True)\n        \n        num_batches = (num_samples + batch_size - 1) // batch_size\n        sample_idx = 0\n        metadata = []\n        \n        for batch_idx in tqdm(range(num_batches), desc=\"Synthesizing\"):\n            current_batch_size = min(batch_size, num_samples - sample_idx)\n            \n            # Generate batch\n            z = torch.randn(current_batch_size, 512, device=self.device)\n            \n            if self.use_3dmm:\n                shape, exp, pose = self.sample_3dmm_params(current_batch_size)\n            else:\n                shape, exp, pose = None, None, None\n            \n            samples = self.generate_sample(z, shape, exp, pose)\n            \n            # Save each sample\n            for i in range(current_batch_size):\n                idx = sample_idx + i\n                \n                triplane_data = {\n                    'static': samples['triplane_static'][i].cpu().numpy(),\n                    'dynamic': samples['triplane_dynamic'][i].cpu().numpy(),\n                    'z': samples['z'][i].cpu().numpy(),\n                    'shape': samples['shape'][i].cpu().numpy(),\n                    'exp': samples['exp'][i].cpu().numpy(),\n                    'pose': samples['pose'][i].cpu().numpy(),\n                }\n                \n                triplane_path = os.path.join(\n                    triplanes_dir,\n                    f'triplane_{idx:06d}.{save_format}'\n                )\n                \n                if save_format == 'npz':\n                    np.savez_compressed(triplane_path, **triplane_data)\n                elif save_format == 'pth':\n                    torch.save({\n                        k: torch.from_numpy(v) for k, v in triplane_data.items()\n                    }, triplane_path)\n                \n                if save_images:\n                    image = samples['image'][i]\n                    image = (image.cpu().permute(1, 2, 0).numpy() + 1) / 2 * 255\n                    image = image.clip(0, 255).astype(np.uint8)\n                    \n                    img_path = os.path.join(img_dir, f'image_{idx:06d}.png')\n                    Image.fromarray(image).save(img_path)\n                \n                metadata.append({\n                    'index': idx,\n                    'triplane_path': triplane_path,\n                    'image_path': os.path.join(img_dir, f'image_{idx:06d}.png') if save_images else None,\n                    'static_shape': triplane_data['static'].shape,\n                    'dynamic_shape': triplane_data['dynamic'].shape,\n                })\n            \n            sample_idx += current_batch_size\n            \n            # Clear memory after each batch (important for CPU)\n            del samples, z\n            if shape is not None:\n                del shape, exp, pose\n            self.device_manager.clear_memory()\n        \n        # Save metadata\n        metadata_path = os.path.join(output_dir, 'metadata.json')\n        with open(metadata_path, 'w') as f:\n            json.dump(metadata, f, indent=2)\n        \n        info = {\n            'num_samples': num_samples,\n            'triplane_resolution': self.resolution,\n            'static_channels': self.decomposer.static_channels,\n            'dynamic_channels': self.decomposer.dynamic_channels,\n            'use_3dmm': self.use_3dmm,\n            'format': save_format,\n            'device': str(self.device)\n        }\n        \n        info_path = os.path.join(output_dir, 'dataset_info.json')\n        with open(info_path, 'w') as f:\n            json.dump(info, f, indent=2)\n        \n        print(f\"\\n{'='*60}\")\n        print(\"✓ Dataset Synthesis Complete!\")\n        print(f\"{'='*60}\")\n        print(f\"Generated Samples: {num_samples}\")\n        print(f\"Triplanes: {triplanes_dir}\")\n        if save_images:\n            print(f\"Images: {img_dir}\")\n        print(f\"Metadata: {metadata_path}\")\n        print(f\"Dataset Info: {info_path}\")\n        print(f\"{'='*60}\\n\")\n\n# ==================== Main Execution ====================\n\ndef main():\n    \"\"\"Execute Data Synthesis\"\"\"\n    \n    # Path settings\n    GENERATOR_PATH = final_model_path\n    \n    if not os.path.exists(GENERATOR_PATH):\n        print(f\"❌ Model file not found: {GENERATOR_PATH}\")\n        return\n    \n    print(f\"✓ Model found: {GENERATOR_PATH}\")\n    \n    # Configuration\n    OUTPUT_DIR = \"./dit_training_data\"\n    NUM_SAMPLES = 30  # Adjust based on your needs\n    SAVE_IMAGES = True\n    SAVE_FORMAT = 'npz'\n    TRIPLANE_RESOLUTION = 256\n    USE_3DMM = True\n    \n    # Force CPU mode (set to False to auto-detect)\n    FORCE_CPU = False  # Set to True to force CPU usage\n    \n    # Initialize synthesizer\n    try:\n        synthesizer = Next3DDataSynthesizer(\n            generator_path=GENERATOR_PATH,\n            triplane_resolution=TRIPLANE_RESOLUTION,\n            use_3dmm=USE_3DMM,\n            force_cpu=FORCE_CPU\n        )\n    except Exception as e:\n        print(f\"\\n❌ Initialization failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return\n    \n    # Synthesize dataset (batch_size will be auto-adjusted)\n    synthesizer.synthesize_dataset(\n        num_samples=NUM_SAMPLES,\n        output_dir=OUTPUT_DIR,\n        batch_size=None,  # Auto-adjust based on device\n        save_images=SAVE_IMAGES,\n        save_format=SAVE_FORMAT\n    )\n    \n    print(\"\\n✓ All operations complete!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}