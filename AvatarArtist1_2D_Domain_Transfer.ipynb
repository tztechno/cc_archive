{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 992580,
          "sourceType": "datasetVersion",
          "datasetId": 543939
        }
      ],
      "dockerImageVersionId": 31236,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tztechno/cc_archive/blob/main/AvatarArtist1_2D_Domain_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "# success 12/24 14:32\n",
        "# output styled was moved to google drive manually\n"
      ],
      "metadata": {
        "id": "c9LEUsjipJe-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "1-RZmqM0pJe_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step1**\n",
        "# **AvatarArtist1: 2D Domain Transfer**"
      ],
      "metadata": {
        "id": "bi4Jn-hbpJfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://kumapowerliu.github.io/AvatarArtist/"
      ],
      "metadata": {
        "id": "fPuIEyBBpJfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1:¬†https://www.kaggle.com/code/stpeteishii/avatarartist1-2d-domain-transfer\n",
        "\n",
        "Step2:¬†https://www.kaggle.com/code/stpeteishii/avatarartist2-next3d-4d-gan-fine-tuning\n",
        "\n",
        "Step3:¬†https://www.kaggle.com/code/stpeteishii/avatarartist3-triplane-decomposition\n",
        "\n",
        "Step4:¬†https://www.kaggle.com/code/stpeteishii/avatarartist4-diffusion-transformer-training\n",
        "\n",
        "Step5:¬†https://www.kaggle.com/code/stpeteishii/avatarartist5-avatar-generation-inference"
      ],
      "metadata": {
        "id": "qYGJXHKIpJfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token0=userdata.get('secret_hf_token')"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZnjZ8vg6pJfB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install diffusers transformers accelerate\n",
        "!pip install controlnet-aux opencv-python pillow\n",
        "!pip install mediapipe==0.10.9"
      ],
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2025-12-23T02:53:16.646178Z",
          "iopub.execute_input": "2025-12-23T02:53:16.646528Z",
          "iopub.status.idle": "2025-12-23T02:55:20.4819Z",
          "shell.execute_reply.started": "2025-12-23T02:53:16.6465Z",
          "shell.execute_reply": "2025-12-23T02:55:20.480172Z"
        },
        "id": "wOfYghjvpJfB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "gd_path='/content/drive/MyDrive/your_folder/pins_dataset'\n",
        "#os.makedirs('/content/pins_dataset', exist_ok=True)"
      ],
      "metadata": {
        "id": "ICBG7-5Sp7fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import random\n",
        "\n",
        "paths=[]\n",
        "for dirname, _, filenames in os.walk(gd_path):\n",
        "    for filename in filenames:\n",
        "        paths+=[(os.path.join(dirname, filename))]\n",
        "print(paths[0:6])\n",
        "random.shuffle(paths)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-23T02:49:45.784573Z",
          "iopub.execute_input": "2025-12-23T02:49:45.78489Z",
          "iopub.status.idle": "2025-12-23T02:49:50.073166Z",
          "shell.execute_reply.started": "2025-12-23T02:49:45.784865Z",
          "shell.execute_reply": "2025-12-23T02:49:50.072105Z"
        },
        "id": "xpW-Tgj7pJfC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"input_images\", exist_ok=True)\n",
        "for path in paths[0:100]:\n",
        "    shutil.copy(path, \"/content/input_images\")\n",
        "\n",
        "for dirname, _, filenames in os.walk('/content/input_images'):\n",
        "    for filename in filenames:\n",
        "        print(filename)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-23T02:52:48.005905Z",
          "iopub.execute_input": "2025-12-23T02:52:48.00626Z",
          "iopub.status.idle": "2025-12-23T02:52:48.019921Z",
          "shell.execute_reply.started": "2025-12-23T02:52:48.006226Z",
          "shell.execute_reply": "2025-12-23T02:52:48.01857Z"
        },
        "id": "hmGmUC5NpJfC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "AvatarArtist: 2D Domain Transfer Script\n",
        "Converts real-life images into specific styles using\n",
        "Stable Diffusion + ControlNet + SDEdit.\n",
        "\n",
        "[IMPORTANT] Hugging Face Token Setup:\n",
        "Method 1: Environment Variable\n",
        "  export HF_TOKEN=\"your_token_here\"\n",
        "\n",
        "Method 2: Command Line\n",
        "  huggingface-cli login\n",
        "\n",
        "Method 3: Specify in Code\n",
        "  artist = AvatarArtist2D(hf_token=\"your_token_here\")\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "import cv2\n",
        "from diffusers import (\n",
        "    StableDiffusionControlNetPipeline,\n",
        "    ControlNetModel,\n",
        "    DDIMScheduler,\n",
        "    UniPCMultistepScheduler\n",
        ")\n",
        "from diffusers.utils import load_image\n",
        "from controlnet_aux import OpenposeDetector, CannyDetector\n",
        "\n",
        "# MediaPipe is optional\n",
        "try:\n",
        "    import mediapipe as mp\n",
        "    MEDIAPIPE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MEDIAPIPE_AVAILABLE = False\n",
        "    print(\"Warning: MediaPipe not available. Using ControlNet only for pose detection.\")\n",
        "\n",
        "\n",
        "class AvatarArtist2D:\n",
        "    \"\"\"Main class for 2D domain transfer.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"runwayml/stable-diffusion-v1-5\",\n",
        "        controlnet_model: str = \"lllyasviel/sd-controlnet-openpose\",\n",
        "        device: str = \"cuda\",\n",
        "        dtype: torch.dtype = torch.float16,\n",
        "        use_canny: bool = False,\n",
        "        hf_token: Optional[str] = hf_token0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model_id: Path or ID for the Stable Diffusion model.\n",
        "                - \"runwayml/stable-diffusion-v1-5\" (Recommended: No token required)\n",
        "                - \"stabilityai/stable-diffusion-2-1\" (May require a token)\n",
        "                - \"stabilityai/stable-diffusion-xl-base-1.0\" (SDXL)\n",
        "            controlnet_model: Path or ID for the ControlNet model.\n",
        "            device: Computing device to use.\n",
        "            dtype: Data type.\n",
        "            use_canny: Use Canny edge detection (simpler and lightweight).\n",
        "            hf_token: Hugging Face token (optional).\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.dtype = dtype\n",
        "        self.use_canny = use_canny\n",
        "        self.hf_token = hf_token or os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "        print(f\"Using Model: {model_id}\")\n",
        "        print(\"Loading models...\")\n",
        "\n",
        "        # Select ControlNet based on the base model\n",
        "        if \"stable-diffusion-v1-5\" in model_id or \"v1-5\" in model_id:\n",
        "            if use_canny:\n",
        "                controlnet_model = \"lllyasviel/sd-controlnet-canny\"\n",
        "            else:\n",
        "                controlnet_model = \"lllyasviel/sd-controlnet-openpose\"\n",
        "        elif \"stable-diffusion-2\" in model_id:\n",
        "            if use_canny:\n",
        "                controlnet_model = \"thibaud/controlnet-sd21-canny-diffusers\"\n",
        "            else:\n",
        "                controlnet_model = \"thibaud/controlnet-sd21-openpose-diffusers\"\n",
        "\n",
        "        print(f\"ControlNet: {controlnet_model}\")\n",
        "\n",
        "        # Load ControlNet\n",
        "        try:\n",
        "            self.controlnet = ControlNetModel.from_pretrained(\n",
        "                controlnet_model,\n",
        "                torch_dtype=dtype,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "            print(f\"‚úì ControlNet loaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö† Error: Failed to load {controlnet_model}\")\n",
        "            print(f\"  Details: {e}\")\n",
        "            print(\"Attempting fallback to Canny model...\")\n",
        "            try:\n",
        "                fallback_model = \"lllyasviel/sd-controlnet-canny\"\n",
        "                self.controlnet = ControlNetModel.from_pretrained(\n",
        "                    fallback_model,\n",
        "                    torch_dtype=dtype,\n",
        "                    token=self.hf_token\n",
        "                )\n",
        "                self.use_canny = True\n",
        "                print(f\"‚úì Using Canny ControlNet as fallback.\")\n",
        "            except Exception as e2:\n",
        "                raise Exception(f\"Failed to load any ControlNet model: {e2}\")\n",
        "\n",
        "        # Stable Diffusion pipeline setup\n",
        "        try:\n",
        "            self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "                model_id,\n",
        "                controlnet=self.controlnet,\n",
        "                torch_dtype=dtype,\n",
        "                safety_checker=None,\n",
        "                token=self.hf_token\n",
        "            )\n",
        "            print(f\"‚úì Stable Diffusion loaded successfully\")\n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            if \"gated\" in error_msg.lower() or \"token\" in error_msg.lower():\n",
        "                raise Exception(\n",
        "                    f\"\\n{'='*60}\\n\"\n",
        "                    f\"üîê Authentication Error: This model requires a Hugging Face token.\\n\"\n",
        "                    f\"\\nInstructions:\"\n",
        "                    f\"\\n1. Get a token at: https://huggingface.co/settings/tokens\"\n",
        "                    f\"\\n2. Set it using one of these methods:\"\n",
        "                    f\"\\n   a) export HF_TOKEN='your_token'\"\n",
        "                    f\"\\n   b) huggingface-cli login\"\n",
        "                    f\"\\n   c) artist = AvatarArtist2D(hf_token='your_token')\"\n",
        "                    f\"\\n\\nAlternatively, use a model that doesn't require a token:\"\n",
        "                    f\"\\n   model_id='runwayml/stable-diffusion-v1-5'\"\n",
        "                    f\"\\n{'='*60}\\n\"\n",
        "                )\n",
        "            raise\n",
        "\n",
        "        # Scheduler configuration (SDEdit compatible)\n",
        "        self.pipe.scheduler = DDIMScheduler.from_config(\n",
        "            self.pipe.scheduler.config\n",
        "        )\n",
        "\n",
        "        self.pipe = self.pipe.to(device)\n",
        "        self.pipe.enable_attention_slicing()\n",
        "\n",
        "        # Control image processor\n",
        "        if self.use_canny:\n",
        "            print(\"Initializing Canny detector...\")\n",
        "            self.processor = CannyDetector()\n",
        "        else:\n",
        "            print(\"Loading Openpose processor...\")\n",
        "            try:\n",
        "                self.processor = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: OpenPose load failed: {e}\")\n",
        "                print(\"Falling back to Canny...\")\n",
        "                self.processor = CannyDetector()\n",
        "                self.use_canny = True\n",
        "\n",
        "        # MediaPipe Face Detection (Optional: for more detailed control)\n",
        "        self.face_mesh = None\n",
        "        if MEDIAPIPE_AVAILABLE:\n",
        "            try:\n",
        "                mp_face_mesh = mp.solutions.face_mesh\n",
        "                self.face_mesh = mp_face_mesh.FaceMesh(\n",
        "                    static_image_mode=True,\n",
        "                    max_num_faces=1,\n",
        "                    min_detection_confidence=0.5\n",
        "                )\n",
        "                print(\"MediaPipe face detection enabled.\")\n",
        "            except Exception as e:\n",
        "                print(f\"MediaPipe initialization failed: {e}\")\n",
        "                self.face_mesh = None\n",
        "\n",
        "        print(\"Initialization complete!\")\n",
        "\n",
        "    def extract_pose_landmarks(self, image: Image.Image) -> Image.Image:\n",
        "        \"\"\"Extract control image from input (OpenPose or Canny).\"\"\"\n",
        "        control_image = self.processor(image)\n",
        "        return control_image\n",
        "\n",
        "    def extract_face_landmarks(self, image: Image.Image) -> Optional[np.ndarray]:\n",
        "        \"\"\"Extract face landmarks using MediaPipe.\"\"\"\n",
        "        if self.face_mesh is None:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            image_np = np.array(image)\n",
        "            results = self.face_mesh.process(cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "            if results.multi_face_landmarks:\n",
        "                landmarks = results.multi_face_landmarks[0]\n",
        "                h, w = image_np.shape[:2]\n",
        "                points = np.array([\n",
        "                    [lm.x * w, lm.y * h]\n",
        "                    for lm in landmarks.landmark\n",
        "                ])\n",
        "                return points\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Face landmark extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    def apply_sdedit(\n",
        "        self,\n",
        "        image: Image.Image,\n",
        "        prompt: str,\n",
        "        control_image: Image.Image,\n",
        "        noise_strength: float = 0.5,\n",
        "        controlnet_conditioning_scale: float = 1.0,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_inference_steps: int = 50,\n",
        "        seed: Optional[int] = None\n",
        "    ) -> Image.Image:\n",
        "        \"\"\"Perform domain transfer applying SDEdit logic.\"\"\"\n",
        "        if seed is not None:\n",
        "            generator = torch.Generator(device=self.device).manual_seed(seed)\n",
        "        else:\n",
        "            generator = None\n",
        "\n",
        "        output = self.pipe(\n",
        "            prompt=prompt,\n",
        "            image=control_image,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
        "            generator=generator,\n",
        "        )\n",
        "        return output.images[0]\n",
        "\n",
        "    def process_single_image(\n",
        "        self,\n",
        "        image_path: str,\n",
        "        output_path: str,\n",
        "        style_prompt: str,\n",
        "        noise_strength: float = 0.5,\n",
        "        controlnet_strength: float = 1.0,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_steps: int = 50,\n",
        "        seed: Optional[int] = None\n",
        "    ) -> bool:\n",
        "        \"\"\"Process a single image.\"\"\"\n",
        "        try:\n",
        "            image = load_image(image_path)\n",
        "            image = image.resize((512, 512))\n",
        "\n",
        "            print(f\"  Extracting control image...\")\n",
        "            control_image = self.extract_pose_landmarks(image)\n",
        "\n",
        "            print(f\"  Transforming style...\")\n",
        "            output_image = self.apply_sdedit(\n",
        "                image=image,\n",
        "                prompt=style_prompt,\n",
        "                control_image=control_image,\n",
        "                noise_strength=noise_strength,\n",
        "                controlnet_conditioning_scale=controlnet_strength,\n",
        "                guidance_scale=guidance_scale,\n",
        "                num_inference_steps=num_steps,\n",
        "                seed=seed\n",
        "            )\n",
        "\n",
        "            output_image.save(output_path)\n",
        "            print(f\"  Saved to: {output_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def process_batch(\n",
        "        self,\n",
        "        input_dir: str,\n",
        "        output_dir: str,\n",
        "        style_prompt: str,\n",
        "        noise_strength: float = 0.5,\n",
        "        controlnet_strength: float = 1.0,\n",
        "        guidance_scale: float = 7.5,\n",
        "        num_steps: int = 50,\n",
        "        extensions: List[str] = [\".jpg\", \".jpeg\", \".png\"],\n",
        "        seed: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"Process all images in a folder.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        input_path = Path(input_dir)\n",
        "        image_files = []\n",
        "        for ext in extensions:\n",
        "            image_files.extend(list(input_path.glob(f\"*{ext}\")))\n",
        "            image_files.extend(list(input_path.glob(f\"*{ext.upper()}\")))\n",
        "\n",
        "        print(f\"\\nProcessing {len(image_files)} images\")\n",
        "        print(f\"Style: {style_prompt}\")\n",
        "        print(f\"Noise Strength: {noise_strength}\")\n",
        "        print(f\"ControlNet Strength: {controlnet_strength}\\n\")\n",
        "\n",
        "        success_count = 0\n",
        "        for i, img_path in enumerate(image_files, 1):\n",
        "            print(f\"[{i}/{len(image_files)}] Processing: {img_path.name}\")\n",
        "            output_path = os.path.join(output_dir, f\"styled_{img_path.name}\")\n",
        "\n",
        "            if self.process_single_image(\n",
        "                str(img_path), output_path, style_prompt,\n",
        "                noise_strength, controlnet_strength,\n",
        "                guidance_scale, num_steps, seed\n",
        "            ):\n",
        "                success_count += 1\n",
        "\n",
        "        print(f\"\\nFinished: Transformed {success_count}/{len(image_files)} images.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution entry point.\"\"\"\n",
        "    INPUT_DIR = \"./input_images\"\n",
        "    OUTPUT_DIR = \"./output_styled\"\n",
        "\n",
        "    # Model Selection\n",
        "    # Option 1: SD 1.5 (Recommended - Fast, no token required)\n",
        "    MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "    # Option 2: SD 2.1 (Higher quality, may require token)\n",
        "    # MODEL_ID = \"stabilityai/stable-diffusion-2-1\"\n",
        "\n",
        "    # Hugging Face Token (if required)\n",
        "    # Method 1: Environment variable export HF_TOKEN=\"your_token\"\n",
        "    # Method 2: Specify directly here\n",
        "    HF_TOKEN = None  # e.g., \"hf_xxxxxxxxxxxxx\"\n",
        "\n",
        "    STYLE_PROMPTS = {\n",
        "        \"pixar\": \"a 3D render of a face in Pixar animation style, high quality, detailed, professional lighting\",\n",
        "        \"anime\": \"anime style portrait, cel shaded, vibrant colors, expressive eyes, detailed\",\n",
        "        \"lego\": \"LEGO minifigure face, plastic texture, simplified features, toy style\",\n",
        "        \"oil_painting\": \"oil painting portrait, classical style, rich colors, brushstrokes visible\",\n",
        "        \"cartoon\": \"cartoon style portrait, bold lines, vibrant colors, simplified features\"\n",
        "    }\n",
        "\n",
        "    STYLE = \"pixar\"\n",
        "    NOISE_STRENGTH = 0.4 #reduced\n",
        "    CONTROLNET_STRENGTH = 0.8\n",
        "    GUIDANCE_SCALE = 7.5\n",
        "    NUM_STEPS = 50\n",
        "    SEED = 42\n",
        "    USE_CANNY = False # True: Canny (Lightweight), False: OpenPose (High Accuracy)\n",
        "\n",
        "    try:\n",
        "        artist = AvatarArtist2D(\n",
        "            model_id=MODEL_ID,\n",
        "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "            use_canny=USE_CANNY,\n",
        "            hf_token=HF_TOKEN\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Initialization Error: {e}\")\n",
        "        print(\"\\nüí° Troubleshooting:\")\n",
        "        print(\"  1. Login to Hugging Face: huggingface-cli login\")\n",
        "        print(\"  2. Or set environment variable: export HF_TOKEN='your_token'\")\n",
        "        print(\"  3. Or use a token-free model: MODEL_ID='runwayml/stable-diffusion-v1-5'\")\n",
        "        return\n",
        "\n",
        "    artist.process_batch(\n",
        "        input_dir=INPUT_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        style_prompt=STYLE_PROMPTS[STYLE],\n",
        "        noise_strength=NOISE_STRENGTH,\n",
        "        controlnet_strength=CONTROLNET_STRENGTH,\n",
        "        guidance_scale=GUIDANCE_SCALE,\n",
        "        num_steps=NUM_STEPS,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4k4Q3MVjpJfC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def show_image(image_dir):\n",
        "    image_paths = [\n",
        "        os.path.join(image_dir, f)\n",
        "        for f in sorted(os.listdir(image_dir))\n",
        "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
        "    ][:6]\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "    axes = axes.flatten()\n",
        "    for ax, img_path in zip(axes, image_paths):\n",
        "        img = Image.open(img_path)\n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_title(os.path.basename(img_path), fontsize=9)\n",
        "    for ax in axes[len(image_paths):]:\n",
        "        ax.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "1jiD97DZpJfD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_image('input_images')"
      ],
      "metadata": {
        "trusted": true,
        "id": "2eM_M0NUpJfD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_image('output_styled')"
      ],
      "metadata": {
        "trusted": true,
        "id": "BIBdnfDbpJfD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Oak9aM2LpJfD"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}